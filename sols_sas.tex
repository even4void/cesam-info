%---------------------------------------------------------------- Séance 08 --
\chapter*{Semaine 8\markboth{Corrigés de la semaine 8}{}}
\label{start:sol8b}
\soln{\ref{exo:8.1}}
Pour saisir les données sous \SAS, on effectue dans un premier temps une
étape \texttt{DATA} dans laquelle les données individuelles sont saisie
manuellement à l'aide de la commande \texttt{input} pour indiquer le nom de
la variable, suivi de \texttt{cards} qui place les données directement dans
le programme \SAS.
\begin{verbatim}
DATA Exercice1_1;
INPUT x;
CARDS;
3.68
2.21
2.45
8.64
4.32
3.43
5.11
3.87
;
RUN;
\end{verbatim}

Le nombre d'observations ainsi que les valeurs des observations extrêmes de
la distribution empirique sont obtenues à partir de la commande \texttt{PROC
  SUMMARY}, en précisant les options correspondantes (\texttt{n},
\texttt{min} et \texttt{max} et \texttt{range}).
\begin{verbatim}
PROC SUMMARY DATA=Exercice1_1 PRINT n min max range; VAR x; RUN;
\end{verbatim}

\begin{verbatim}
                                       Procédure SUMMARY

                                    Variable d'analyse : x

                       N         Minimum         Maximum      Intervalle
                       -------------------------------------------------
                       8       2.2100000       8.6400000       6.4300000
                       -------------------------------------------------
\end{verbatim}

Comme le jeu de données est limité en termes d'effectifs, le plus simple
consiste à modifier l'étape \texttt{DATA} précédente en effectuant les
modifications demandées : (a) remplacement de la valeur 8.64 par 3.64, et
(b) recodage de la valeur 5.11 en point (".") qui est le symbole par défaut
utilisé par \SAS pour représenter les données manquantes.
\begin{verbatim}
DATA Exercice1_1;
INPUT x;
CARDS;
3.68
2.21
2.45
3.64    /* (a) */
4.32
3.43
.       /* (b) */
3.87
;
RUN;
\end{verbatim}
%
%
%
\soln{\ref{exo:8.2}}
La saisie des données s'effectue de la même manière qu'à l'exercice
précédent, c'est-à-dire à partir d'une étape \texttt{DATA} :
\begin{verbatim}
DATA Exercice1_2;
INPUT X;
detect=1;
IF X <= log10(50) THEN detect=0;
CARDS;
3.64
2.27
1.43
1.77
4.62
3.04
1.01
2.14
3.02
5.62
5.51
5.51
1.01
1.05
4.19
2.63
4.34
4.85
4.02
5.92
;
RUN;
\end{verbatim}
On notera une petite différence par rapport à l'exercice~\ref{exo:1.1}, à
savoir que l'on définit également une variable \texttt{detect} qui prend la
valeur 0 si $X\le \log_{10}(50)$ et 1 autrement. Ceci est effectué avant
l'instruction \texttt{cards} qui signifie le début de la sauvegarde des
données numériques en mémoire.

Le nombre de patients avec une charge virale indétectable s'obtient à partir
d'un simple tri à plat de la variable \texttt{detect} grâce à la commande
\texttt{PROC FREQ}.
\begin{verbatim}
PROC FREQ DATA=exercice1_2; TABLES detect; RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure FREQ

                                                         Fréquence    Pctage.
                 detect    Fréquence     Pourcentage     cumulée      cumulé
                 ------------------------------------------------------------
                      0           4         20.00               4      20.00
                      1          16         80.00              20     100.00
\end{verbatim}

Pour calculer la charge virale médiane pour les seules observations valides
(c'est-à-dire ayant une valeur au-dessus du seuil limite de détection), il
est nécessaire de sélectionner les unités statistiques remplissant les
conditions de validité et d'utiliser la commande \texttt{PROC SUMMARY}.
\begin{verbatim}
DATA detect; SET exercice1_2;
Y=exp(X*log(10));
IF detect=1; RUN;
PROC SUMMARY DATA=detect PRINT median; VAR Y; RUN;
\end{verbatim}

\begin{verbatim}
                                       Procédure SUMMARY

                                    Variable d'analyse : Y

                                              Médiane
                                         ------------
                                             12979.73
                                         ------------
\end{verbatim}
%
%
%
\soln{\ref{exo:8.3}}
Pour lire les données contenues dans le fichier \texttt{anorexia.data} dont
un aperçu est fourni ci-dessous
\begin{verbatim}
Group Before After
g1 80.5  82.2
g1 84.9  85.6
g1 81.5  81.4
g1 82.6  81.9
g1 79.9  76.4
\end{verbatim}
on va utiliser la commande \texttt{infile} dans l'étape \texttt{DATA}, en
précisant que la lecture des données doit commencer à la 2\ieme\ ligne
(\texttt{firstobs=2}) et que les données sont séparées par des espaces (en
nombre variable).
\begin{verbatim}
DATA anorexia;
INFILE "C:\data\anorexia.dat" firstobs=2 dlm="09"X;
INPUT  groupe $ 1-2  before 3-7   after 8-13;
RUN;
\end{verbatim}
On peut très bien associer des étiquettes plus informatives aux modalités
prises par la variable qualitative \texttt{Group} à l'aide d'une étape
\texttt{FORMAT} :
\begin{verbatim}
PROC FORMAT;
VALUE Therapie
  1='Thérapie comportementale'
  2='Thérapie familiale'
  3='Thérapie contrôle'
;
RUN;
\end{verbatim}

Pour obtenir les effectifs par type de thérapie, on utilise un simple tri à
plat à l'aide de la commande \texttt{PROC FREQ}. L'affichage des résultats
peut être personnalisé en utilisant le renommage des groupes réalisé à
l'étape précédente.
\begin{verbatim}
PROC FREQ DATA=anorexia; TABLES groupe; FORMAT groupe therapie.; RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure FREQ

                                                                  Fréquence    Pctage.
                          groupe    Fréquence     Pourcentage     cumulée      cumulé
        ------------------------------------------------------------------------------
        Thérapie comportementale          29         40.28              29      40.28
        Thérapie familiale                26         36.11              55      76.39
        Thérapie contrôle                 17         23.61              72     100.00
\end{verbatim}

La transformation d'unités pour les poids ne pose pas de problème
spécifique, mais il faut décider si l'on crée de nouvelles variables ou si
l'on remplace les valeurs existantes. Ici, on créera deux nouvelles
variables, \verb|before_kg| et \verb|after_kg| :
\begin{verbatim}
DATA anorexia; SET anorexia;
before_kg=before/2.2;
after_kg=after/2.2;
RUN;
\end{verbatim}

Pour les scores de différences, on crée également une nouvelles variable
à l'aide d'une étape \texttt{DATA} :
\begin{verbatim}
DATA anorexia; SET anorexia;
diff=after_kg-before_kg;
RUN;
\end{verbatim}
et l'on résumé la distribution des scores de différence (moyenne et étendue)
par groupe de traitement à l'aide de \texttt{PROC SUMMARY}. On notera que
puisque l'on fait intervenir une variable de groupement, il est nécessaire
dans un premier temps de trier les données par classe (variable
\texttt{groupe}). 
\begin{verbatim}
PROC SORT DATA=anorexia; BY groupe; RUN;
PROC SUMMARY DATA=anorexia PRINT n mean min max range; VAR diff; BY groupe; FORMAT groupe therapie.; RUN;
\end{verbatim}

\begin{verbatim}
------------------------------- groupe=Thérapie comportementale -------------------------------

                                       Procédure SUMMARY

                                  Variable d'analyse : diff

               N         Moyenne         Minimum         Maximum      Intervalle
              ------------------------------------------------------------------
              29       1.3667712      -4.1363636       9.5000000      13.6363636
              ------------------------------------------------------------------


---------------------------------- groupe=Thérapie familiale ----------------------------------

                                  Variable d'analyse : diff

               N         Moyenne         Minimum         Maximum      Intervalle
              ------------------------------------------------------------------
              26      -0.2045455      -5.5454545       7.2272727      12.7727273
              ------------------------------------------------------------------


---------------------------------- groupe=Thérapie contrôle -----------------------------------

                                  Variable d'analyse : diff

               N         Moyenne         Minimum         Maximum      Intervalle
              ------------------------------------------------------------------
              17       3.3021390      -2.4090909       9.7727273      12.1818182
              ------------------------------------------------------------------
\end{verbatim}
%
%
%
\soln{\ref{exo:8.4}}
La saisie des données brutes se fera comme dans les exercices~8.1 et 8.2 à
l'aide d'une étape \texttt{DATA} (commande \texttt{cards}).
\begin{verbatim}
DATA X;
INPUT X;
CARDS;
24.9
25.0
25.0
25.1
25.2
25.2
25.3
25.3
25.3
25.4
25.4
25.4
25.4
25.5
25.5
25.5
25.5
25.6
25.6
25.6
25.7
25.7
25.8
25.8
25.9
26.0
;
RUN;
\end{verbatim}

Les indicateurs de tendance centrale (moyenne, médiane et mode) peuvent être
calculés et affichés à partir de la commande \texttt{PROC SUMMARY}.
\begin{verbatim}
PROC SUMMARY DATA=X PRINT mean median mode; VAR X; RUN;
\end{verbatim}

\begin{verbatim}
                                       Procédure SUMMARY

                                    Variable d'analyse : X

                              Moyenne         Médiane            Mode
                         --------------------------------------------
                           25.4461538      25.4500000      25.4000000
                         --------------------------------------------
\end{verbatim}

Quant à la variance, on changera simplement les options de calcul dans
\texttt{PROC SUMMARY} :
\begin{verbatim}
PROC SUMMARY DATA=X PRINT var; VAR X; RUN;
\end{verbatim}

\begin{verbatim}
                                       Procédure SUMMARY

                                    Variable d'analyse : X

                                             Variance
                                         ------------
                                            0.0793846
                                         ------------
\end{verbatim}

Pour le recodage en 4 classes d'intervalles pré-définis pour la variable
\texttt{X}, on crée une variable auxiliaire \verb|X_classes| à laquelle on
affecte les valeurs 1, 2, 3 et 4 selon les valeurs prises par \texttt{X}.
\begin{verbatim}
DATA X; SET X;
/* 24.9-25.1, 25.2-25.4, 25.5-25.7, 25.8-26.0 */

X_classes=1;
IF X ge 25.2 THEN X_classes=2;
IF X ge 25.5 THEN X_classes=3;
IF X ge 25.8 THEN X_classes=4;
RUN;
\end{verbatim}
Le tableau d'effectifs peut s'obtenir directement avec une commande
\texttt{tables} dans \texttt{PROC FREQ} :
\begin{verbatim}
PROC FREQ DATA=X; TABLES X_classes; RUN;
\end{verbatim}

\begin{verbatim}
                                       Procédure FREQ

                                                           Fréquence    Pctage.
                X_classes    Fréquence     Pourcentage     cumulée      cumulé
                ---------------------------------------------------------------
                        1           4         15.38               4      15.38
                        2           9         34.62              13      50.00
                        3           9         34.62              22      84.62
                        4           4         15.38              26     100.00
\end{verbatim}

Pour faciliter la lecture des résultats, on peut tout à fait associer des
étiquettes plus informatives aux quatre classes avec \texttt{PROC FORMAT}
et associer celles-ci au tableau de résultat renvoyé par \texttt{tables}.
\begin{verbatim}
PROC FORMAT;
    VALUE classes   24.9-25.1="Intervalle 1"
                    25.2-25.4="Intervalle 2"
                    25.5-25.7="Intervalle 3"
                    25.8-26.0="Intervalle 4";
RUN;

PROC FREQ DATA=X; TABLES X; FORMAT X classes.; RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure FREQ

                                                            Fréquence    Pctage.
                         X    Fréquence     Pourcentage     cumulée      cumulé
              ------------------------------------------------------------------
              Intervalle 1           4         15.38               4      15.38
              Intervalle 2           9         34.62              13      50.00
              Intervalle 3           9         34.62              22      84.62
              Intervalle 4           4         15.38              26     100.00
\end{verbatim}

Enfin, pour la représentation sous forme d'histogramme, \SAS dispose de ses
propres algorithmes de calcul pour la détermination du nombre de classes à
construire, tout comme R. Tout est géré à partir de la commande
\texttt{PROC GCHART}.
\begin{verbatim}
PROC GCHART DATA=X; Hbar X; RUN;  
\end{verbatim}

\includegraphics{./figs/sas_hbar}


On pourrait également utiliser un diagramme en barres orientées
verticalement en précisant l'option \texttt{Vbar}.
\begin{verbatim}
PROC GCHART DATA=X; Vbar X; RUN;
\end{verbatim}

\includegraphics{./figs/sas_vbar}
%
%
%
\soln{\ref{exo:8.5}}
Pour importer les données stockées dans un simple fichier texte, on utilise
la commande \verb|infile| dans une étape \texttt{DATA}.
\begin{verbatim}
DATA elderly;
INFILE  "C:\data\elderly.dat" dlm="09"X ;
INPUT  taille @@ ;
RUN;
\end{verbatim}
Ici, on notera que l'on ne précise pas la manière dont sont codées les
valeurs manquantes car le "." est le format utilisé par défaut par \SAS. une
autre solution consisterait à introduire une commande \texttt{datalines}
après la commande \texttt{input} et à copier/coller les données du fichier
texte. 

Le nombre total d'observations manquantes peut s'obtenir à partir de
\texttt{PROC SUMMARY} en spécifiant l'option \texttt{nmiss}.
\begin{verbatim}
PROC SUMMARY DATA=elderly PRINT nmiss; VAR taille; RUN;
\end{verbatim}

\begin{verbatim}
                                      Procédure SUMMARY

                                 Variable d'analyse : taille

                                               Nbre
                                           manquant
                                           --------
                                                  5
                                           --------
\end{verbatim}

Pour obtenir la taille moyenne et son intervalle de confiance à 95~\%
associé, il suffit d'entrer la commande suivante :
\begin{verbatim}
PROC SUMMARY DATA=elderly PRINT clm uclm lclm; VAR taille; RUN;
\end{verbatim}

\begin{verbatim}
                                       Procédure SUMMARY

                                 Variable d'analyse : taille

   Borne inférieure de l'IC à95% pour la moy.    Borne supérieure de l'IC à95% pour la moy.
   ----------------------------------------------------------------------------------------
                                  159.1966006                                   160.4681393
   ----------------------------------------------------------------------------------------
\end{verbatim}

Enfin, pour afficher la distribution des tailles sous forme d'une courbe de
densité, on utilise la commande \texttt{PROC UNIVARIATE} et
\texttt{HISTOGRAM} avec l'option \texttt{/ kernel}. Le degré de lissage peut
être contrôlé à l'aide de l'option \texttt{C}.
\begin{verbatim}
PROC UNIVARIATE DATA=elderly; VAR taille; HISTOGRAM taille / kernel; RUN;
\end{verbatim}

\begin{verbatim}
                                     Procédure UNIVARIATE
                                      Variable :  taille

                                           Moments

    N                                       346    Somme des poids                     346
    Moyenne                           159.83237    Somme des observations            55302
    Ecart-type                       6.01261415    Variance                     36.1515289
    Skewness                         0.14823207    Kurtosis                     0.14951551
    Somme des carrés non corrigée       8851522    Somme des carrés corrigée    12472.2775
    Coeff Variation                  3.76182506    Std Error Mean               0.32324014
\end{verbatim}

\includegraphics{./figs/sas_kerneld}
%
%
%
\soln{\ref{exo:8.6}}
Les données sur les poids à la naissance de Hosmer \& Lemeshow (1989) ont
été exportées au format texte dans le fichier \texttt{birthwt.dat}, et elles
peuvent être importées ainsi :
\begin{verbatim}
PROC IMPORT OUT= WORK.BIRTHWT
            DATAFILE= "C:\data\birthwt.dat"
            DBMS=DLM REPLACE;
     DELIMITER='20'x;
     GETNAMES=NO;
     DATAROW=1;
RUN;

DATA birthwt1; SET birthwt;
  low = var1;
  age = var2;
  lwt = var3 ;
  race = var4 ;
  smoke = var5;
  ptl = var6 ;
  ht = var7 ;
  ui = var8 ;
  ftw = var9 ;
  bwt = var10 ;
  DROP var1-var10;
RUN;
\end{verbatim}
La première étape (\texttt{PROC IMPORT}) consiste à définir les options pour
l'importation des données : les noms de variables ne figurent pas dans le
fichier de données (\texttt{GETNAMES=NO}) et la lecture des données doit
commencer dès la 1\iere\ ligne du fichier (\texttt{DATAROW=1}), sachant que
les données individuelles sont séparées par des espaces. La seconde étape
(\texttt{DATA}) consiste à assigner des noms de variables au tableau de
données \texttt{WORK.BIRTHWT} généré par \SAS. On notera qu'il est
nécessaire de supprimer les anciens noms de variable après renommage, d'où
l'usage de la commande \texttt{DROP}.

Pour associer de nouvelles étiquettes aux variables \texttt{low},
\texttt{race}, \texttt{smoke}, \texttt{ui} et \texttt{ht}, il suffit de
définir des "labels" et de les associer aux variables en question (cela ne
change le format de représentation des variables en mémoire, qui restent
stockées sous forme de nombre).
\begin{verbatim}
PROC FORMAT;
      VALUE  low 1="Poids inférieur à 2.5 Kg"
                 0="Poids supérieur à 2.5 Kg";
      VALUE  ethnicite 1="White"
                       2="Black"
                       3="Other";
      VALUE  tabac 1="consommation tabac durant grossesse"
                   0="Pas de consommation tabac durant grossesse";
      VALUE  Hypert 1="Antecedent d hypertension"
                    0="Pas d antecedent d hypertension";
      VALUE  uterine 1="Manisfestation d irritabilite uterine"
                     0="Pas de manisfestation d irritabilite uterine";
RUN;
\end{verbatim}
On pourra vérifier que ces modifications ont bien été prises en compte en
utilisant une comamnde telle que \texttt{PROC FREQ} pour afficher les
tableaux d'effectifs associés à ces variables qualitatives.
\begin{verbatim}
PROC FREQ DATA=birthwt1; 
  TABLES low race smoke ui ht;
  FORMAT low low. race ethnicite. smoke tabac. ht hypert. ui uterine.;
RUN;
\end{verbatim}
Voici les résultats produits par \SAS pour une partie des variables.
\begin{verbatim}
                                        Procédure FREQ

                                                                  Fréquence    Pctage.
                             low    Fréquence     Pourcentage     cumulée      cumulé
        ------------------------------------------------------------------------------
        Poids supérieur à 2.5 Kg         130         68.78             130      68.78
        Poids inférieur à 2.5 Kg          59         31.22             189     100.00


                                                         Fréquence    Pctage.
                   race    Fréquence     Pourcentage     cumulée      cumulé
                  -----------------------------------------------------------
                  White          96         50.79              96      50.79
                  Black          26         13.76             122      64.55
                  Other          67         35.45             189     100.00


                                                                          Fréquence   Pctage.
                                      smoke   Fréquence    Pourcentage    cumulée     cumulé
 --------------------------------------------------------------------------------------------
 Pas de consommation tabac durant grossesse        115        60.85            115     60.85
 consommation tabac durant grossesse                74        39.15            189    100.00
\end{verbatim}

La conversion du poids des mères en \emph{kg} ne pose pas de problème
particulier, et ici on remplacera directement les données disponibles dans
une étape \texttt{DATA}.
\begin{verbatim}
DATA birthwt2; SET birthwt1;
  lwt=lwt/2.2; 
RUN;
\end{verbatim}

Les indicateurs de tendance centrale et de dispersion relative sont obtenus
à partir de la commande \texttt{PROC SUMMARY}, en spécifiant les options
adéqautes. 
\begin{verbatim}
PROC SUMMARY DATA=birthwt2 PRINT mean median qrange; VAR lwt; RUN;
\end{verbatim}

\begin{verbatim}
                                       Procédure SUMMARY

                                   Variable d'analyse : lwt

                                                            Intervalle
                              Moyenne         Médiane    interquartile
                         ---------------------------------------------
                           59.0067340      55.0000000       13.6363636
                         ---------------------------------------------
\end{verbatim}

Enfin, un histogramme du poids des mères est construit à l'aide de la
commande \texttt{PROC GCHART}. Ici, sans autre option un histogramme de
densité sera construit.
\begin{verbatim}
PROC GCHART DATA=birthwt2; Hbar lwt; RUN;

PROC UNIVARIATE DATA=birthwt2;
  VAR lwt;
  HISTOGRAM lwt / kernel;
RUN;
\end{verbatim}

\begin{verbatim}
                                     Procédure UNIVARIATE
                                        Variable :  lwt

                                           Moments

    N                                       189    Somme des poids                     189
    Moyenne                           59.006734    Somme des observations       11152.2727
    Ecart-type                       13.8997183    Variance                      193.20217
    Skewness                          1.4020068    Kurtosis                      2.4038854
    Somme des carrés non corrigée    694381.198    Somme des carrés corrigée     36322.008
    Coeff Variation                   23.556156    Std Error Mean               1.01105574
\end{verbatim}

\includegraphics{./figs/sas_hbar2}

\includegraphics{./figs/sas_kerneld2}

Concernant la proportion de mères ayant fumé durant la grossesse et
le calcul de l'intervalle de confiance à 95~\% associé, on peut utiliser la
commande \texttt{PROC SUMMARY}, qui comme dans le cas de la commande \R
\texttt{prop.test} suppose de grands échantillons :
\begin{verbatim}
PROC SUMMARY DATA=birthwt2 PRINT clm uclm lclm; VAR smoke; RUN;
\end{verbatim}

\begin{verbatim}
                                       Procédure SUMMARY

                                  Variable d'analyse : smoke

   Borne inférieure de l'IC à95% pour la moy.    Borne supérieure de l'IC à95% pour la moy.
   ----------------------------------------------------------------------------------------
                                    0.3213118                                     0.4617570
   ----------------------------------------------------------------------------------------
\end{verbatim}

Les diagrammes en barres peuvent être affichés, selon une orientation
verticale ou horizontale, à l'aide de \texttt{PROC GCHART}, avec une syntaxe
identique au cas des histogrammes pour des variables numériques.
\begin{verbatim}
PROC GCHART DATA=birthwt2; Vbar smoke; RUN;
PROC GCHART DATA=birthwt2; Hbar smoke; RUN;
\end{verbatim}

Pour générer les terciles, on classera dans un premier temps les
observations par ordre croissant, puis on découpera les valeurs en trois
sous-effectifs égaux (de taille $189/3=63$).
\begin{verbatim}
/* PROC UNIVARIATE DATA=birthwt2; VAR lwt; RUN; */
PROC RANK DATA=birthwt2 OUT=ranking; VAR age; RANKS ordre; RUN;

DATA birthwt3; SET ranking;
age_classe=1;
IF ordre gt 189/3 THEN age_classe=2; 
IF ordre gt 2*(189/3) THEN age_classe=3;
RUN;
\end{verbatim}
Le croisement de cette nouvelle variable avec la variable indicatrice de
sous-poids donne les résultats suivants, exprimées en termes de proportions :
\begin{verbatim}
PROC FREQ DATA=birthwt3; TABLE age_classe*low; RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure FREQ

                                  Table de age_classe par low

                           age_classe      low

                           Fréquence      ‚
                           Pourcentage    ‚
                           Pctage en ligne‚
                           Pctage en col. ‚       0‚       1‚  Total
                           ---------------ˆ--------ˆ--------ˆ
                                        1 ‚     46 ‚     23 ‚     69
                                          ‚  24.34 ‚  12.17 ‚  36.51
                                          ‚  66.67 ‚  33.33 ‚
                                          ‚  35.38 ‚  38.98 ‚
                           ---------------ˆ--------ˆ--------ˆ
                                        2 ‚     34 ‚     17 ‚     51
                                          ‚  17.99 ‚   8.99 ‚  26.98
                                          ‚  66.67 ‚  33.33 ‚
                                          ‚  26.15 ‚  28.81 ‚
                           ---------------ˆ--------ˆ--------ˆ
                                        3 ‚     50 ‚     19 ‚     69
                                          ‚  26.46 ‚  10.05 ‚  36.51
                                          ‚  72.46 ‚  27.54 ‚
                                          ‚  38.46 ‚  32.20 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Total               130       59      189
                                             68.78    31.22   100.00
\end{verbatim}

Un tri à plat de la variable \texttt{race} est effectuée de la même manière
à partir de la commande \texttt{PROC FREQ}.
\begin{verbatim}
PROC FREQ DATA=birthwt3; TABLES race; FORMAT race ethnicite.; RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure FREQ

                                                         Fréquence    Pctage.
                   race    Fréquence     Pourcentage     cumulée      cumulé
                  -----------------------------------------------------------
                  White          96         50.79              96      50.79
                  Black          26         13.76             122      64.55
                  Other          67         35.45             189     100.00
\end{verbatim}

Enfin, pour résumer la distribution des variables selon la variable
indicatrice \texttt{low}, on reprendra la commande \texttt{PROC FREQ} en
sépcifiant la liste des variables d'intérêt, 
\verb|(race smoke ui ht age_classe)|, à croiser avec \verb|low| via
l'opérateur \texttt{*}.
\begin{verbatim}
PROC FREQ DATA=birthwt3; 
  TABLES (race smoke ui ht age_classe)*low;
  FORMAT low low. race ethnicite. smoke tabac. ht hypert. ui uterine.;
RUN;
\end{verbatim}
Voici un exemple de sortie pour le croisement entre \texttt{race} et \texttt{low}.
\begin{verbatim}
                                        Procédure FREQ

                                     Table de race par low

                           race            low

                           Fréquence      ‚
                           Pourcentage    ‚
                           Pctage en ligne‚
                           Pctage en col. ‚Poids su‚Poids in‚  Total
                                          ‚périeur ‚férieur ‚
                                          ‚à 2.5 Kg‚à 2.5 Kg‚
                           ---------------ˆ--------ˆ--------ˆ
                           White          ‚     73 ‚     23 ‚     96
                                          ‚  38.62 ‚  12.17 ‚  50.79
                                          ‚  76.04 ‚  23.96 ‚
                                          ‚  56.15 ‚  38.98 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Black          ‚     15 ‚     11 ‚     26
                                          ‚   7.94 ‚   5.82 ‚  13.76
                                          ‚  57.69 ‚  42.31 ‚
                                          ‚  11.54 ‚  18.64 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Other          ‚     42 ‚     25 ‚     67
                                          ‚  22.22 ‚  13.23 ‚  35.45
                                          ‚  62.69 ‚  37.31 ‚
                                          ‚  32.31 ‚  42.37 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Total               130       59      189
                                             68.78    31.22   100.00
\end{verbatim}
\label{stop:sol8b}

%---------------------------------------------------------------- Séance 09 --
\chapter*{Semaine 9\markboth{Corrigés de la semaine 9}{}}
\label{start:sol9b}
\soln{\ref{exo:9.1}} On rappelle que seuls les valeurs numériques des poids
à la naissance sont disponibles dans le fichier \texttt{sirds.dat}, et qu'il
nous faut contruire la variable de groupement (enfants décédés \emph{versus}
vivants). Une manière de procéder consiste à effectuer deux étapes
\texttt{DATA} en incluant une variable qualitative indiquant le status, en
plus des données brutes insérer par copier/coller.
\begin{verbatim}
DATA DCD;
INPUT poids @@;
deces=1;
DATALINES;
1.050 1.175 1.230 1.310 1.500 1.600 1.720 1.750 1.770 2.275
2.500 1.030 1.100 1.185 1.225 1.262 1.295 1.300 1.550 1.820
1.890 1.940 2.200 2.270 2.440 2.560 2.730
;
RUN;

DATA VIV;
INPUT poids @@;
deces=0;
CARDS;
1.130 1.575 1.680 1.760 1.930 2.015 2.090 2.600 2.700 2.950
3.160 3.400 3.640 2.830 1.410 1.715 1.720 2.040 2.200 2.400
2.550 2.570 3.005
;
RUN;

DATA DRIA; SET DCD VIV; RUN;
\end{verbatim}

Le test de Student est réalisé en utilisant la commande \texttt{PROC TTEST},
qui par défaut fournit les résultats sous l'hypothèse d'homoskédasticité ou
d'hétéroskédasticité, ainsi que les résumés descriptifs numériques et
graphiques pour la distribution du poids selon le status clinique. La
variable de classification est introduite après l'instruction \texttt{CLASS}
et la variable réponse après l'instruction \texttt{VAR}. Par défaut, le test
reporté est bilatéral.
\begin{verbatim}
PROC TTEST DATA=DRIA PLOTS=all;
  CLASS deces;
  VAR poids;
RUN;
\end{verbatim}

\begin{verbatim}
                                      The TTEST Procedure

                                       Variable:  poids

       deces           N     Moyenne    Ecart-type    Err. type     Minimum     Maximum

       0              23      2.3074        0.6647       0.1386      1.1300      3.6400
       1              27      1.6917        0.5176       0.0996      1.0300      2.7300
       Diff (1-2)             0.6157        0.5896       0.1673

                                               Moyenne de                       Ecart-type de
deces         Méthode           Moyenne        l'IC à 95%       Ecart-type        l'IC à 95%

0                                2.3074      2.0199   2.5948        0.6647      0.5141   0.9408
1                                1.6917      1.4870   1.8965        0.5176      0.4077   0.7094
Diff (1-2)    Pooled             0.6157      0.2793   0.9520        0.5896      0.4917   0.7366
Diff (1-2)    Satterthwaite      0.6157      0.2710   0.9603

                                                            Valeur
                 Méthode          Variances       DDL    du test t    Pr > |t|

                 Pooled           Equal            48         3.68      0.0006
                 Satterthwaite    Unequal       41.28         3.61      0.0008

                                     Egalité des variances

                                                          Valeur
                     Méthode     DDL Num.    DDL Res.          F    Pr > F

                     Folded F          22          26       1.65    0.2218
\end{verbatim}

\includegraphics{./figs/sas_ttest}
%
%
%
\soln{\ref{exo:9.2}}
Les données de l'étude sur le sommeil servant de base à
l'article de Student peuvent être importées sous \SAS comme à l'exercice
précédent, c'est-à-dire en combinant les résultats de deux étapes
\texttt{DATA}. On en profitera lors de la dernière étape pour créer une
variable auxiliaire pour les scores de différence.
\begin{verbatim}
DATA DHH;
INPUT GMSD @@;
CARDS;
0.7 -1.6 -0.2 -1.2 -0.1 3.4 3.7 0.8 0.0 2.0
;
RUN;

DATA LHH;
INPUT GMSL @@;
CARDS;
1.9 0.8 1.1 0.1 -0.1 4.4 5.5 1.6 4.6 3.4
;
RUN;

DATA HH; MERGE DHH LHH; diff_GMS=GMSL-GMSD; RUN;
\end{verbatim}

Un résumé numérique pour l'ensemble des variables numériques (\texttt{GMSD},
\texttt{GMSL}, et \verb|diff_GMS|) peut être obtenu avec \texttt{PROC SUMMARY} 
de la manière suivante :
\begin{verbatim}
PROC SUMMARY DATA=HH PRINT n mean var lclm uclm; VAR GMSD GMSL diff_GMS; RUN;
\end{verbatim}

\begin{verbatim}
                                       Procédure SUMMARY

 Variable     N         Moyenne        Variance    Borne inférieure de l'IC à95% pour la moy.
 --------------------------------------------------------------------------------------------
 GMSD        10       0.7500000       3.2005556                                    -0.5297804
 GMSL        10       2.3300000       4.0090000                                     0.8976775
 diff_GMS    10       1.5800000       1.5128889                                     0.7001142
 --------------------------------------------------------------------------------------------

                    Variable    Borne supérieure de l'IC à95% pour la moy.
                    ------------------------------------------------------
                    GMSD                                         2.0297804
                    GMSL                                         3.7623225
                    diff_GMS                                     2.4598858
                    ------------------------------------------------------
\end{verbatim}

Les gains moyens de temps de sommeil pour chaque molécule peuvent être
représentés à l'aide d'un diagramme en barres grâce à \texttt{PROC GCHART}.
\begin{verbatim}
PROC GCHART DATA=hh; Hbar diff_gms / midpoints=(0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6); RUN;
\end{verbatim}

Enfin, pour réaliser un test $t$ pour données appariées, on utilisera toujours la
commande \texttt{PROC TTEST}, mais en spécifiant l'option \texttt{PAIRED},
comme indiqué ci-après.
\begin{verbatim}
PROC TTEST DATA=hh;
  PAIRED GMSD*GMSL;
RUN;
\end{verbatim}

\begin{verbatim}
                                      The TTEST Procedure

                                   Difference:  GMSD - GMSL

                N     Moyenne    Ecart-type    Err. type     Minimum     Maximum

               10     -1.5800        1.2300       0.3890     -4.6000           0

                               Moyenne de                       Ecart-type de
                Moyenne        l'IC à 95%       Ecart-type        l'IC à 95%

                -1.5800     -2.4599  -0.7001        1.2300      0.8460   2.2455

                                             Valeur
                                   DDL    du test t    Pr > |t|

                                     9        -4.06      0.0028
\end{verbatim}

\includegraphics{./figs/sas_ttestpaired}
%
%
%
\soln{\ref{exo:9.3}}
Dans le cas des données dites "groupées", ou plus généralement d'un tableau
de contingence quelconque, on se contente généralement de saisir le tableau
d'effectif en faisant apparaître distinctement les variables de
classification et les effectifs associés. Sous \SAS, on peut procéder ainsi :
\begin{verbatim}
DATA symptom;
INPUT regime amelioration effectif;
CARDS;
1 1 26
0 1 21
1 0 38
0 0 44
;
RUN;

PROC FORMAT; VALUE ouinon 1="Oui" 0="Non"; RUN;
\end{verbatim}

Ensuite, on peut répondre aux trois questions à partir d'une même commande,
\texttt{PROC FREQ}, en se rappelant qu'il est nécessaire de renseigner
l'option \texttt{WEIGHT} pour indiquer une pondération par les effectifs.
\begin{verbatim}
PROC FREQ DATA=symptom ORDER=data;
  TABLES amelioration * regime / chisq;
  WEIGHT effectif;
  FORMAT regime amelioration ouinon.;
RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure FREQ

                               Table de amelioration par regime

                           amelioration     regime

                           Fréquence      ‚
                           Pourcentage    ‚
                           Pctage en ligne‚
                           Pctage en col. ‚Oui     ‚Non     ‚  Total
                           ---------------ˆ--------ˆ--------ˆ
                           Oui            ‚     26 ‚     21 ‚     47
                                          ‚  20.16 ‚  16.28 ‚  36.43
                                          ‚  55.32 ‚  44.68 ‚
                                          ‚  40.63 ‚  32.31 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Non            ‚     38 ‚     44 ‚     82
                                          ‚  29.46 ‚  34.11 ‚  63.57
                                          ‚  46.34 ‚  53.66 ‚
                                          ‚  59.38 ‚  67.69 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Total                64       65      129
                                             49.61    50.39   100.00


                     Statistiques pour la table de amelioration par regime

                  Statistique                       DDL      Valeur      Prob
                  -----------------------------------------------------------
                  Khi-2                               1      0.9632    0.3264
                  Test du rapport de vraisemblance    1      0.9646    0.3260
                  Khi-2 continuité ajustée            1      0.6376    0.4246
                  Khi-2 de Mantel-Haenszel            1      0.9558    0.3283
                  Coefficient Phi                            0.0864
                  Coefficient de contingence                 0.0861
                  V de Cramer                                0.0864


                                     Test exact de Fisher
                            --------------------------------------
                            Cellule (1,1) Fréquence (F)         26
                            Pr <= F unilatérale à gauche    0.8779
                            Pr >= F unilatérale à droite    0.2124

                            Probabilité de la table (P)     0.0903
                            Pr <= P bilatéral               0.3636

                                 Taille de l'échantillon = 129
\end{verbatim}

\includegraphics{./figs/sas_regime}
%
%
%
\soln{\ref{exo:9.4}}
Comme dans l'exercice précédent, le plus simple pour travailler avec ce type
de tableau à deux entrées est de créer une struture de donnes où l'on fait
correspondre les effectifs pour chacun des croisement des modalités des deux
facteurs d'étude (infarctus et traitement) dans une étape \texttt{DATA}.
\begin{verbatim}
DATA myocarde;
INPUT Infractus traitement nombre;
CARDS;
1 1 28
1 2 18
2 1 656
2 2 658
;
RUN;

PROC FORMAT; 
  VALUE ouinon 1="Oui" 2 = "Non";
  VALUE treat 1=" Placebo" 2 = "Aspirine";
RUN;
\end{verbatim}

Ensuite, on peut répondre aux trois questions à partir d'une même commande,
\texttt{PROC FREQ} et l'option \texttt{/ all}, en se rappelant qu'il est
nécessaire de renseigner l'option \texttt{WEIGHT} pour indiquer une
pondération par les effectifs.
\begin{verbatim}
PROC FREQ DATA=myocarde;
  TABLES traitement*Infractus / all;
  WEIGHT nombre;
  FORMAT Infractus ouinon. traitement treat.;
RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure FREQ

                               Table de traitement par Infractus

                           traitement      Infractus

                           Fréquence      ‚
                           Pourcentage    ‚
                           Pctage en ligne‚
                           Pctage en col. ‚Oui     ‚Non     ‚  Total
                           ---------------ˆ--------ˆ--------ˆ
                            Placebo       ‚     28 ‚    656 ‚    684
                                          ‚   2.06 ‚  48.24 ‚  50.29
                                          ‚   4.09 ‚  95.91 ‚
                                          ‚  60.87 ‚  49.92 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Aspirine       ‚     18 ‚    658 ‚    676
                                          ‚   1.32 ‚  48.38 ‚  49.71
                                          ‚   2.66 ‚  97.34 ‚
                                          ‚  39.13 ‚  50.08 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Total                46     1314     1360
                                              3.38    96.62   100.00


                    Statistiques pour la table de traitement par Infractus

                  Statistique                       DDL      Valeur      Prob
                  -----------------------------------------------------------
                  Khi-2                               1      2.1300    0.1444
                  Test du rapport de vraisemblance    1      2.1474    0.1428
                  Khi-2 continuité ajustée            1      1.7146    0.1904
                  Khi-2 de Mantel-Haenszel            1      2.1284    0.1446
                  Coefficient Phi                            0.0396
                  Coefficient de contingence                 0.0395
                  V de Cramer                                0.0396


                                     Test exact de Fisher
                            --------------------------------------
                            Cellule (1,1) Fréquence (F)         28
                            Pr <= F unilatérale à gauche    0.9468
                            Pr >= F unilatérale à droite    0.0949

                            Probabilité de la table (P)     0.0417
                            Pr <= P bilatéral               0.1768
\end{verbatim}
%
%
%
\soln{\ref{exo:9.5}}
Les données au format \Stata peuvent être importées sous \SAS à l'aide de
\texttt{PROC IMPORT} en précisant le type de source de données, ici
\texttt{DBMS=STATA}.
\begin{verbatim}
PROC IMPORT OUT= WORK.polymorphism
            DATAFILE= "C:\data\polymorphism.dta"
            DBMS=STATA REPLACE;
RUN;
\end{verbatim}

La commande \texttt{PROC GLM} est utilisée pour les modèles linéaires
(ANOVA, régression linéaire simple ou multiple, ANCOVA). Dans le cas d'une
ANOVA, on indique le facteur de classification après l'instruction
\texttt{CLASS} et le modèle d'ANOVA après l'instruction \texttt{MODEL}. Un
modèle d'ANOVA à un facteur s'écrira donc \texttt{y=g} où \texttt{y} désigne
la variable réponse et \texttt{g} le facteur d'étude. 
\begin{verbatim}
PROC GLM DATA=polymorphism; 
  CLASS genotype; 
  MODEL age=genotype; 
RUN;
\end{verbatim}

\begin{verbatim}
                                       The GLM Procedure

Dependent Variable: age   Age at Diagnosis

                                           Somme des         Moyenne     Valeur
      Source                     DDL          carrés     quadratique          F    Pr > F

      Model                        2      2315.73355      1157.86678       7.86    0.0010

      Error                       56      8245.79187       147.24628

      Corrected Total             58     10561.52542


                                     Coef de
                       R-carré           Var    Racine MSE    age Moyenne

                      0.219261      20.00939      12.13451       60.64407


                                                             Moyenne     Valeur
      Source                     DDL       Type I SS     quadratique          F    Pr > F

      genotype                     2     2315.733552     1157.866776       7.86    0.0010


                                                             Moyenne     Valeur
      Source                     DDL     Type III SS     quadratique          F    Pr > F

      genotype                     2     2315.733552     1157.866776       7.86    0.0010
\end{verbatim}

Le tableau de variance retourné par la commande inclut les sommes de carrés
pour le facteur (\texttt{Model}) et la résiduelle (\texttt{Error}), ainsi
que la somme de carrés totale et le coefficient de détermination ($\eta^2$)
reflétant la part de variance expliquée par le modèle. On notera que \SAS
fournit par défaut deux estimations des sommes de carrés pour le facteur
d'étude (type I et III), mais celles-ci sont identiques dans le cas d'un
plan complet équilibré.

La commande \texttt{PROC GLM} fournit également des sorties graphiques,
incluant une représentation de la distribution de l'âge selon le génotype
sous forme de boîtes à moustaches.

\includegraphics{./figs/sas_boxplot}

Il est également possible de représenter la distribution des âges selon le
génotype sous forme de diagramme en barres
\begin{verbatim}
PROC SORT DATA=polymorphism; BY genotype; RUN;

PROC GCHART DATA=polymorphism; Vbar age; BY genotype; RUN;
\end{verbatim}

\includegraphics{./figs/sas_genotype}

Les moyennes de groupes, avec leur intervalles de confiance à 95~\%, peuvent
être obtenues grâce à \texttt{PROC SUMMARY}. La variable de conditionnement
est spécifiée après l'instruction \texttt{BY}.
\begin{verbatim}
PROC SORT DATA=polymorphism; BY genotype; RUN;

PROC SUMMARY DATA=polymorphism PRINT n mean stddev ucl lcl; VAR age; BY genotype; RUN;
\end{verbatim}
Voici la sortie produite par \SAS pour le génotype 1.6/1.6 :

\begin{verbatim}
-------------------------------------- Genotype=1.6/1.6 ---------------------------------------

                                       Procédure SUMMARY

                           Variable d'analyse : age Age at Diagnosis

        N         Moyenne      Ecart-type    Borne supérieure de l'IC à95% pour la moy.
       --------------------------------------------------------------------------------
       14      64.6428571      11.1810770                                    71.0986188
       --------------------------------------------------------------------------------

                           Variable d'analyse : age Age at Diagnosis

                          Borne inférieure de l'IC à95% pour la moy.
                          ------------------------------------------
                                                          58.1870955
                          ------------------------------------------
\end{verbatim}

En ce qui concerne les comparaisons multiples, on utilise toujours
\texttt{PROC GLM}, en ajoutant cette fois l'option 
\verb|MEANS genotype / BON CLDIFF| qui fournit les tests corrigés par la
méthode de Bonferroni (également appelé tests de Dunn sous \SAS). 
\begin{verbatim}
PROC GLM DATA=polymorphism; 
  CLASS genotype; 
  MODEL age=genotype;
  MEANS genotype / BON CLDIFF;
RUN;
\end{verbatim}

\begin{verbatim}
                                       The GLM Procedure

                             Tests t de Bonferroni (Dunn) pour age

 NOTE: Ce test contrôle le taux d'erreur par expérience de Type I, mais il a généralement un
 taux d'erreur de Type II supérieur à celui de Tukey pour toutes les comparaisons par paires.


                            Alpha                             0.05
                            Degrés de liberté de l'erreur       56
                            Erreur quadratique moyenne    147.2463
                            Valeur critique de t           2.46802


                Comparaisons significatives au niveau 0.05 indiquées par ***.


                                           Différence      Intervalle de
                                                entre      confiance à 95
               Comparaison degenotype    les moyennes       % simultané

                 1.6/1.6 - 1.6/0.7              0.264      -9.483   10.010
                 1.6/1.6 - 0.7/0.7             14.268       3.308   25.228  ***
                 1.6/0.7 - 1.6/1.6             -0.264     -10.010    9.483
                 1.6/0.7 - 0.7/0.7             14.004       4.678   23.331  ***
                 0.7/0.7 - 1.6/1.6            -14.268     -25.228   -3.308  ***
                 0.7/0.7 - 1.6/0.7            -14.004     -23.331   -4.678  ***
\end{verbatim}

L'option \texttt{CLDIFF} permet de représenter les résultats portant sur les
différences de moyennes sous forme d'intervalles de confiance, alors que
\texttt{CLM} fait de même pour les moyennes de groupe (intervalles de
confiance simultanés).
\begin{verbatim}
PROC GLM DATA=polymorphism PLOT=MEANPLOT(CLBAND); 
  CLASS genotype; 
  MODEL age=genotype;
  MEANS genotype / BON CLM;
RUN;
\end{verbatim}
%
%
%
\soln{\ref{exo:9.6}}
Pour importer le tableau de données sous \SAS, on procèdera comme dans les
exercices précédents : création de deux tableaux de données dans des étapes
\texttt{DATA} et association entre les deux à l'aide de la commande
\texttt{MERGE}.
\begin{verbatim}
DATA bio1;
INPUT pb @@;
CARDS;
19.8       15.9        15.4
20.5       19.7        17.1
23.7       20.8        18.2
27.1       21.7        18.5
29.6       22.5        19.3
29.9       24.0        21.2
;
RUN;

DATA bio2;
INPUT groupe $ @@;
CARDS;
A           B           C
A           B           C
A           B           C
A           B           C
A           B           C
A           B           C
;
RUN;

DATA biologie; MERGE bio1 bio2; RUN;
\end{verbatim}

À la place de \texttt{PROC GLM}, on peut utiliser la commande \texttt{ANOVA}
pour réaliser une ANOVA à un facteur, la syntaxe générale étant similaire
dans les deux cas (utilisation des instructions \texttt{CLASS} et
\texttt{MODEL}). 
\begin{verbatim}
PROC ANOVA DATA=biologie; 
  CLASS groupe; 
  MODEL pb=groupe;
RUN;
\end{verbatim}

\begin{verbatim}
                                      The ANOVA Procedure

Dependent Variable: pb

                                           Somme des         Moyenne     Valeur
      Source                     DDL          carrés     quadratique          F    Pr > F

      Model                        2     142.8233333      71.4116667       6.82    0.0078

      Error                       15     156.9616667      10.4641111

      Corrected Total             17     299.7850000


                                     Coef de
                       R-carré           Var    Racine MSE    pb Moyenne

                      0.476419      15.12780      3.234828      21.38333


                                                             Moyenne     Valeur
      Source                     DDL        Anova SS     quadratique          F    Pr > F

      groupe                       2     142.8233333      71.4116667       6.82    0.0078
\end{verbatim}

Pour comparer les paires de moyennes entre elles, on rajoutera une
instructions \texttt{MEANS}, en spécifiant l'option \verb|/ BON| pour
travailler avec une correction de type Bonferroni.
\begin{verbatim}
PROC ANOVA DATA=biologie; 
  CLASS groupe; 
  MODEL pb=groupe;
  MEANS groupe / BON;
RUN;
\end{verbatim}

\begin{verbatim}
                                      The ANOVA Procedure

                             Tests t de Bonferroni (Dunn) pour pb

 NOTE: Ce test contrôle le taux d'erreur par comparaison de Type I, mais il a généralement un
                          taux d'erreur de Type II supérieur à REGWQ.


                          Alpha                                 0.05
                          Degrés de liberté de l'erreur           15
                          Erreur quadratique moyenne        10.46411
                          Valeur critique de t               2.69374
                          Différence significative minimale   5.0309


                 Les moyennes avec la même lettre ne sont pas très différentes.


                     Bon Groupement       Moyenne      N    groupe

                                  A        25.100      6    A
                                  A
                             B    A        20.767      6    B
                             B
                             B             18.283      6    C

                                        Le Système SAS       19:38 Saturday, April 16, 2011  50

                                      Procédure NPAR1WAY

                          Analyse de la variance pour la variable pb
                                 Classée par variable groupe

                          groupe              N              Moyenne
                          ------------------------------------------
                          A                   6            25.100000
                          B                   6            20.766667
                          C                   6            18.283333


                                   Somme des        Moyenne
             Source    DDL            carrés    quadratique    Valeur F    Pr > F
             --------------------------------------------------------------------
             Parmi       2        142.823333      71.411667      6.8244    0.0078
             Dans       15        156.961667      10.464111
\end{verbatim}

Enfin, si l'on remet en cause la normalité des distributions parentes, la
comparaison des trois groupes peut être effectuée par une ANOVA sur les
rangs (Kruskal-Wallis) à partir de la commande \texttt{PROC NPAR1WAY} dans
laquelle on indique le facteur de classification (\texttt{CLASS}) et la
variable réponse (\texttt{VAR}).
\begin{verbatim}
PROC NPAR1WAY DATA=biologie; 
  CLASS groupe; 
  VAR pb; 
RUN;
\end{verbatim}

\begin{verbatim}

                                      Procédure NPAR1WAY

                   Scores de Wilcoxon (Sommes du rang) pour la variable pb
                                 Classés par variable groupe

                               Somme des      Attendue    Ecart-type         Score
            groupe       N        scores       sous H0       sous H0         moyen
            ----------------------------------------------------------------------
            A            6          82.0          57.0     10.677078     13.666667
            B            6          59.0          57.0     10.677078      9.833333
            C            6          30.0          57.0     10.677078      5.000000


                                    Test de Kruskal-Wallis

                                     Khi-2         7.9415
                                     DLL                2
                                     Pr > Khi-2    0.0189
\end{verbatim}
%
%
%
\soln{\ref{exo:9.7}}
Le fichier contenant les données, \texttt{weights.sav}, a été exporté depuis
R au format \Stata à l'aide des commandes suivantes :
\begin{verbatim}
library(foreign)
weights <- read.spss("weights.sav", to.data.frame=TRUE)
write.dta(weights, file="weights.dta")
\end{verbatim}
On peut donc l'importer avec \texttt{PROC IMPORT}, en renseignant le type de
source de données (\texttt{DBMS=STATA}), comme on l'a fait pour l'exercice~9.5.
\begin{verbatim}
PROC IMPORT OUT= WORK.weight
            DATAFILE= "C:\data\weights.dta"
            DBMS=STATA REPLACE;

RUN;
\end{verbatim}

Le tableau d'effectifs et de fréquences relatives pour la variable
\texttt{parity} s'obtient à partir de la commande \texttt{PROC FREQ} en
spécifiant l'instruction \texttt{TABLES} pour la variable d'intérêt :
\begin{verbatim}
PROC CONTENTS DATA=weight; RUN;

PROC FREQ DATA=weight; TABLES parity; RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure FREQ

                                            PARITY

                                                               Fréquence    Pctage.
                       PARITY    Fréquence     Pourcentage     cumulée      cumulé
           ------------------------------------------------------------------------
           Singleton                  180         32.73             180      32.73
           One sibling                192         34.91             372      67.64
           2 siblings                 116         21.09             488      88.73
           3 or more siblings          62         11.27             550     100.00
\end{verbatim}

Les moyennes et écarts-type du poids selon la taille de la fratrie
s'obtiennent ainsi :
\begin{verbatim}
PROC SORT DATA=weight; BY parity; RUN;

PROC SUMMARY DATA=weight PRINT n mean stddev min max; VAR weight; BY parity; RUN;
\end{verbatim}

\begin{verbatim}
                                       Procédure SUMMARY

                               Variable d'analyse : WEIGHT WEIGHT

                N         Moyenne      Ecart-type         Minimum         Maximum
              -------------------------------------------------------------------
              180       4.2589445       0.6195011       2.9199999       5.7500000
              -------------------------------------------------------------------


------------------------------------- PARITY=One sibling --------------------------------------

                               Variable d'analyse : WEIGHT WEIGHT

                N         Moyenne      Ecart-type         Minimum         Maximum
              -------------------------------------------------------------------
              192       4.3886979       0.5925823       3.1699999       6.3299999
              -------------------------------------------------------------------


-------------------------------------- PARITY=2 siblings --------------------------------------

                               Variable d'analyse : WEIGHT WEIGHT

                N         Moyenne      Ecart-type         Minimum         Maximum
              -------------------------------------------------------------------
              116       4.4600862       0.6051999       3.0900000       6.4899998
              -------------------------------------------------------------------


---------------------------------- PARITY=3 or more siblings ----------------------------------

                               Variable d'analyse : WEIGHT WEIGHT

                N         Moyenne      Ecart-type         Minimum         Maximum
              -------------------------------------------------------------------
               62       4.4341935       0.5352632       3.1999999       5.4800000
              -------------------------------------------------------------------
\end{verbatim}

L'ANOVA à un facteur se réalise comme dans les exercices précédents, à
l'aide par exemple de la commande \texttt{PROC GLM} et en fournissant la
variable réponse et la variable qualitative décrivant les groupes à
comparer.
\begin{verbatim}
PROC GLM DATA=weight; 
  CLASS parity; 
  MODEL weight=parity; 
RUN;
\end{verbatim}

\begin{verbatim}
                                       The GLM Procedure

Dependent Variable: WEIGHT   WEIGHT

                                           Somme des         Moyenne     Valeur
      Source                     DDL          carrés     quadratique          F    Pr > F

      Model                        3       3.4769599       1.1589866       3.24    0.0219

      Error                      546     195.3648793       0.3578111

      Corrected Total            549     198.8418392


                                   Coef de
                     R-carré           Var    Racine MSE    WEIGHT Moyenne

                    0.017486      13.69940      0.598173          4.366418


                                                             Moyenne     Valeur
      Source                     DDL       Type I SS     quadratique          F    Pr > F

      PARITY                       3      3.47695990      1.15898663       3.24    0.0219


                                                             Moyenne     Valeur
      Source                     DDL     Type III SS     quadratique          F    Pr > F

      PARITY                       3      3.47695990      1.15898663       3.24    0.0219
\end{verbatim}

Pour tester l'homogénéité des variance, il est nécessaire de rajouter
l'option \verb|/ HOVTEST| dans une instruction \texttt{MEANS}, comme indiqué
ci-dessous :
\begin{verbatim}
PROC GLM DATA=weight; 
  CLASS parity; 
  MODEL weight=parity;
  MEANS parity / HOVTEST;
RUN;
\end{verbatim}
Voici les résultats additionnels concernant le test d'homoskédasticité.
\begin{verbatim}
                                       The GLM Procedure

                       Levene's Test for Homogeneity of WEIGHT Variance
                         ANOVA of Squared Deviations from Group Means

                                      Somme
                                        des        Moyenne     Valeur
               Source       DDL      carrés    quadratique          F    Pr > F

               PARITY         3      0.4731         0.1577       0.70    0.5552
               Error        546       123.8         0.2268
\end{verbatim}
Par défaut, le test utilisé est un test de Levene. Pour obtenir un test de
Bartlett, il suffit de modifier l'option ainsi : \verb|HOVTEST=BARTLETT|.

Pour le recodage de la variable \texttt{parity} en 3 classes, voici une
solution possible sous \SAS :
\begin{verbatim}
DATA weight1; SET weight; Newparity=parity+1-1; RUN;

PROC FREQ DATA=weight1; TABLES newparity; RUN;

DATA weight2; SET weight1;
  Nparity=newparity; 
  IF newparity GE 3 THEN Nparity=3;
RUN;

PROC FREQ DATA=weight2; TABLES Nparity; RUN;
\end{verbatim}

\begin{verbatim}
                                       The GLM Procedure

                 Niveau de                      ------------WEIGHT-----------
                 PARITY                   N          Moyenne       Ecart-type

                 2 siblings             116       4.46008618       0.60519991
                 3 or more siblings      62       4.43419353       0.53526321
                 One sibling            192       4.38869792       0.59258231
                 Singleton              180       4.25894446       0.61950106
\end{verbatim}

\begin{verbatim}
                                        Procédure FREQ

                                                          Fréquence    Pctage.
                 Nparity    Fréquence     Pourcentage     cumulée      cumulé
                 -------------------------------------------------------------
                       1         180         32.73             180      32.73
                       2         192         34.91             372      67.64
                       3         178         32.36             550     100.00
\end{verbatim}

Le modèle d'analyse de variance peut être à nouveau estimé sur ces nouvelles
données (\texttt{weight2}) avec \texttt{PROC GLM}.
\begin{verbatim}
PROC GLM DATA=weight2; 
  CLASS Nparity; 
  MODEL weight=Nparity;
  MEANS Nparity / HOVTEST;
RUN;
\end{verbatim}

\begin{verbatim}
                                       The GLM Procedure

Dependent Variable: WEIGHT   WEIGHT

                                           Somme des         Moyenne     Valeur
      Source                     DDL          carrés     quadratique          F    Pr > F

      Model                        2       3.4498715       1.7249358       4.83    0.0083

      Error                      547     195.3919677       0.3572065

      Corrected Total            549     198.8418392


                                   Coef de
                     R-carré           Var    Racine MSE    WEIGHT Moyenne

                    0.017350      13.68782      0.597668          4.366418


                                                             Moyenne     Valeur
      Source                     DDL       Type I SS     quadratique          F    Pr > F

      Nparity                      2      3.44987154      1.72493577       4.83    0.0083


                                                             Moyenne     Valeur
      Source                     DDL     Type III SS     quadratique          F    Pr > F

      Nparity                      2      3.44987154      1.72493577       4.83    0.0083

                                       The GLM Procedure

                       Levene's Test for Homogeneity of WEIGHT Variance
                         ANOVA of Squared Deviations from Group Means

                                       Somme
                                         des        Moyenne     Valeur
               Source        DDL      carrés    quadratique          F    Pr > F

               Nparity         2      0.2054         0.1027       0.45    0.6368
               Error         547       124.4         0.2274
\end{verbatim}

Le test de tendance linéaire peut être réalisé à partir d'une régression
linéaire simple. On peut utiliser \texttt{PROC GLM}, comme dans le cas de
l'ANOVA, mais en omettant l'instruction \texttt{CLASS} qui indique à \SAS de
traiter la variable explicative comme une variable qualitative.
\begin{verbatim}
PROC GLM DATA=weight2;  
  MODEL weight=Nparity;
RUN;
\end{verbatim}

\begin{verbatim}
                                       The GLM Procedure

Dependent Variable: WEIGHT   WEIGHT

                                           Somme des         Moyenne     Valeur
      Source                     DDL          carrés     quadratique          F    Pr > F

      Model                        1       3.3080082       3.3080082       9.27    0.0024

      Error                      548     195.5338310       0.3568136

      Corrected Total            549     198.8418392


                                   Coef de
                     R-carré           Var    Racine MSE    WEIGHT Moyenne

                    0.016636      13.68029      0.597339          4.366418


                                                             Moyenne     Valeur
      Source                     DDL       Type I SS     quadratique          F    Pr > F

      Nparity                      1      3.30800821      3.30800821       9.27    0.0024


                                                             Moyenne     Valeur
      Source                     DDL     Type III SS     quadratique          F    Pr > F

      Nparity                      1      3.30800821      3.30800821       9.27    0.0024


                                  Valeur                       Valeur
              Paramètre          estimée     Erreur type    du test t    Pr > |t|

              Intercept      4.174513379      0.06797862        61.41      <.0001
              Nparity        0.096127177      0.03157065         3.04      0.0024
\end{verbatim}
\label{stop:sol9b}

%---------------------------------------------------------------- Séance 08 --
\chapter*{Semaine 10\markboth{Corrigés de la semaine 10}{}}
\label{start:sol10b}
\soln{\ref{exo:10.1}}
Les données tabulées sont disponibles dans un fichier texte que l'on peut
importer à l'aide d'une étape \texttt{DATA} avec une instruction
\texttt{INFILE} et en précisant que la première ligne du fichier contient le
nom des variables.
\begin{verbatim}
DATA cystic;
INFILE  "C:\data\cystic.dat" firstobs=2;
INPUT Sub Age Sex Height Weight BMP FEV RV FRC TLC PEmax;
RUN;
\end{verbatim}

Pour calculer la valeur du coefficient de corrélation de Bravias-Pearson, il
faut utiliser \texttt{PROC CORR} et renseigner les variables d'intérêt comme
indiqué ci-après. \SAS offre automatiquement une sortie graphique si l'on
spécifie une instruction de type \verb|ODS GRAPHICS ON| avant le lancement
de la procédure \texttt{CORR}. 
\begin{verbatim}
PROC CORR DATA=cystic fisher; VAR PEmax Weight;  RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure CORR

                               2  Variables :    PEmax    Weight


                                     Statistiques simples

  Variable           N       Moyenne    Ecart-type         Somme       Minimum       Maximum

  PEmax             25     109.12000      33.43691          2728      65.00000     195.00000
  Weight            25      38.44400      17.90901     961.10000      12.90000      73.80000


                        Coefficients de corrélation de Pearson, N = 25
                                  Proba > |r| sous H0: Rho=0

                                             PEmax        Weight

                              PEmax        1.00000       0.63629
                                                          0.0006

                              Weight       0.63629       1.00000
                                            0.0006


                Pearson Statistique de corrélation (Transformation z de Fisher)

                                                                                   Estimation
             Avec                        Corrélation         Z de   Ajustement            des
  Variable   variable          N   d'échantillonnage       Fisher       biaisé   corrélations

  PEmax      Weight           25             0.63629      0.75191      0.01326        0.62833

                Pearson Statistique de corrélation (Transformation z de Fisher)

                            Avec                                    p Value de
                 Variable   variableIntervalle de confiance à 95%     H0:Rho=0

                 PEmax      Weight         0.310222      0.819903       0.0004
\end{verbatim}

L'option \texttt{fisher} indique à \SAS que les calculs doivent utiliser une
transformation inverse de Fisher, ce qui fournit en retour une estimation de
l'intervalle de confiance à 95~\% pour le coefficent de corrélation entre
les variables \texttt{PEmax} et \texttt{Weight}.

La même procédure sera utilisée pour le test d'hypothèse avec comme
alternative $H_0: \rho=0.3$ : on rajoutera simplement une option
\texttt{RHO0=}.
\begin{verbatim}
PROC CORR DATA=cystic fisher (RHO0=0.3); VAR PEmax Weight; RUN;
\end{verbatim}
Une partie des résultats produits par \SAS est reproduite ci-après.
\begin{verbatim}
                Pearson Statistique de corrélation (Transformation z de Fisher)

                      Avec                                    ------H0:Rho=Rho0-----
           Variable   variableIntervalle de confiance à 95%         Rho0     p Value

           PEmax      Weight         0.310222      0.819903      0.30000      0.0408
\end{verbatim}


Pour afficher l'ensemble des diagrammes de dispersion, on utilise
\texttt{PROC COR} en spécifiant l'option graphique (\verb|PLOTS=matrix|). La
liste des variables d'étude est indiquée après \texttt{VAR}.
\begin{verbatim}
PROC CORR DATA=cystic PLOTS=matrix (NVAR=all histogram); VAR sex Age Height
Weight BMP FEV RV FRC TLC PEmax; RUN;
\end{verbatim}
Voici une partie des résultats retournés par \SAS.
\begin{verbatim}
                                        Procédure CORR

  10  Variables :    Sex      Age      Height   Weight   BMP      FEV      RV       FRC
                     TLC      PEmax


                                     Statistiques simples

  Variable           N       Moyenne    Ecart-type         Somme       Minimum       Maximum

  Sex               25       0.44000       0.50662      11.00000             0       1.00000
  Age               25      14.48000       5.05899     362.00000       7.00000      23.00000
  Height            25     152.80000      21.50000          3820     109.00000     180.00000
  Weight            25      38.44400      17.90901     961.10000      12.90000      73.80000
  BMP               25      78.28000      12.00528          1957      64.00000      97.00000
  FEV               25      34.72000      11.19717     868.00000      18.00000      57.00000
  RV                25     255.20000      86.01696          6380     158.00000     449.00000
  FRC               25     155.40000      43.71880          3885     104.00000     268.00000
  TLC               25     113.96000      16.97174          2849      81.00000     147.00000
  PEmax             25     109.12000      33.43691          2728      65.00000     195.00000


                       Coefficients de corrélation de Pearson, N = 25
                                  Proba > |r| sous H0: Rho=0

                          Sex           Age        Height        Weight           BMP

         Sex          1.00000      -0.16712      -0.16755      -0.19234      -0.13756
                                     0.4246        0.4234        0.3570        0.5120

         Age         -0.16712       1.00000       0.92605       0.90648       0.37776
                       0.4246                      <.0001        <.0001        0.0626

         Height      -0.16755       0.92605       1.00000       0.92211       0.44076
                       0.4234        <.0001                      <.0001        0.0274

                       Coefficients de corrélation de Pearson, N = 25
                                  Proba > |r| sous H0: Rho=0

                          FEV            RV           FRC           TLC         PEmax

         Sex         -0.52826       0.27135       0.18361       0.02636      -0.28857
                       0.0066        0.1895        0.3797        0.9005        0.1618

         Age          0.29449      -0.55194      -0.63936      -0.47341       0.61347
                       0.1530        0.0042        0.0006        0.0168        0.0011

         Height       0.31666      -0.56952      -0.62428      -0.45952       0.59922
                       0.1230        0.0030        0.0009        0.0208        0.0015
\end{verbatim}
Comme on peut le voir, \SAS fournit la valeur du coefficient de corrélation
pour chaque paire de variable, ainsi que le degré de signification pour le
test associé de nullité de la corrélation.

\includegraphics{./figs/sas_scattermatrix}

Pour les corrélations de Spearman, il suffit d'ajouter l'option \texttt{SPEARMAN} :
\begin{verbatim}
PROC CORR DATA=cystic SPEARMAN KENDALL; VAR sex Age Height Weight BMP FEV RV FRC TLC PEmax; RUN;
\end{verbatim}


La corrélation partielle entre les variables \texttt{PEmax} et
\texttt{Weight} en tenant compte de \texttt{Age} est obtenue en précisant
l'option \texttt{Partial} suivi du nom de la variable de contrôle.
\begin{verbatim}
PROC CORR DATA=cystic PLOTS=scatter(alpha=.33 .66); VAR PEmax Weight; PARTIAL age; RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure CORR

                          1 Variables partielles :    Age
                          2            Variables :    PEmax    Weight


                                     Statistiques simples

  Variable           N       Moyenne    Ecart-type         Somme       Minimum       Maximum

  Age               25      14.48000       5.05899     362.00000       7.00000      23.00000
  PEmax             25     109.12000      33.43691          2728      65.00000     195.00000
  Weight            25      38.44400      17.90901     961.10000      12.90000      73.80000

                                     Statistiques simples

                                           Variance    Ecart-type
                             Variable     partielle       partiel

                             Age
                             PEmax        727.57226      26.97355
                             Weight        59.67344       7.72486


                   Coefficients de corrélation partielle de Pearson, N = 25
                              Proba > |r| sous H0: Rho partiel =0

                                             PEmax        Weight

                              PEmax        1.00000       0.24047
                                                          0.2577

                              Weight       0.24047       1.00000
                                            0.2577
\end{verbatim}
% Le tercilage, réalisé à l'aide de l'option \verb|scatter(alpha=.33 .66)|
% permet de produire le graphqiue suivant.

% \includegraphics{./figs/sas_tercilage}

\soln{\ref{exo:10.2}} Le chargement des données ne pose pas de difficultés
particulières car celles-ci ont été exportées depuis Excel et sont au format
CSV. On veillera cependant à préciser le type de délimiteur de champ, ici un
point-virgule, lors de l'étape \texttt{DATA}.
\begin{verbatim}
PROC IMPORT OUT= WORK.QUETELET
            DATAFILE= "C:\data\quetelet.txt"
            DBMS=DLM REPLACE;
     DELIMITER='3B'x;
     GETNAMES=YES;
     DATAROW=2;
RUN;
\end{verbatim}

Le coefficient de corrélation liénaire entre les variables \texttt{pas} et
\texttt{qtt} peut être estimé, ainsi que son intervalle de confiance, avec
la procédure \texttt{PROC CORR} discutée à l'exercice précédent. L'option
\texttt{alpha=0.10} permet de spécifier le niveau de confiance.
\begin{verbatim}
PROC CORR DATA=quetelet fisher (alpha=0.10); VAR PAS QTT; RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure CORR

                               2  Variables :    PAS      QTT


                                     Statistiques simples

  Variable           N       Moyenne    Ecart-type         Somme       Minimum       Maximum

  PAS               32     144.53125      14.39755          4625     120.00000     180.00000
  QTT               32          3441     497.07807        110115          2368          4637


                        Coefficients de corrélation de Pearson, N = 32
                                  Proba > |r| sous H0: Rho=0

                                              PAS           QTT

                                PAS       1.00000       0.74200
                                                         <.0001

                                QTT       0.74200       1.00000
                                           <.0001


                Pearson Statistique de corrélation (Transformation z de Fisher)

                                                                                   Estimation
             Avec                        Corrélation         Z de   Ajustement            des
  Variable   variable          N   d'échantillonnage       Fisher       biaisé   corrélations

  PAS        QTT              32             0.74200      0.95492      0.01197        0.73658

                Pearson Statistique de corrélation (Transformation z de Fisher)

                            Avec                                    p Value de
                 Variable   variableIntervalle de confiance à 90%     H0:Rho=0

                 PAS        QTT            0.563205      0.847834       <.0001
\end{verbatim}

La commande \texttt{PROC REG} permet d'effectuer une régression linéaire
pour une variable réponse (placée en premier dans la liste des variables) et
une ou plusieurs variables explicatives. On l'utilise comme suit :
\begin{verbatim}
PROC REG DATA=quetelet; MODEL pas=qtt; RUN;
\end{verbatim}

\begin{verbatim}
                                         Procédure REG
                                        Modèle : MODEL1
                                  Variable dépendante : PAS

                          Nombre d'observations lues               32
                          Nombre d'observations utilisées          32


                                      Analyse de variance

                                              Somme des        Moyenne     Valeur
    Source                           DDL         carrés    quadratique          F    Pr > F

    Modèle                             1     3537.94574     3537.94574      36.75    <.0001
    Erreur                            30     2888.02301       96.26743
    Total sommes corrigées            31     6425.96875


                 Root MSE                  9.81160    R carré           0.5506
                 Moyenne dépendante      144.53125    R car. ajust.     0.5356
                 Coeff Var                 6.78856


                               Valeurs estimées des paramètres

                              Valeur estimée         Erreur       Valeur
          Variable     DDL    des paramètres           type    du test t    Pr > |t|

          Intercept      1          70.57640       12.32187         5.73      <.0001
          QTT            1           0.02149        0.00355         6.06      <.0001
\end{verbatim}
Par défaut, on obtient un tableau d'analyse de variance pour le modèle de
régression et un tableau des coefficients du modèle, ici l'ordonnée à
l'origine (70.58) et la pente de la droite de régression (21.49). Le test
de Student associé à la pente permet d'évaluer sa significativité au vu des
données. 

L'ajout d'une instruction \verb|ODS GRAPHICS ON| fournit des sorties
graphiques, incluant le diagramme de dispersion et la droite de régression
(avec intervalle de confiance) superposée.

\includegraphics{./figs/sas_regqtt1}

Si l'on souhaite faire apparaître distinctement les observations selon le statut fumeur ou non, il est possible d'utiliser directement des commandes graphiques séparées de la procédure \texttt{REG}. Voici un exemple avec \texttt{SGPLOT}.
\begin{verbatim}
PROC SGPLOT DATA=quetelet;
  SCATTER x=qtt y=pas / group=tab;
  REG x=qtt y=pas ;
RUN;
\end{verbatim}

\includegraphics{./figs/sas_regqtt2}

Pour restreindre les analyses précédentes au seul groupe des fumeurs (ou des
non-fumeurs), on procèdera exactement de la même manière mais en faisant
précéder les instruction \SAS pour la régression d'une étape \texttt{DATA}
limitant les observations au groupe d'intérêt (variable \texttt{tab}), par
exemple
\begin{verbatim}
DATA fumeur; SET quetelet; IF TAB=1; RUN;
PROC REG DATA=fumeur; MODEL pas=qtt; RUN;
\end{verbatim}

\begin{verbatim}
                                         Procédure REG
                                        Modèle : MODEL1
                                  Variable dépendante : PAS

                          Nombre d'observations lues               17
                          Nombre d'observations utilisées          17


                                      Analyse de variance

                                              Somme des        Moyenne     Valeur
    Source                           DDL         carrés    quadratique          F    Pr > F

    Modèle                             1     2088.16977     2088.16977      19.40    0.0005
    Erreur                            15     1614.30082      107.62005
    Total sommes corrigées            16     3702.47059


                 Root MSE                 10.37401    R carré           0.5640
                 Moyenne dépendante      147.82353    R car. ajust.     0.5349
                 Coeff Var                 7.01783


                               Valeurs estimées des paramètres

                              Valeur estimée         Erreur       Valeur
          Variable     DDL    des paramètres           type    du test t    Pr > |t|

          Intercept      1          79.25533       15.76837         5.03      0.0002
          QTT            1           0.02012        0.00457         4.40      0.0005
\end{verbatim}

\soln{\ref{exo:10.3}}
Puisque les données sont disponibles au format CSV \og classique\fg\
(utilisant la virgule comme séparateur de champ), on peut les importer très
simplement à l'aide de \texttt{PROC IMPORT}.
\begin{verbatim}
PROC IMPORT OUT= WORK.FRAMINGHAM
            DATAFILE= "C:\data\Framingham.csv"
            DBMS=CSV REPLACE;
     GETNAMES=YES;
     DATAROW=2;
RUN;
\end{verbatim}

% FIXME:
% en attente des données log-transformées

Pour représenter graphiquement les variations de la pression artérielle
(\texttt{sbp} en fonction de l'IMC (\texttt{bmi}), on peut utiliser un
diagramme de dispersion grâce à \texttt{PROC SGPLOT}.
\begin{verbatim}
PROC SGPLOT DATA=framingham;
  SCATTER x=bmi y=sbp / group=sex;
  REG  x=bmi y=sbp ;
 /* scatter x=bmi y=sbp / group=sex;
  REG  x=bmi  y=sbp ;   */
RUN;
\end{verbatim}

\includegraphics{./figs/sas_scattersbp}

Les analyses présentées ci-dessous se basent sur une approche par analyse de
covariance, où plusieurs modèles sont étudiés (modèle avec interaction entre
sexe et IMC, modèle sans interaction).
Dans un premier temps, on transformera les deux variables \texttt{bmi} et
\texttt{sbp} à l'aide d'une transformation logarithmique dans une étape
\texttt{DATA}.
\begin{verbatim}
DATA framingham;
SET framingham;
logBMI = Log(bmi);
Logsbp = Log(sbp);
RUN;
\end{verbatim}

Le modèle avec interaction s'exprime de la manière suivante dans
\texttt{PROC GLM} : \verb|sbp=bmi sex bmi*sex|, le terme à gauche du signe
\texttt{=} correspond à la variable réponse, et les variables à à droite
sont les variables explicatives du modèle, l'interaction entre deux
variables étant symbolisée par le symbole \texttt{*}.
\begin{verbatim}
PROC GLM DATA=framingham; CLASS sex;
MODEL  Logsbp=logBMI sex logBMI*sex;
RUN;
\end{verbatim}

\begin{verbatim}
                                       The GLM Procedure

Dependent Variable: Logsbp

                                           Somme des         Moyenne     Valeur
      Source                     DDL          carrés     quadratique          F    Pr > F

      Model                        3      14.6615562       4.8871854     213.28    <.0001

      Error                     4686     107.3769013       0.0229144

      Corrected Total           4689     122.0384574


                                     Coef de
                       R-carré           Var    Racine MSE    Logsbp Moyenne

                      0.120139      3.105104      0.151375          4.875039


                                                             Moyenne     Valeur
      Source                     DDL       Type I SS     quadratique          F    Pr > F

      logBMI                       1     14.09242303     14.09242303     615.00    <.0001
      sex                          1    ￼ 0.19026910      0.19026910       8.30    0.0040
      bmi*sex                      1    ￼ 0.37886405      0.37886405      16.53    <.0001


                                                             Moyenne     Valeur
      Source                     DDL     Type III SS     quadratique          F    Pr > F

      logBMI                       1     10.76099758     10.76099758      469.62    <.0001
      sex                          1      0.35458038    ￼ 0.35458038       15.47    <.0001
      bmi*sex                      1      0.37886405      0.37886405       16.63    <.0001
\end{verbatim}

\includegraphics{./figs/sas_sbpancova}

Le modèle sans interaction est spécifié sur le même principe, en omettant le
terme \texttt{bmi*sex}.
\begin{verbatim}
PROC GLM DATA=framingham; CLASS sex;
MODEL LOgsbp=logBMI sex;
RUN;
\end{verbatim}


\includegraphics{./figs/sas_sbpancova2}

Le modèle de régression stratifié par sexe ne pose pas de problème majeur,
et contrairement à R il n'est pas nécessaire de calculer les intervalles de
confiance pour les pentes avec une commande séparée car ceux-ci sont
directement fournis dans le tableau de résultats renvoyés par \SAS.
\begin{verbatim}
PROC SORT DATA=framingham; BY sex; RUN;
PROC REG DATA=framingham; MODEL Logsbp=logBMI/clb; BY sex; RUN;
\end{verbatim}

Voici les résultats produits par \SAS pour le premier niveau de la variable \texttt{sex} :
\begin{verbatim}
-------------------------------------------- sex=1 --------------------------------------------

                                         Procédure REG
                                        Modèle : MODEL1
                                  Variable dépendante : Logsbp

                   Nombre d'observations lues                           2049
                   Nombre d'observations utilisées                      2047
                   Nombre d'observations avec valeurs manquantes           2


                                      Analyse de variance

                                              Somme des        Moyenne     Valeur
    Source                           DDL         carrés    quadratique          F    Pr > F

    Modèle                             1        2.59013        2.59013     137.93    <.0001
    Erreur                          2045       38.40260        0.01878
    Total sommes corrigées          2046       40.99272


                 Root MSE                  0.13704    R carré           0.0632
                 Moyenne dépendante      ￼ 4.87353    R car. ajust.     0.0627
                 Coeff Var                ￼2.81183


                               Valeurs estimées des paramètres

                  Valeur estimée       Erreur     Valeur             Intervalle de confiance
  Variable   DDL  des paramètres         type  du test t  Pr > |t|           à 95 %

  Intercept    1         3.98804      0.07546      52.85    <.0001      3.84006      4.13603
  bmi          1         0.27265      0.02322      11.74    <.0001      0.22712      0.31817
\end{verbatim}


\soln{\ref{exo:10.4}} Dans un premier temps, il est nécessaire de construire
le tableau d'effectifs donné dans l'énoncé. Cela peut être réalisé à l'aide
d'une étape \texttt{DATA} comme indiqué ci-après.
\begin{verbatim}
DATA mucovisi;
INPUT infection traitement nombre;
cards;
0 1 157
1 1 52
0 0 119
1 0 103
;
RUN;

PROC FORMAT; VALUE ouinon 0="Non" 1="Oui";
             VALUE treat 0="Placebo (B)" 1="Traitement (A)";
RUN;
\end{verbatim}
On notera que l'on a ajouté des étiquettes plus informatives aux codes
numériques utilisés pour représenter les modalités des deux variables
binaires, \texttt{infection} et \texttt{traitement}.

Voici une manière de reproduire le tableau de l'énoncé avec le test du
$\chi^2$ associé (option \texttt{chisq}). On notera l'utilisation de
l'instruction \texttt{WEIGHT} pour indiquer à \SAS qu'il s'agit de données
groupées et que le tableau doit être pondéré par les effectifs reportés dans
la 3\ieme\ colonne des données (nommée \texttt{nombre} durant l'étape
\texttt{DATA}).
\begin{verbatim}
PROC FREQ DATA=mucovisi ORDER=data; TABLES traitement*infection/expected chisq; WEIGHT nombre ;
FORMAT infection ouinon. traitement treat.;
RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure FREQ

                               Table de traitement par infection

                           traitement      infection

                           Fréquence      ‚
                           Attendu        ‚
                           Pourcentage    ‚
                           Pctage en ligne‚
                           Pctage en col. ‚Non     ‚Oui     ‚  Total
                           ---------------ˆ--------ˆ--------ˆ
                           Traitement (A) ‚    157 ‚     52 ‚    209
                                          ‚ 133.84 ‚ 75.162 ‚
                                          ‚  36.43 ‚  12.06 ‚  48.49
                                          ‚  75.12 ‚  24.88 ‚
                                          ‚  56.88 ‚  33.55 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Placebo (B)    ‚    119 ‚    103 ‚    222
                                          ‚ 142.16 ‚ 79.838 ‚
                                          ‚  27.61 ‚  23.90 ‚  51.51
                                          ‚  53.60 ‚  46.40 ‚
                                          ‚  43.12 ‚  66.45 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Total               276      155      431
                                             64.04    35.96   100.00


                    Statistiques pour la table de traitement par infection

                  Statistique                       DDL      Valeur      Prob
                  -----------------------------------------------------------
                  Khi-2                               1     21.6401    <.0001
                  Test du rapport de vraisemblance    1     21.9537    <.0001
                  Khi-2 continuité ajustée            1     20.7159    <.0001
                  Khi-2 de Mantel-Haenszel            1     21.5899    <.0001
                  Coefficient Phi                            0.2241
                  Coefficient de contingence                 0.2187
                  V de Cramer                                0.2241
\end{verbatim}
On notera que les effectifs observés et théoriques sont reportés dans le
même tableau par \SAS.

En ajoutant l'option \texttt{relrisk} à l'instruction \texttt{TABLES}, on
obtient les mesures de risque classiquement retrouvées en épidémiologie.

Si l'on tient compte des données par centre, il est nécessaire de
reconstruire les tableaux d'effectifs, en ne considérant que les marges
colonnes des tableaux d'effectifs donnés dans l'énoncé. Voici comment
procéder avec \SAS.
\begin{verbatim}
DATA mucocentres;
INPUT centre infection traitement nombre;
cards;
1 0 1 51
1 1 1 8
1 0 0 47
1 1 0 19
2 0 1 91
2 1 1 35
2 0 0 61
2 1 0 71
3 0 1 15
3 1 1 9
3 0 0 11
3 1 0 13
;
RUN;
\end{verbatim}

Pour réaliser un test de Mantel-Haenszel, on utilise l'ensemble des données
(3 tableaux $2\times 2$) et \texttt{PROC FREQ}.
\begin{verbatim}
PROC FREQ DATA=mucocentres ORDER=data; TABLES centre*traitement*infection/cmh;
WEIGHT nombre;
FORMAT infection ouinon. traitement treat.;
RUN;
\end{verbatim}
Voici une partie des résultats renvoyés par \SAS.
\begin{verbatim}
                                        Procédure FREQ

                              Table 1 de traitement par infection
                                     Contrôle de centre=1

                           traitement      infection

                           Fréquence      ‚
                           Pourcentage    ‚
                           Pctage en ligne‚
                           Pctage en col. ‚Non     ‚Oui     ‚  Total
                           ---------------ˆ--------ˆ--------ˆ
                           Traitement (A) ‚     51 ‚      8 ‚     59
                                          ‚  40.80 ‚   6.40 ‚  47.20
                                          ‚  86.44 ‚  13.56 ‚
                                          ‚  52.04 ‚  29.63 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Placebo (B)    ‚     47 ‚     19 ‚     66
                                          ‚  37.60 ‚  15.20 ‚  52.80
                                          ‚  71.21 ‚  28.79 ‚
                                          ‚  47.96 ‚  70.37 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Total                98       27      125
                                             78.40    21.60   100.00


                    Statistiques descriptives pour traitement par infection
                                     Contrôle pour centre

            Statistique de Cochran-Mantel-Haenszel (Basé sur les scores de tables)

      Statistique    Hypothèse alternative                      DDL      Valeur      Prob
      -----------------------------------------------------------------------------------
           1         Corrélation non nulle                        1     23.0087    <.0001
           2         Différence des scores moyens des lignes      1     23.0087    <.0001
           3         Association générale                         1     23.0087    <.0001


                     Estimations du risque relatif commun (Ligne1/Ligne2)

        Type d'étude            Méthode                Valeurrvalle de confiance à 95 %
        -------------------------------------------------------------------------------
        Cas-Témoins             Mantel-Haenszel        2.7617       1.8147       4.2030
          (Rapport de cotes)    Logit                  2.7632       1.8150       4.2066

        Cohorte                 Mantel-Haenszel        1.4091       1.2213       1.6257
          (Risque Col1)         Logit                  1.3523       1.1819       1.5474

        Cohorte                 Mantel-Haenszel        0.5311       0.4052       0.6961
          (Risque Col2)         Logit                  0.5379       0.4112       0.7036


                                   Test de Breslow-Day pour
                               Homogénéité des rapports de cotes
                               ---------------------------------
                               Khi-2                      0.4727
                               DDL                             2
                               Pr > Khi-2                 0.7895


                             Taille totale de l'échantillon = 431
\end{verbatim}

\soln{\ref{exo:10.5}} Les données peuvent être enregistrées sous \SAS par
saisie manuelle dans une étape \texttt{DATA}, comme indiqué ci-après.
\begin{verbatim}
DATA sck;
INPUT ck pres abs;
cards;
0       2      88
40       13      26
80       30      8
120       30      5
160       21      0
200       19      1
240       18      1
280       13      1
320       19      0
360       15      0
400       7      0
440       8      0
480       35      0
;
RUN;
\end{verbatim}

Le nombre total de sujets correspond à la somme des valeurs dans les
variables \texttt{pres} et \texttt{abs}. Le plus simple est donc de faire la
somme de l'ensemble de ces valeurs pour avoir l'effectif total :
\begin{verbatim}
DATA sck1; SET sck; particip=pres+abs; RUN;
PROC SUMMARY DATA=sck1 print sum; VAR particip; RUN;
\end{verbatim}

\begin{verbatim}
                                       Procédure SUMMARY

                                Variable d'analyse : particip

                                                Somme
                                         ------------
                                          360.0000000
                                         ------------
\end{verbatim}

Concernant la représentation des fréquences relatives des variables
\texttt{pres} et \texttt{abs}, il est nécessaire de calculer ces deux
quantités.  Pour cela, on créé un nouveau tableau de données que l'on
appelera \texttt{sck2}.
\begin{verbatim}
DATA sck2; SET sck1; p_pres=pres/particip; p_abs=abs/particip; RUN;
\end{verbatim}
Ensuite, il est possible d'utiliser \texttt{PROC SGPLOT} comme dans l'un des
exercices précédents, en spécifiant ce que l'on souhaite faire figurer sur
chacun des deux axes (abscisses et ordonnées du graphique en deux
dimensions).
\begin{verbatim}
PROC SGPLOT DATA=sck2 ;
   SERIES x=ck y=p_pres;
   SERIES x=ck y=p_abs;
RUN;
\end{verbatim}

\includegraphics{./figs/sas_ppres}

Pour le modèle de régression logistique, il est possible d'utiliser la
procédure \texttt{LOGISTIC} avec la syntaxe indiquée ci-après. Comme on
souhaite exploiter les valeurs prédites par le modèle par la suite, il est
nécessaire de les sauvegarder et pour cela on ajoute une option
\texttt{OUTPUT}.
\begin{verbatim}
PROC LOGISTIC DATA=sck2; MODEL pres/particip=ck;
OUTPUT out=fic1 pred=p1;
RUN;
\end{verbatim}

\begin{verbatim}
                                      Procédure LOGISTIC

                                  Informations sur le modèle

                     Table                                 WORK.SCK2
                     Variable de réponse (Evénements)      pres
                     Variable de réponse (Expériences)     particip
                     Modèle                                logit binaire
                     Technique d'optimisation              Score de Fisher


                            Nombre d'observations lues           13
                            Nombre d'observations utili          13
                            Somme des fréquences lues           360
                            Somme des fréquences utilis         360


                                       Profil de réponse

                                Valeur     Résultat     Fréquence
                              ordonnée     binaire         totale

                                     1     Evénemen           230
                                     2     Non-évén           130


                                 Etat de convergence du modèle

                        Critère de convergence (GCONV=1E-8) respecté.


                              Statistiques d'ajustement du modèle

                                                          Constante
                                          Constante              et
                            Critère      uniquement     covariables

                            AIC             472.919         191.773
                            SC              476.806         199.545
                            -2 Log L        470.919         187.773


                          Test de l'hypothèse nulle globale : BETA=0

                    Test                      Khi-2      DDL     Pr > Khi-2

                    Rapp. de vrais.        283.1466        1         <.0001
                    Score                  159.1423        1         <.0001
                    Wald                    73.9842        1         <.0001

                    Estimations par l'analyse du maximum de vraisemblance

                                   Valeur      Erreur         Khi-2
             Paramètre    DDL     estimée        type       de Wald    Pr > Khi-2

             Intercept      1     -2.3263      0.2994       60.3851        <.0001
             ck             1      0.0351     0.00408       73.9842        <.0001


                              Estimations des rapports de cotes

                                    Valeur
                                   estimée    Intervalle de confiance
                         Effet    du point         de Wald à 95 %

                         ck          1.036       1.027          1.044
\end{verbatim}

On peut d'ailleurs vérifier les valeurs prédites en affichant simplement la
variable \texttt{p1} créée à l'étape précédente.
\begin{verbatim}
PROC PRINT DATA=fic1; VAR ck p1; RUN;
\end{verbatim}

\begin{verbatim}
                                     Obs     ck       p1

                                       1      0    0.08897
                                       2     40    0.28453
                                       3     80    0.61824
                                       4    120    0.86833
                                       5    160    0.96410
                                       6    200    0.99094
                                       7    240    0.99776
                                       8    280    0.99945
                                       9    320    0.99986
                                      10    360    0.99997
                                      11    400    0.99999
                                      12    440    1.00000
                                      13    480    1.00000
\end{verbatim}

\begin{verbatim}
proc logistic data=sck2 ; model pres/particip= ck;
 score out=score1;
run;
proc logistic data=sck2 outmodel=sasuser.ckmodel; model pres/particip= ck;
 score data=sck2 out=score2;
run;
data ck3;
input ck;
cards;
0
5
10
15
20
25
30
35
40
45
50
55
60
65
66
67
68
69
70
71
75
80
85
90
95
100
110
;
run;

proc logistic inmodel=sasuser.Ckmodel;
   score data=Ck3 out=Score3;
run;
\end{verbatim}

% FIXME:
% expliquer ce que fait le code ci-dessus

Pour afficher sur un même graphique les proportions empiriques (malades) et
les valeurs prédites par le modèle, on utilise une fois encore \texttt{PROC SGPLOT}.
\begin{verbatim}
PROC SGPLOT DATA=fic1;
 SCATTER x=ck y=p_pres;
 SCATTER x=ck y=p1;
RUN;
\end{verbatim}

\includegraphics{./figs/sas_logisticpred}

Enfin, pour la courbe ROC, on utilise exactement la même syntaxe que dans le
cas du modèe de régression logistique mais on ajoute l'option
\texttt{ROC}. La valeur de l'aire sous la courbe et son intervalle de
confiance à 95~\% sont retournés par \SAS qui génère également le graphique
correspondant.
\begin{verbatim}
PROC LOGISTIC DATA=sck2 PLOTS=roc(id=prob); MODEL pres/particip=ck;
ROC ck;
RUN;
\end{verbatim}

\begin{verbatim}
                                      Procédure LOGISTIC
                                       Modèle ROC : ROC1

                              Estimations des rapports de cotes

                                    Valeur
                                   estimée    Intervalle de confiance
                         Effet    du point         de Wald à 95 %

                         ck          1.036       1.027          1.044


                                Statistiques d'association ROC

           ---------------- Mann-Whitney ---------------
                        Erreur   Intervalle de confiance   D de Somers
  ROC          Zone       type        de Wald à 95 %            (Gini)      Gamma      Tau-a

  Modèle     0.9593    0.00987     0.9399         0.9786        0.9185     0.9488     0.4250
  ROC1       0.9593    0.00987     0.9399         0.9786        0.9185     0.9488     0.4250
\end{verbatim}

\includegraphics{./figs/sas_roc}

\soln{\ref{exo:10.6}} Les données ont été sauvegardées dans un format
compact (3 colonnes indiquant la présence ou non d'un cancer, le niveau de
consommation d'alcool, et les effectifs associés). Voici une manière de
stocker le tableau de données sous \SAS.
\begin{verbatim}
DATA alcool;
INPUT Conso Cancer nombre;
cards;
1 1 96
0 1 104
1 0 109
0 0 666
;
RUN;

PROC FORMAT; VALUE alcool 1=">= 80" 0="<80";
             VALUE cas    1="Cas" 0="Temoins" ;
RUN;
\end{verbatim}

La proportion d'individus à risque, c'est-à-dire ayant une consommation
journalière d'alcool $\ge 80$ g s'obtient à partir d'un simple tableau
d'effectifs croisant les variables \texttt{cancer} et \texttt{alcohol} (il
faut indiquer comment remplir les cellules en ajoutant une option
\texttt{weight}). \SAS fournira automatiquement les profils lignes,
c'est-à-dire les les fréquences relatives par ligne.
\begin{verbatim}
PROC FREQ DATA=alcool; TABLES cancer*conso/ RELRISK; WEIGHT nombre;
FORMAT conso alcool. cancer cas.;
RUN;
\end{verbatim}

\begin{verbatim}
                                        Procédure FREQ

                                   Table de Cancer par Conso

                           Cancer          Conso

                           Fréquence      ‚
                           Pourcentage    ‚
                           Pctage en ligne‚
                           Pctage en col. ‚<80     ‚>= 80   ‚  Total
                           ---------------ˆ--------ˆ--------ˆ
                           Temoins        ‚    666 ‚    109 ‚    775
                                          ‚  68.31 ‚  11.18 ‚  79.49
                                          ‚  85.94 ‚  14.06 ‚
                                          ‚  86.49 ‚  53.17 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Cas            ‚    104 ‚     96 ‚    200
                                          ‚  10.67 ‚   9.85 ‚  20.51
                                          ‚  52.00 ‚  48.00 ‚
                                          ‚  13.51 ‚  46.83 ‚
                           ---------------ˆ--------ˆ--------ˆ
                           Total               770      205      975
                                             78.97    21.03   100.00


                        Statistiques pour la table de Cancer par Conso

                        Estimations du risque relatif (Ligne1/Ligne2)

            Type d'étude                        Valeurtervalle de confiance à 95 %
            ----------------------------------------------------------------------
            Cas-témoins (rapport de cotes)      5.6401        4.0006        7.9515
            Cohorte (risque col. 1)             1.6526        1.4422        1.8937
            Cohorte (risque col. 2)             0.2930        0.2337        0.3673

                                 Taille de l'échantillon = 975
\end{verbatim}


Le modèle de régression logistique se formule ainsi sous \SAS ; encore une
fois, on indique par une option \texttt{WEIGHT} la pondération par les
effectifs des données groupées.
\begin{verbatim}
PROC LOGISTIC DATA=alcool;
 MODEL Cancer = conso;
 WEIGHT nombre;
RUN;
\end{verbatim}

\begin{verbatim}
                                      Procédure LOGISTIC

                                  Informations sur le modèle

                       Table                            WORK.ALCOOL
                       Variable de réponse              Cancer
                       Nombre de niveaux de réponse     2
                       Variable de pondération          nombre
                       Modèle                           logit binaire
                       Technique d'optimisation         Score de Fisher


                            Nombre d'observations lues            4
                            Nombre d'observations utili           4
                            Somme des poids lus                 975
                            Somme des poids utilisés            975


                                       Profil de réponse

                       Valeur                  Fréquence      Pondération
                     ordonnée       Cancer        totale           totale

                            1            0             2        775.00000
                            2            1             2        200.00000

                            La probabilité modélisée est Cancer=0.


                                 Etat de convergence du modèle

                        Critère de convergence (GCONV=1E-8) respecté.


                              Statistiques d'ajustement du modèle

                                                          Constante
                                          Constante              et
                            Critère      uniquement     covariables

                            AIC             991.488         897.056
                            SC              990.875         895.828
                            -2 Log L        989.488         893.056


                          Test de l'hypothèse nulle globale : BETA=0

                    Test                      Khi-2      DDL     Pr > Khi-2

                    Rapp. de vrais.         96.4328        1         <.0001
                    Score                  110.2554        1         <.0001
                    Wald                    97.4522        1         <.0001

                    Estimations par l'analyse du maximum de vraisemblance

                                   Valeur      Erreur         Khi-2
             Paramètre    DDL     estimée        type       de Wald    Pr > Khi-2

             Intercept      1      1.8569      0.1054      310.1652        <.0001
             Conso          1     -1.7299      0.1752       97.4522        <.0001


                              Estimations des rapports de cotes

                                    Valeur
                                   estimée    Intervalle de confiance
                         Effet    du point         de Wald à 95 %

                         Conso       0.177       0.126          0.250
\end{verbatim}
\label{stop:sol10b}

%---------------------------------------------------------------- Séance 11 --
\chapter*{Semaine 11\markboth{Corrigés de la semaine 11}{}}
\label{start:sol11b}

\soln{\ref{exo:11.1}} Le fichier de données est un fichier texte avec des
tabulations comme séparateur de champ. On peut l'importer sous \SAS en
utilisant une procédure \texttt{IMPORT}, l'option \texttt{DBMS} permettant
de spécifier le type de délimiteur de champ utilisé pour ce fichier de
données.
\begin{verbatim}
PROC IMPORT OUT= WORK.PBC
            DATAFILE= "C:\data\pbc.txt"
            DBMS=TAB REPLACE;
     GETNAMES=YES;
     DATAROW=2;
RUN;
PROC CONTENTS DATA=pbc; RUN;
PROC FORMAT; VALUE treat 1="Placebo" 2="DPCA";
             VALUE Stat 0="Vivant" 1="décédé";
RUN;
\end{verbatim}

On peut vérifier la proportion de patients décédés (\texttt{status}, 0 =
vivant et 1 = décédé) et leur répartition selon le groupe de traitement à
l'aide de tris simple et croisé. Les fréquences relatives par lignes ou
colonnes sont fournies directement par \SAS.
\begin{verbatim}
PROC FREQ DATA=pbc; TABLES status*rx; FORMAT status stat. rx treat.; RUN;
\end{verbatim}

\begin{verbatim}
                         Procédure FREQ

                     Table de status par rx

           status          rx

           Fréquence      ‚
           Pourcentage    ‚
           Pctage en ligne‚
           Pctage en col. ‚Placebo ‚DPCA    ‚  Total
           ---------------ˆ--------ˆ--------ˆ
           Vivant         ‚     93 ‚     94 ‚    187
                          ‚  29.81 ‚  30.13 ‚  59.94
                          ‚  49.73 ‚  50.27 ‚
                          ‚  58.86 ‚  61.04 ‚
           ---------------ˆ--------ˆ--------ˆ
           décédé         ‚     65 ‚     60 ‚    125
                          ‚  20.83 ‚  19.23 ‚  40.06
                          ‚  52.00 ‚  48.00 ‚
                          ‚  41.14 ‚  38.96 ‚
           ---------------ˆ--------ˆ--------ˆ
           Total               158      154      312
                             50.64    49.36   100.00
\end{verbatim}

Pour afficher la distribution des temps de suivi, on utilisera un simple
diagramme de dispersion comme on l'a vu dans le cas de \R. Pour faire
apparaître distinctement les observations selon le status (0 ou 1), on
ajoutera une option \texttt{group} dans la procédure \texttt{SGPLOT}.
\begin{verbatim}
PROC SGPLOT DATA=pbc;
  SCATTER x=years  y=number / group=status;
FORMAT status stat. ;
RUN;
\end{verbatim}

\includegraphics{./figs/sas_followup}

La médiane de la durée de suivi par groupe de traitement peut s'obtenir à
l'aide de \texttt{PROC SORT} en opérant par groupe grâce à l'option
\texttt{BY}.
\begin{verbatim}
PROC SORT DATA=pbc; BY rx; RUN;
PROC SUMMARY DATA=pbc PRINT median; VAR years; BY rx; FORMAT rx treat.;
\end{verbatim}

\begin{verbatim}
-------------------------- rx=Placebo --------------------------

                       Procédure SUMMARY

                   Variable d'analyse : years

                               Médiane
                          ------------
                             5.1882000
                          ------------


--------------------------- rx=DPCA ----------------------------

                   Variable d'analyse : years

                               Médiane
                          ------------
                             4.9582500
                          ------------
\end{verbatim}

Le nombre de décès enregistrés au-delà de 10.5 années de suivi s'obtient
avec un simple tri à plat (\texttt{PROC FREQ}) ; il est cependant nécessaire
de créer une variable binaire codant pour la condition "années de suivi
$>10.5$ ou non".
\begin{verbatim}
DATA pbc ; SET pbc;
EVE=0; if years GE 10.5 then EVE=1;
RUN;
PROC FORMAT; VALUE pos 0="Suivi < 10.5"   1="Suivi >= 10.5"; RUN;
PROC FREQ DATA=pbc; TABLES EVE*status; FORMAT eve pos. status stat.; RUN;
\end{verbatim}

\begin{verbatim}
                         Procédure FREQ

                    Table de EVE par status

           EVE             status

           Fréquence      ‚
           Pourcentage    ‚
           Pctage en ligne‚
           Pctage en col. ‚Vivant  ‚décédé  ‚  Total
           ---------------ˆ--------ˆ--------ˆ
           Suivi < 10.5   ‚    164 ‚    121 ‚    285
                          ‚  52.56 ‚  38.78 ‚  91.35
                          ‚  57.54 ‚  42.46 ‚
                          ‚  87.70 ‚  96.80 ‚
           ---------------ˆ--------ˆ--------ˆ
           Suivi >= 10.5  ‚     23 ‚      4 ‚     27
                          ‚   7.37 ‚   1.28 ‚   8.65
                          ‚  85.19 ‚  14.81 ‚
                          ‚  12.30 ‚   3.20 ‚
           ---------------ˆ--------ˆ--------ˆ
           Total               187      125      312
                             59.94    40.06   100.00
\end{verbatim}

Concernant l'analyse des patients transplantés, le plus simple est de créer
un nouveau tableau de données.
\begin{verbatim}
DATA transplant;
INPUT number @@;
tranpl=1;
cards;
5 105 111 120 125 158 183 241 246 247 254 263 264 265 274 288 291 295 297 345 361 362 375 380 383
;
RUN;
\end{verbatim}
Ensuite, les statistiques recherchées s'obtiennent simplement avec une
combinaison des procédures \texttt{MEANS}, \texttt{FREQ} et
\texttt{SUMMARY}.
\begin{verbatim}
PROC SORT DATA=transplant; BY number; RUN;

DATA all; MERGE pbc transplant; BY number; RUN;
DATA pbc_transplant; SET all; years_J=years*365;
if tranpl=1;
if sample=1;
RUN;

PROC MEANS DATA=pbc_transplant; VAR age; RUN;
PROC FREQ DATA=pbc_transplant; TABLES sex; RUN;
PROC SUMMARY DATA=pbc_transplant PRINT median; VAR years_J; RUN;
\end{verbatim}

% FIXME:
% not the same results as Stata or R? N=10 !!

La table de mortalité avec l'estimateur de la fonction de survie de
Kaplan-Meier s'obtient grâce à la commande \verb|PROC LIFETEST|.
\begin{verbatim}
PROC LIFETEST DATA=pbc;
 time years*status(0);
RUN;
\end{verbatim}
Voici un aperçu partiel des résultats fournis par \SAS :
\begin{verbatim}
                       Procédure LIFETEST

              Estimations de survie de Kaplan-Meier

                                       Erreur
                                      type de        Nombre
    years     Survie    Défaillance    survie     ayant échoué

   0.0000      1.0000           0            0          0
   0.1123      0.9968     0.00321      0.00320          1
   0.1396      0.9936     0.00641      0.00452          2
   0.1944      0.9904     0.00962      0.00552          3
   0.2108      0.9872      0.0128      0.00637          4
   0.3012      0.9840      0.0160      0.00711          5
\end{verbatim}

La survie médiane est présentée sous la forme du 2\ieme\ quartile
(Pourcentage 50), avec son intervalle de confiance à 95~\%.
\begin{verbatim}
      Statistiques descriptives pour variable temps years

                     Estimations du quartile

                                 Valeur estimée
                 Pourcentage       du point

                          75              .
                          50             9.2950
                          25             4.0712

                     Estimations du quartile

                 Intervalle de confiance à 95 %
           Transformation    [Inférieur    Supérieur)

           LOGLOG               11.4743         .
           LOGLOG                8.4490       10.5106
           LOGLOG                3.2033        5.2704
\end{verbatim}

La courbe de survie s'obtient quant à elle à l'aide en ajoutant une option
\verb|plot| à la commande précédente.
\begin{verbatim}
PROC LIFETEST DATA=pbc PLOT=survival(cl);
 time years*status(0);
RUN;
\end{verbatim}

\includegraphics{./figs/sas_kmyears}

L'analyse stratifiée par groupe de traitement suit le même principe, et il
suffit d'ajouter une instruction \verb|strata| lors de l'appel à la commande
\verb|PROC LIFETEST|, comme indiqué ci-après.
\begin{verbatim}
PROC LIFETEST DATA=pbc PLOT=survival(cl);
 time years*status(0);
 strata rx;
RUN;
\end{verbatim}
Les résultats produits par \SAS sont affichés pour chacun des deux niveaux
de la variable \texttt{rx}. Voici un aperçu pour le cas \texttt{rx=1}.
\begin{verbatim}
                       Procédure LIFETEST

                       Strate 1 : rx = 1

              Estimations de survie de Kaplan-Meier

                                       Erreur
                                      type de        Nombre
    years     Survie    Défaillance    survie     ayant échoué

   0.0000      1.0000           0            0          0
   0.1123      0.9937     0.00633      0.00631          1
   0.1944      0.9873      0.0127      0.00889          2
   0.3587      0.9810      0.0190       0.0109          3
   0.3833      0.9747      0.0253       0.0125          4
   0.4901      0.9684      0.0316       0.0139          5
\end{verbatim}
Voici les résultats concernant les médianes de survie pour les deux
groupes. Pour le groupe \texttt{rx=1} (Placebo) :
\begin{verbatim}
      Statistiques descriptives pour variable temps years

                     Estimations du quartile

                                 Valeur estimée
                 Pourcentage       du point

                          75              .
                          50             8.9856
                          25             4.3149

                     Estimations du quartile

                 Intervalle de confiance à 95 %
           Transformation    [Inférieur    Supérieur)

           LOGLOG               11.1677         .
           LOGLOG                6.9541       11.4743
           LOGLOG                3.1540        5.6975
\end{verbatim}
Pour le groupe \texttt{rx=2} (DPCA) :
\begin{verbatim}
      Statistiques descriptives pour variable temps years

                     Estimations du quartile

                                 Valeur estimée
                 Pourcentage       du point

                          75              .
                          50             9.3854
                          25             3.9069

                     Estimations du quartile

                 Intervalle de confiance à 95 %
           Transformation    [Inférieur    Supérieur)

           LOGLOG               10.5489         .
           LOGLOG                8.4600       10.5489
           LOGLOG                2.4367        6.7515
\end{verbatim}

Les courbes de Kaplan-Meier correspondantes sont reproduites ci-dessous.

\includegraphics{./figs/sas_kmyears2}

Concernant le test du log-rank, il est fourni par la même commande
\verb|PROC LIFETEST| en ajoutant l'option \verb|test| suivi du facteur
d'intérêt.
\begin{verbatim}
PROC LIFETEST DATA=pbc PLOT=survival(cl);
 time years*status(0);
 test rx;
RUN;
\end{verbatim}
Par défaut, \SAS affiche le résultat du test du log-rank, mais également du
test de Gehan-Wilcoxon :
\begin{verbatim}
          Test d'égalité sur niveaux de discrétisation

                                              Pr >
           Test        Khi-2        DDL      Khi-2

           Log-rang      0.1017       1      0.7498
           Wilcoxon      0.0018       1      0.9664
           -2Log(LR)     0.0634       1      0.8013
\end{verbatim}

Pour recoder la variable \texttt{age} en variable qualitative à 3 classes,
on peut utiliser une simple étape \verb|DATA|, comme indiqué ci-après.
\begin{verbatim}
DATA pbc; SET pbc;
   age_classe=1; IF age GT 40 THEN age_classe=2; IF age GT 55 THEN age_classe=3;
PROC PRINT; VAR age age_classe; RUN;
\end{verbatim}
La commande \verb|PROC PRINT| permet de vérifier que les valeurs numériques
ont bien associées à la bonne classe pour la nouvelle variable
\verb|age_classe|. Le test du log-rank s'obtient en utilisant le même
principe que celui exposé plus haut, c'est-à-dire en ajoutant une
instruction \verb|test| dans \verb|PROC LIFETEST| suivi du facteur de
stratification \verb|strata|.

Enfin, le modèle de Cox se réalise grâce à la commande \verb|PROC PHREG|. 
Comme dans le cas des autres modèles de régression, l'option
\verb|model| permet de spécifier la relation entre la variable réponse (ici,
des données de survie représentées par l'instruction \verb|years*status(0)|)
et la ou les variables explicatives, ici \texttt{rx}. Dans le modèle
ci-dessous, on n'inclut pas la variable de stratification \verb|age_classe|.
\begin{verbatim}
PROC PHREG data=pbc; MODEL years*status(0) = rx; RUN;
\end{verbatim}
Les résultats renvoyés par \SAS sont reportés ci-dessous.
\begin{verbatim}
                      The PHREG Procedure

                  Informations sur le modèle

               Data Set                 WORK.PBC
               Dependent Variable       years
               Censoring Variable       status
               Censoring Value(s)       0
               Ties Handling            BRESLOW


            Number of Observations Read         312
            Number of Observations Used         312


  Récapitulatif du nombre d'événements et de valeurs censurées

                                             Pourcentage
          Total    Evénement     Censuré         censuré

            312          125         187           59.94


                       Etat de convergence

         Convergence criterion (GCONV=1E-8) satisfied.


              Statistiques d'ajustement du modèle

                                Sans            Avec
            Critère      covariables     covariables

            -2 LOG L        1279.960        1279.858
            AIC             1279.960        1281.858
            SBC             1279.960        1284.686


          Test de l'hypothèse nulle globale : BETA=0

    Test                      Khi-2      DDL     Pr > Khi-2

    Likelihood Ratio         0.1017        1         0.7498
    Score                    0.1017        1         0.7498
    Wald                     0.1015        1         0.7500


     Estimations par l'analyse du maximum de vraisemblance

               Valeur estimée     Erreur
 Paramètre DDL des paramètres       type      Khi-2 Pr > Khi-2

 rx          1       -0.05709    0.17916     0.1015     0.7500

     Estimations par l'analyse du maximum de vraisemblance

                                  Rapport
                      Paramètre de risque

                      rx            0.945
\end{verbatim}
%
%
%
\soln{\ref{exo:11.2}} 
Pour importer les données, il est nécessaire de bien indiquer le type de
séparateur de champs dans la procédure d'importation. Ici, il s'agit
d'espaces, donc il faut indiquer \verb|DELIMITER=' 'x|.
\begin{verbatim}
PROC IMPORT OUT= WORK.Prostate
            DATAFILE= "C:\data\prostate.dat"
            DBMS=DLM REPLACE;
     DELIMITER=' 'x;
     GETNAMES=YES;
     DATAROW=2;
RUN;
\end{verbatim}
Le reste des commandes est à peu près comparable à celles utilisées dans
l'exercice précédent. La commande \verb|PROC LIFETEST| permet de fournir une
estimation de la fonction de survie, en spécifiant via l'option \verb|TIME|
les informations permettant d'identifier les variables de durée
(\texttt{time}) et le status (\texttt{status}).
\begin{verbatim}
PROC LIFETEST DATA=prostate;
 TIME time*status(0);
RUN;
\end{verbatim}

En présence d'une variable de stratification, on ajoute l'option
\verb|STRATA|, suivie du nom de la variable de stratification.
\begin{verbatim}
PROC LIFETEST DATA=prostate;
 TIME time*status(0);
 STRATA treatment;
RUN;
\end{verbatim}

Enfin, pour afficher la courbe de survie, on ajoute l'option
\verb|PLOT=survival(cl)|. 
\begin{verbatim}
PROC LIFETEST DATA=prostate  plot=survival(cl) /*conftype=linear*/;
 TIME time*status (0);
 STRATA treatment;
RUN;
\end{verbatim}
\label{stop:sol11b}
