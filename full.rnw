
\documentclass[11pt]{report}
\usepackage[francais]{babel}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{fancyhdr}
\usepackage{url}
\usepackage{calc}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{parskip}
\usepackage{hyperref,wasysym}
\usepackage{multirow}
\usepackage{dcolumn}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{/}{D{/}{/}{-1}}
\usepackage{xspace}
\usepackage{wasysym}
\usepackage{enumitem}  % iterable item list
\usepackage{fancyvrb}

\usepackage[noae,nogin]{Sweave}
\SweaveOpts{prefix.string=figs,strip.white=all,keep.source=TRUE}

% bibliography stuff
\usepackage[style=verbose-trad2,natbib=true,backend=bibtex]{biblatex}
\bibliography{refs}

% Xelatex setup
\usepackage{fontspec,xltxtra,xunicode}
\usepackage[math-style=ISO,bold-style=ISO]{unicode-math}
\defaultfontfeatures{Scale=MatchLowercase}
\setmainfont{Myriad Pro}
%\setsansfont{Myriad Pro}
%\setmathfont[Numbers=OldStyle]{Asana Math}
%\setmonofont[Scale=0.86]{Inconsolata}
\setmonofont[Scale=0.71]{Menlo}

% page layout
\usepackage[left=2cm,top=2cm,bottom=2cm,right=2cm,nohead]{geometry}

\usepackage[french=guillemets*]{csquotes} 
\MakeOuterQuote{"} 

% we now need the following two import statements with TeXLive 2012 (updated
% Sep. 2012) -- don't know why.
\usepackage{xcolor}
\usepackage{textcomp}
% custom color for R code chunks
\definecolor{Sinput}{rgb}{0.3,0.3,0.3}
\definecolor{Soutput}{rgb}{0.5,0.5,0.5}
\definecolor{Scode}{rgb}{0,0.42,0.33}

% add line break when using paragraph sectioning
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
  {-3.25ex\@plus -1ex \@minus -.2ex}%
  {1.5ex \@plus .2ex}%
  {\normalfont\normalsize\bfseries}}
\makeatother

\addto\captionsfrancais{%
  \renewcommand\chaptername{Séance}}
\titleformat{\chapter}
  {\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter.}{20pt}{\huge}

\setlist[enumerate]{label=\fbox{\textbf{\arabic{chapter}.\arabic*}}}

% Manage solutions
\usepackage{answers}   % [nosolutionfiles]
\Newassociation{sol}{Solution}{solutions}
\renewcommand{\Solutionlabel}[1]{\fbox{#1}}
\newcommand{\soln}[1]{\vskip1ex\par\noindent\fbox{#1}\quad}

\theoremstyle{definition}
\newtheorem{exo}{Exercice}[chapter]
\newcommand{\R}{\textsf{R}\xspace}
\SetLabelAlign{parright}{\parbox[t]{\labelwidth}{\raggedleft#1}}

\setlist[description]{style=multiline,topsep=10pt,leftmargin=35pt,font=\normalfont,%
    align=parright}


% index
\usepackage{multicol}
\usepackage{makeidx}
\addto\captionsfrancais{%
\renewcommand*\indexname{Liste des commandes \R}}
\newcommand{\foo}[1]{\texttt{#1}}
\newcommand{\cmd}[1]{\index{#1@\foo{#1}}}
\newcommand{\blankpage}{
  \newpage
  \thispagestyle{empty}
  \mbox{}
  \newpage
  }


\makeindex


\begin{document}
\title{Méthodologie Statistique avec R et Stata\\
  Exercices corrigés\\ \vskip5em\centerline{\includegraphics[scale=.5]{cesam}}}
\author{}
\date{2012–2013}
\maketitle

   
% Sweave custom outputs
% could use R CMD pgfsweave --pgfsweave-only $< but it's too much color
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom = {\color{Sinput}}} 
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom = {\color{Soutput}},fontsize=\small}
\DefineVerbatimEnvironment{Scode}{Verbatim}
{formatcom = {\color{Sinput}}} 
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}
%\renewcommand{\texttt}[1]{\textcolor{Sinput}{#1}}
\setkeys{Gin}{width=.4\textwidth}
\SweaveOpts{prefix.string=figs/fig}

<<echo=FALSE>>=
library(lattice)
library(gridExtra)
ltheme <- canonical.theme(color=FALSE)     
ltheme$strip.background$col <- "transparent"
lattice.options(default.theme=ltheme)
options(width=100, show.signif.stars=FALSE)
set.seed(101)
@ 

\blankpage

\tableofcontents

\blankpage

\pagestyle{plain}
\setcounter{page}{1} 
\chapter*{Organisation}

\section*{Introduction}

\section*{Objectif}

\section*{Déroulement des séances}

\section*{Conventions typographiques}
Dans la suite de ce document, les extraits de sessions R ou Stata sont
reconnaissables soit par le type de police utilisée -- \texttt{summary} ou
\texttt{summarize} désignent les commandes R et Stata permettant d'afficher
un résumé descriptif pour une variable numérique -- soit par le format de
paragraphe : les lignes commençant par \texttt{> } indiquent les commandes
entrées sous R (on ne saisira pas le symbole \texttt{>}) et celles
commençant par \texttt{. } les commandes entrées sous Stata. Les sorties R
ou Stata utilisent le même format de police mais ne sont préfixées par aucun
symbole. Par exemple,
\begin{verbatim}
> summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  2.264   2.640   3.166   3.411   4.100   4.958
\end{verbatim}
signifie que la commande \verb|summary(x)| a été tapée sous R, et R a
renvoyé le résultat affiché juste après. De même, sous Stata en tapant la
comamnde \verb|summarize x| on obtiendrait les résultats suivants :
\begin{verbatim}
. summarize x

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
           x |        10    2.555868    1.122793   .9779007   4.112015
\end{verbatim}

\section*{Obtenir de l'aide}
R et Stata possède tous deux un système d'aide en ligne permettant de
vérifier comment utiliser les commandes et d'inspecter les différentes
options disponibles. 

Sous R, il suffit d'utiliser la commande \texttt{help} suivi du nom de la
commande. Par exemple, \verb|help(rnorm)| renverra l'aide sur la commande
\texttt{rnorm}, dont un extrait est affiché ci-dessous.
\begin{Verbatim}[fontsize=\small]
Normal                  package:stats                  R Documentation

The Normal Distribution

Description:

     Density, distribution function, quantile function and random
     generation for the normal distribution with mean equal to ‘mean’
     and standard deviation equal to ‘sd’.

Usage:

     dnorm(x, mean = 0, sd = 1, log = FALSE)
     pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
     qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
     rnorm(n, mean = 0, sd = 1)
     
Arguments:

     x,q: vector of quantiles.

       p: vector of probabilities.

       n: number of observations. If ‘length(n) > 1’, the length is
          taken to be the number required.

    mean: vector of means.

      sd: vector of standard deviations.

log, log.p: logical; if TRUE, probabilities p are given as log(p).

lower.tail: logical; if TRUE (default), probabilities are P[X <= x]
          otherwise, P[X > x].
\end{Verbatim}
Le début de l'aide consiste généralement en un descriptif sommaire de la
commande, la manière de l'utiliser et les principales options. Un exemple
d'utilisation est généralement fourni à la fin de l'aide en ligne.

Sous Stata, il s'agit de la même commande, \texttt{help}, mais on n'utilise
pas de parenthèses. Par exemple, voici un extrait de l'aide renvoyée pour la
commande \texttt{drawnorm} après avoir tapé \verb|help drawnorm|.
\begin{Verbatim}[fontsize=\small]
Title
-----

    [D] drawnorm -- Draw sample from multivariate normal distribution


Syntax
------

        drawnorm newvarlist [, options]

    options                 Description
    -------------------------------------------------------------------------------------
    Main
      clear                 replace the current dataset
      double                generate variable type as double; default is float
      n(#)                  # of observations to be generated; default is current number
      sds(vector)           standard deviations of generated variables
      corr(matrix|vector)   correlation matrix
      cov(matrix|vector)    covariance matrix
      cstorage(full)        correlation/covariance structure is stored as a symmetric k*k matrix
      cstorage(lower)       correlation/covariance structure is stored as a lower triangular matrix
      cstorage(upper)       correlation/covariance structure is stored as an upper triangular matrix
      forcepsd              force the covariance/correlation matrix to be positive semidefinite
      means(vector)         means of generated variables; default is means(0)

    Options
      seed(#)               seed for random-number generator
    -------------------------------------------------------------------------------------


Menu
----

    Data > Create or change data > Other variable-creation commands > Draw sample from normal distribution
\end{Verbatim}
Les premières lignes indiquent généralement comment utiliser la commande
avec une ou plusieurs variables, les éventuels critères de sélection des
observations que l'on peut appliquer et les options disponibles. Un exemple
d'utilisation est généralement fourni à la fin de l'aide en ligne.


% Colophon
\vfill
\hfill{\rm\textcolor{Scode}{\scriptsize Version 1.0 (\today). \LaTeX\ +
    pdf\TeX\, R~\Sexpr{paste(R.Version()$major,R.Version()$minor,sep=".")},
    Stata 12.1 MP}.}

\blankpage


\part{Analyses statistiques avec R}
\chapter{Éléments du langage}\label{chap:langage}
\Opensolutionfile{solutions}[solutions1]

\begin{center} 
\fbox{\begin{minipage}{5in} 
\textbf{\large Objectifs :} 

\noindent
Manipuler des données simples (variable quantitative ou qualitative) avec R
• importer des fichiers de données externes • recoder les valeurs d'une
variable • identifier des sous-ensembles d'observations
\end{minipage}} 
\end{center}

\vspace*{3ex}

\section*{Énoncés}
%
%
%
\begin{exo}\label{exo:1.1}
Un chercheur a recueilli les mesures biologiques suivantes (unités
arbitraires) :
\begin{verbatim}
3.68  2.21  2.45  8.64  4.32  3.43  5.11  3.87
\end{verbatim}
\begin{description}
\item[(a)] Stocker la séquence de mesures dans une variable appelée
  \texttt{x}.  
\item[(b)] Indiquer le nombre d'observations (à l'aide de \R), les valeurs
  minimale et maximale, ainsi que l'étendue.  
\item[(c)] En fait, le chercheur réalise que la valeur 8.64 correspond à une
  erreur de saisie et doit être changée en 3.64. De même, il a un doute sur
  la 7\ieme mesure et décide de la considérer comme une valeur manquante :
  effectuer les transformations correspondantes. 
\end{description}
\begin{sol}
On commence par saisir les données :
<<ex1-1a>>=
x <- c(3.68,2.21,2.45,8.64,4.32,3.43,5.11,3.87)
x
@
Le nombre d'éléments de \texttt{x}, c'est-à-dire le nombre d'observations ou
unités statistiques, s'obtient ainsi :
<<ex1-1b>>=
length(x)
@ 
En ce qui concerne l'étendue de variation des mesures collectées, on peut
utiliser une commande générique, telle que \verb|summary(x)|, ou des
commandes plus spécifiques, comme indiqué ci-dessous :
<<ex1-1c>>=
min(x)
max(x)
range(x)
range(x)[2] - range(x)[1]  # ou diff(range(x))
@ 
\cmd{min}\cmd{max}\cmd{range}
On notera que \verb|range(x)| renvoit deux valeurs, correspondant dans
l'ordre de lecture au minimum et au maximum, de la variable \texttt{x}, ce
qui nous permet de calculer l'étendue comme la différence entre ces deux
valeurs. 

Concernant les transformations suggérées, on peut remplacer la 4\ieme
mesure, \verb|x[4]|, comme ceci : \verb|x[4] <- 3.64|. En fait cela
nécessite de connaître le numéro ou la position de l'observation, ce qui
se révèle peu pratique dans le cas où le nombre d'observations est
grand. Une solution alternative consiste donc à "isoler" l'observation en
question à l'aide de sa valeur (on parlera d'un test logique sur les valeurs
de \texttt{x}) :
<<ex1-1d>>=
x[x == 8.64] <- 3.64
x
@

Enfin, pour recoder la 7\ieme observation en valeur manquante, on procèdera
comme suit :
<<ex1-1e>>=
x[7] <- NA
x
@ 
\cmd{NA}
\end{sol}
\end{exo}
%
%
%
\begin{exo}\label{exo:1.2}
La charge virale plasmatique permet de décrire la quantité de virus (p.~ex.,
VIH) dans un échantillon de sang. Ce marqueur virologique qui permet de
suivre la progression de l’infection et de mesurer l’efficacité des
traitements est rapporté en nombre de copies par millilitre, et la plupart
des instruments de mesure ont un seuil de détectabilité de 50
copies/ml. Voici une série de mesures, $X$, exprimées en logarithmes (base 10)
collectées sur 20 patients :
\begin{verbatim}
3.64 2.27 1.43 1.77 4.62 3.04 1.01 2.14 3.02 5.62 5.51 5.51 1.01 1.05 4.19
2.63 4.34 4.85 4.02 5.92
\end{verbatim}
Pour rappel, une charge virale de 100 000 copies/ml équivaut à 5 log.
\begin{description}
\item[(a)] Indiquer combien de patients ont une charge virale considérée
  comme non-détectable. 
\item[(b)] Quelle est le niveau de charge virale médian, en copies/ml, pour
  les données considérées comme valides ?
\end{description}
\begin{sol}
Avant toute chose, il est nécessaire d'exprimer la limite de détection (50
copies/ml) en logarithmes ; celle-ci vaut en fait
<<ex1-2a>>=
log10(50)
@ 
\cmd{log10}
Ensuite, il suffit de filtrer les observations qui ne remplissent pas la
condition $X>1.70$ (on utilisera le résultat numérique exact, pas la valeur
approchée) :
<<ex1-2b>>=
X <- c(3.64,2.27,1.43,1.77,4.62,3.04,1.01,2.14,3.02,5.62,5.51,5.51,1.01,1.05,4.19,
       2.63,4.34,4.85,4.02,5.92)
length(X[X <= log10(50)])
@ 
\cmd{length}

Finalement, la charge virale médiane pour les 16 patients ayant une mesure
considérée comme valide peut se calculer comme suit :
<<ex1-2c>>=
Xc <- X[X > log10(50)]
round(median(10^Xc), 0)
@ 
\cmd{median}\cmd{round}
Notons que le résultat ci-dessus est présenté sans décimales.
\end{sol}
\end{exo}
%
%
%
\begin{exo}\label{exo:1.3}
Le fichier \texttt{dosage.txt} contient une série de 15 dosages biologiques,
stockés au format numérique avec 3 décimales, comme suit
\begin{verbatim}
6.379 6.683 5.120 ...
\end{verbatim}
\begin{description}
\item[(a)] Utiliser \verb|scan| pour lire ces données (bien lire l'aide en
  ligne concernant l'usage de cette commande, en particulier l'option
  \texttt{what=}). 
\item[(b)] Corriger la série de mesures afin de pouvoir calculer la moyenne
  arithmétique.   
\item[(c)] Enregistrer les données corrigées dans fichier texte appelé
  \texttt{data.txt}. 
\end{description}
\begin{sol}
Lorsque les données stockées dans un fichier texte ne sont pas trop
volumineuses, il est recommendé d'y jeter un oeil à l'aide d'un simple
éditeur de texte avant de les importer dans \R. Dans le cas présent, il
s'avère qu'il y a eu un problème de codage du séparateur décimal : pour une
des mesures, une virgule a été utilisée à la place d'un point pour séparer
la partie entière de la partie décimale des nombres.

Si l'on se contente de l'instruction suivante
<<ex1-3a, eval=FALSE>>=
x <- scan("dosage.txt")
@ 
\cmd{scan}
\R retournera un message d'erreur du type
\begin{verbatim}
Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  : 
  scan() attendait 'a real' et a reçu '2,914'
\end{verbatim}
puisque par défaut \R tente de lire des nombres (au format
anglo-saxon). Pour corriger cela, il faut donc demander à \R 
<<ex1-3b>>=
x <- scan("dosage.txt", what="character")
str(x)
head(x)
@ 
\cmd{scan}\cmd{str}\cmd{head}

Toutefois, il n'est pas possible de calculer directement la moyenne sur les
données importées de cette manière puisqu'il s'agit de caractères
(\texttt{character}). On corrigera donc l'observation mal enregistrée, avant
de convertir les mesures en nombres exploitables par \R :
<<ex1-3c>>=
x[x == "2,914"] <- "2.914"
x <- as.numeric(x)
head(x)
round(mean(x), 3)
@ 
\cmd{as.numeric}\cmd{head}\cmd{round}\cmd{mean}

Enfin, pour sauvegarder les données corrigées dans un nouveau fichier, on
peut utiliser la commande générale \texttt{write.table}, qui possède
beaucoup d'options, mais qui en générale fonctionne de la manière suivante :
<<ex1-3d>>=
write.table(x, file="data.txt", row.names=FALSE)
@ 
Un aperçu du fichier texte ainsi sauvegardé est fourni ci-dessous :
\begin{verbatim}
"x"
6.379
6.683
5.12
6.707
6.149
5.06
\end{verbatim}
Si l'on ne souhaite pas faire apparaître le nom de la variable sur la
première ligne, on rajoutera l'option \verb|col.names=FALSE|.
\end{sol}
\end{exo}
%
%
%
\begin{exo}\label{exo:1.4}
Lors d'une enquête épidémiologique, les données suivantes ont été collectées
(à partir d'un questionnaire retourné par voie postale) : l'âge (en années),
le sexe (M, masculin, F, féminin, ou T, transgenre), le niveau de QI (score
numérique, positif), le statut socio-économique (variable qualitative à
trois modalités, A, B et C), et un score de qualité de vie (exprimé sur une
échelle allant de 0 à 100 points). Un aperçu des données pour les 10
premiers individus est fourni ci-dessous :
\vskip1em

\begin{tabular}{cccccc}
\toprule
id & age & sexe & qi & sse & qdv \\
\midrule
1 &  26  &  M & 126 &  B & 72 \\
2 &  31  &  F & 123 &  A & 73 \\
3 &  28  &  M & 114 &  B & 72 \\
4 &  28  &  M & 125 &  B & 72 \\
5 &  29  &  F & 134 &  A & 76 \\
6 &  33  &  F & 141 &  B & 74 \\
7 &  32  &  F & 123 &  B & 72 \\
8 &  21  &  M & 114 &  A & 71 \\
9 &  36  &  M & 122 &  C & 71 \\
10 & 30  &  M & 127 &  A & 66 \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
  \item[(a)] Créer un \verb|data.frame|, nommé \texttt{dfrm}, pour stocker les données de
ces 10 individus.
  \item[(b)] L'ensemble de la base de données a été enregistré dans
un fichier Excel, puis exportée au format \textsf{CSV} (c'est-à-dire un
fichier texte dans lequel les données de chaque individu sont écrites sur
une même ligne, avec des virgules séparant les valeurs de chaque variable,
encore appelé "champs") sous le nom \texttt{enquete.csv} : charger le
fichier sous \R, et vérifier la concordance des données avec celles créées
au préalable.
\item[(c)] Indiquer la proportion d'hommes et de femmes dans cet échantillon.
\item[(d)] La variable qdv contient-elle des valeurs manquantes ? Si oui,
  combien et pour quels numéros d'observations (lignes du tableau de
  données) ?
\end{description}
\begin{sol}
Commençons par saisir les mesures correspondant à chacune des 6 variables
une par une :
<<ex1-4a>>=
id <- 1:10
age <- c(26,31,28,28,29,33,32,21,36,30)
sexe <- c("M","F","M","M","F","F","F","M","M","M")
qi <- c(126,123,114,125,134,141,123,114,122,127)
sse <- c("B","A","B","B","A","B","B","A","C","A")
qdv <- c(72,73,72,72,76,74,72,71,71,66)
dfrm <- data.frame(id, age, sexe, qi, sse, qdv)
dfrm
@
\cmd{data.frame}
Les variables intermédiaires (\texttt{id}, \texttt{age}, etc.) sont toujours
présentes dans l'espace de travail de R (ce que l'on peut vérifier en
tapant la commande \verb|ls()|), et on peut vouloir les supprimer pour
éviter toute confusion lors de leur manipulation puisqu'elles sont à présent
dispnibles directement depuis le \texttt{data.frame} nommé \texttt{dfrm}. On
utilisera la commande \texttt{rm} comme ceci :
<<ex1-4b>>=
rm(id, age, sexe, qi, sse, qdv)
ls()
@ 
\cmd{rm}\cmd{ls}
Pour exporter le fichier au format CSV, on utiliserait la commande
\texttt{write.csv} (séparateur de champ = virgule) ou \texttt{write.csv2}
(séparateur de champ = point-virgule).
<<ex1-4c, eval=FALSE>>=
write.csv(dfrm, file="dfrm.csv")
@ 
\cmd{write.csv}
Chargeons à présent les données réelles du fichier \texttt{enquete.csv}. 
<<ex1-4d>>=
enquete <- read.csv("enquete.csv")
str(enquete)
@ 
\cmd{read.csv}\cmd{str}
La commande \texttt{str} fournit un aperçu du type de chaque variable
(variable numérique ou qualitative) et des premières observations. On voit
ici que les variables \texttt{sexe} et \texttt{sse} sont bien considérées
comme des variables qualitatives (\texttt{factor} sous R). Pour afficher les
10 premières observations, on peut utiliser la commande \texttt{head}, par exemple
<<ex1-4e>>=
head(enquete, n=10)
@ 
\cmd{head}
ou alors directement indexer le tableau de données avec les 10 premiers
éléments : 
<<ex1-4f>>=
enquete[1:10,]
@
Ici, on indique entre crochets les observations qui nous intéressent (1 à
10, le \og à\fg\ se traduisant par \texttt{:} sous R) puis les colonnes
d'intérêt (ici, on ne met rien pour avoir l'ensemble des variables). On peut
ainsi vérifier la bonne concordance des valeurs.

La proportion d'hommes et de femmes peut être obtenue à l'aide de la
commande \texttt{table}, en tenant compte du nombre d'observations que l'on
peut peut calculer comme le nombre de lignes du tableau de données :
<<ex1-4g>>=
table(enquete$sexe) / nrow(enquete)
@ 
\cmd{table}
Notons que l'on aurait très bien pu compter chacune des modalités prises par
la variable \texttt{sexe}, soit \texttt{F} ou \texttt{H}, en se rappelant
que les classes ou modalités d'une variable qualitatives doivent être
entourées d'apostrophes anglo-saxonnes (\og quote\fg).
Par exemple, pour les femmes on pourrait procéder ainsi :
<<ex1-4h>>=
sum(enquete$sexe == "F") / nrow(enquete) 
@
\cmd{sum}\cmd{nrow}
Cette approche ne fonctionnera que s'il n'y a aucune donnée manquante car la
commande \texttt{nrow} renvoit la taille du tableau de données (nombre de
lignes) indépendemment de son contenu. Si l'on veut contrôler la présence de
valeurs manquantes, il est donc préférable d'utiliser la commande
\texttt{complete.cases}, soit
<<ex1-4i>>=
nrow(enquete)
sum(complete.cases(enquete$sexe))
sum(complete.cases(enquete$qdv))
@ 
\cmd{nrow}\cmd{sum}\cmd{complete.cases}
ce qui montre que pour la variable \texttt{qdv} il y a en réalité trois
données manquantes. La même information peut être obtenue à partir de la
commande plus générale \texttt{summary} :
<<ex1-4j>>=
summary(enquete)
@
\cmd{summary}
Pour identifier les observations manquantes pour la variable \texttt{qdv},
le plus simple est d'utiliser \texttt{which} en combinaison avec la commande
\texttt{is.na} qui renvoit vrai ou faux selon que la donnée est considérée
comme manquante ou non, soit :
<<ex1-4k>>=
which(is.na(enquete$qdv))
@ 
\cmd{which}\cmd{is.na}
\end{sol}
\end{exo}
\begin{exo}\label{exo:1.5}
Le fichier \texttt{anorexia.dat} contient les données d'une étude clinique
chez des patientes anorexiques ayant reçu l'une des trois thérapies
suivantes : thérapie comportementale, thérapie familiale, thérapie
contrôle.\autocite{hand93} 
\begin{description}
\item[(a)] Combien y'a-t-il de patientes au total ? Combien y'a-t-il de
  patientes par groupe de traitement ?
\item[(b)] Les mesures de poids sont en livres. Les convertir en
  kilogrammes.    
\item[(c)] Créer une nouvelle variable contenant les scores de différences
  (\texttt{After} - \texttt{Before}).
\item[(d)] Indiquer la moyenne et l'étendue (min/max) des scores de
  différences par groupe de traitement.
\end{description}
\begin{sol}
Un aperçu des données contenues dans le fichier \texttt{anorexia.dat} est
fourni ci-dessous (5 premières lignes du fichier) :
\begin{verbatim}
Group Before After
g1 80.5  82.2
g1 84.9  85.6
g1 81.5  81.4
g1 82.6  81.9
\end{verbatim}
On voit donc que la première ligne est une ligne d'en-tête indiquant le nom
des variables, et chacune des lignes suivantes représente une unité
statistique pour laquelle on a le groupe d'appartenance, la mesure de poids
avant le début de la prise en charge et la mesure de poids à la fin de la thérapie.
Pour importer ce type de données, on utilisera la commande
\texttt{read.table} en précisant l'option \verb|header=TRUE| pour bien
prendre en compte la ligne d'en-tête.
<<ex1-5a>>=
anorex <- read.table("anorexia.dat", header=TRUE)
names(anorex)
head(anorex)
@ 
Le nombre total de patientes correspond au nombre de lignes du tableau de
données :
<<ex1-5b>>=
nrow(anorex)
@ 
Il y a donc au total 72 patientes. Pour trouver la répartition des effectifs
par groupe, le plus simple est de faire un tri à plat de la variable
qualitative \texttt{Group} :
<<ex1-5c>>=
table(anorex$Group)
@ 
Pour convertir les poids exprimés en livres en kilogrammes, il suffit de
diviser chaque mesure par 2.2 (approximativement). On réalise cette
opération pour chacune des deux variables \texttt{Before} et \texttt{After}.
<<ex1-5d>>=
anorex$Before <- anorex$Before/2.2
anorex$After <- anorex$After/2.2
@ 
On notera que dans ce cas-là, les valeurs d'origine de ces deux variables
seront simplement remplacées. Si l'on souhaite y avoir accès de nouveau, il
faudra recharger le fichier. Une autre solution aurait consisté à créer deux
nouvelles variables, comme ceci :
<<ex1-5e, eval=FALSE>>=
anorex$Before.kg <- anorex$Before/2.2
anorex$After.kg <- anorex$After/2.2
@
Pour calculer les scores de différences \texttt{After} - \texttt{Before}, il
suffit de soustraire les valeurs des deux variables, en se rappelant que ce
genre d'opérations opère élément par élément (c'est-à-dire pour chaque unité
statistique). Ici, on ajoutera la variable nouvellement créée au tableau de
données \texttt{anorex}.
<<ex1-5f>>=
anorex$poids.diff <- anorex$After - anorex$Before
head(anorex)
@ 
Enfin, pour calculer la moyenne et l'étendue des scores de différence par
groupe de traitement, on peut procéder de deux manières. Soit on isole
chaque groupe et on calcule les statistiques demandées, soit on \og
factorise\fg\ l'opération de calcul en opérant par modalité de la variable
qualitative. La première solution s'obtiendrait ainsi, pour le 1\ier\ groupe
par exemple :
<<ex1-5g>>=
mean(anorex$poids.diff[anorex$Group == "g1"])
range(anorex$poids.diff[anorex$Group == "g1"])
@
L'autre solution plus économique consiste à utiliser la commande
\texttt{tapply} de la manière suivante :
<<ex1-5h>>=
tapply(anorex$poids.diff, anorex$Group, mean)
tapply(anorex$poids.diff, anorex$Group, range)
@ 
Comme on le voit, pour utiliser la commande \texttt{tapply} on spécifie la
variable réponse (\texttt{poids.diff}), le facteur de classification
(\texttt{Group}) et la commande à appliquer (\texttt{mean}). On notera que
plutôt que de préfixer chaque nom de variable par le nom du tableau de
données, on peut simplifier la syntaxe en utilisant \texttt{with} :
<<ex1-5i>>=
with(anorex, tapply(poids.diff, Group, mean))
@ 
\end{sol}
\end{exo}
\Closesolutionfile{solutions}

\section*{Corrigés}
\input{solutions1}


\chapter{Statistiques descriptives et estimation}\label{chap:descriptive}
\Opensolutionfile{solutions}[solutions2]

\begin{center} 
\fbox{\begin{minipage}{5in} 
\textbf{\large Objectifs :} 

\noindent
Calculer les principaux indicateurs de tendance centrale et de dispersion
d'une variable quantitative • afficher un histogramme et une courbe de
densité • construire un tableau de fréquences d'effectifs • calculer des
intervalles de confiance associés à une moyenne ou une proportion empirique
\end{minipage}} 
\end{center}

\vspace*{3ex}

\section*{Énoncés}
%
%
%
\begin{exo}\label{exo:2.1}
Une variable quantitative $X$ prend les valeurs suivantes sur un échantillon
de 26 sujets :
\begin{verbatim}
24.9,25.0,25.0,25.1,25.2,25.2,25.3,25.3,25.3,25.4,25.4,25.4,25.4,
25.5,25.5,25.5,25.5,25.6,25.6,25.6,25.7,25.7,25.8,25.8,25.9,26.0
\end{verbatim}
\begin{description}
\item[(a)] Calculer la moyenne, la médiane ainsi que le mode de $X$. 
\item[(b)]  Quelle est la valeur de la variance estimée à partir de ces données ? 
\item[(c)] En supposant que les données sont regroupées en 4 classes dont les
  bornes sont : 24.9–25.1, 25.2–25.4, 25.5–25.7, 25.8–26.0, afficher la
  distribution des effectifs par classe sous forme d'un tableau d'effectifs. 
\item[(d)] Représenter la distribution de $X$ sous forme d'histogramme, sans
  considération d'intervalles de classe \emph{a priori}.
\end{description}
\begin{sol}
Une manière de représenter ce type de données consiste à utiliser saisir les
observations comme ceci :
<<ex2-1a>>=
x <- c(24.9,25.0,25.0,25.1,25.2,25.2,25.3,25.3,25.3,25.4,25.4,25.4,25.4,
       25.5,25.5,25.5,25.5,25.6,25.6,25.6,25.7,25.7,25.8,25.8,25.9,26.0)
@ 

La médiane et la moyenne sont obtenues à l'aide des commandes
suivantes :
<<ex2-1b>>=
median(x)
mean(x)
@
\cmd{median}\cmd{mean}
Concernant la médiane, on peut vérifier qu'il s'agit bien de la valeur
correspondant au deuxième quartile ou au 50\ieme\ percentile, c'est-à-dire
la valeur de $X$ telle que 50~\% des observations lui sont inférieures :
<<>>=
quantile(x)
@ 
\cmd{quantile}

Pour le mode, il est nécessaire d'afficher la distribution des effectifs
selon les valeurs de $X$, puis vérifier à quelle valeur de $X$ est associé
l'effectif le plus grand:
<<ex2-1c>>=
table(x)
@
\cmd{table}
<<ex2-1d, echo=FALSE>>=
m <- names(table(x)[table(x)==max(table(x))])
@
Ici, on voit qu'il y a deux modes : \Sexpr{m[1]} et \Sexpr{m[2]}.

L'estimé de la variance est donné par :
<<ex2-1e>>=
var(x)
@ 
\cmd{var}

En supposant que les données sont regroupées en 4 classes dont les bornes
sont : 24.9–25.1, 25.2–25.4, 25.5–25.7, 25.8–26.0, on peut recalculer la
distribution des effectifs par classe.
<<ex2-1f>>=
xc <- cut(x, breaks=c(24.9,25.2,25.5,25.8,26.0), include.lowest=TRUE, right=FALSE)
table(xc)
@
\cmd{cut}\cmd{table}
On remarquera que R utilise la notation anglo-saxonne pour représenter les
bornes des intervalles de classe : le symbole \texttt{)} à droite d'un
nombre signifie que ce nombre est exclu de l'intervalle, alors que
\texttt{]} signifie que l'intervalle contient ce nombre.

On peut visualiser la distribution des effectifs à l'aide d'un histogramme
comme suit :
<<ex2-1g, fig=TRUE>>=
histogram(~ x, type="count")
@
\cmd{histogram}

Par défaut, R détermine automatiquement les intervalles de classe. Si l'on
souhaite spécifier soi-même les bornes des intervalles de classe, on
utilisera l'option \texttt{breaks=}. Par exemple, pour afficher la
distribution des effectifs selon les 4 classes définies plus haut, on écrirait :
\begin{verbatim}
> histogram(~ x, type="count", breaks=c(24.9,25.2,25.5,25.8,26.0))
\end{verbatim}
\end{sol}
\end{exo}
%
% Everitt 2011 p. 38
%
\begin{exo}\label{exo:2.2}
On dispose des temps de survie de 43 patients souffrant de leucémie
granulocytaire chronique, mesurés en jours depuis le diagnostique :
\autocite[p.~38]{everitt01} 
\begin{verbatim}
7,47,58,74,177,232,273,285,317,429,440,445,455,468,495,497,532,571,
579,581,650,702,715,779,881,900,930,968,1077,1109,1314,1334,1367,
1534,1712,1784,1877,1886,2045,2056,2260,2429,2509
\end{verbatim}
\begin{description}
\item[(a)] Calculer le temps de survie médian. 
\item[(b)] Combien de patients ont une survie inférieure (strictement) à 900
  jours au moment de l'étude ? 
\item[(c)] Quelle la durée de survie associée au 90\ieme\ percentile ?
\end{description}
\begin{sol}
Les données sont saisies comme à l'exercice précédent.  
<<ex2-2a>>=
s <- c(7,47,58,74,177,232,273,285,317,429,440,445,455,468,495,497,532,571,
       579,581,650,702,715,779,881,900,930,968,1077,1109,1314,1334,1367,
       1534,1712,1784,1877,1886,2045,2056,2260,2429,2509)
@ 
Le temps de survie médian est obtenu facilement :
<<ex2-2b>>=
median(s)
@ 
\cmd{median}

Pour déterminer le nombre de patients avec une survie $\le 900$ jours, il
suffit d'effectuer un test :
<<ex2-2c>>=
table(s <= 900)
@
\cmd{table}
La colonne libellée \texttt{TRUE} indique le nombre d'observations vérifiant
la condition ci-dessus. 
% Une autre solution possible est \verb|length(which(s <= 900))| (expliquer).

La survie associée au 90\ieme\ percentile peut s'obtenir à l'aide de la commande
\texttt{quantile}, par exemple :
<<ex2-2d>>=
quantile(s, 0.9)
@ 
\cmd{quantile}
On peut vérifier que le résultat correspond bien à la valeur de survie telle
que 90~\% des observations ne dépasse pas cette valeur.
<<ex2-2e>>=
table(s <= quantile(s, 0.9))
@ 
\cmd{table}
Ici, $38/43$ est bien inférieur à 0.90 tandis que $39/43=0.91$.
\end{sol}
\end{exo}
%
% Everitt 2011 p. 38
%
\begin{exo}\label{exo:2.3}
Le fichier \texttt{elderly.dat} contient la taille mesurée en cm de 351
personnes âgées de sexe féminin, sélectionnées aléatoirement dans la
population lors d'une étude sur l'ostéoporose. Quelques observations sont
cependant manquantes.
\begin{description}
\item[(a)] Combien y'a t-il d'observations manquantes au total ?
\item[(b)] Donner un intervalle de confiance à 95~\% pour la taille moyenne
  dans cet échantillon, en utilisant une approximation normale.
\item[(c)] Représenter la distribution des tailles observées sous forme
  d'une courbe de densité.  
\end{description}
\begin{sol}
Pour lire le fichier qui ne se compose que d'une série de valeurs
numériques, on utilise la commande \texttt{scan}. Attention, comme il y a
des valeurs manquantes, codées dans le fichier texte par des ".", il est
nécessaire de s'assurer que ces observations sont bien identifiées comme
telles par \R. 
<<ex2-3a>>=
tailles <- scan("elderly.dat", na.strings=".")
@ 
\cmd{scan}
On pourrait afficher la distribution des observations à l'aide d'un
simple histogramme (\verb|histogram(~ tailles)|). Cela permet en particulier
de vérifier la forme générale de la distribution et la présence
d'eventuelles valeurs "extrêmes".

Le nombre d'observations non complètes (données manquantes) est déterminé en
comptant le nombre de valeurs manquantes (\texttt{NA}), ce qui est
équivalent à établir un tri à plat des valeurs considérées comme manquantes
par \R à l'aide de la commande \texttt{table}.
<<ex2-3b>>=
sum(is.na(tailles))
table(is.na(tailles))
@
\cmd{sum}\cmd{is.na}\cmd{table}
On a donc 5 observations manquantes au total.

La moyenne est obtenue à l'aide de la commande \texttt{mean}. Cependant,
comme il existe des valeurs manquantes il est nécessaire de préciser
l'option \verb|na.rm=TRUE| pour indiquer à \R de calculer la moyenne
arithmétique sur les cas complets. La même remarque vaut pour l'usage de
\texttt{sd}. On a également besoin d'avoir une estimation de
l'écart-type. Le quantile de référence (97.5~\%) pour la loi normale 97.5~\%
est généralement pris à 1.96 dans les tables statistiques mais on peut
obtenir sa valeur avec R grâce à \texttt{qnorm}.  Soit au final :
<<ex2-3c>>=
m <- mean(tailles, na.rm=TRUE)    # moyenne
s <- sd(tailles, na.rm=TRUE)      # écart-type
n <- sum(!is.na(tailles))         # nombre d'observations
m - qnorm(0.975) * s/sqrt(n)      # borne inf. IC 95 %
m + qnorm(0.975) * s/sqrt(n)      # borne sup. IC 95 %
@ 
\cmd{mean}\cmd{sd}\cmd{sum}\cmd{is.na}\cmd{qnorm}\cmd{sqrt}
Attention, utiliser la commande \texttt{length} pour compter le nombre
d'observations serait erroné dans ce contexte : en présence de valeurs
manquantes, il faut explicitement s'assurer que l'on travaille bien sur les
cas complets. La commande \verb|sum(!is.na(tailles))| est équivalente à la
commande, peut-être plus explicite, \verb|sum(complete.cases(tailles))|.
Les deux dernières instructions peuvent se simplifier en exploitant la
capacité de \R à répéter le même calcul sur des séries de données stockées
dans une variable. Ici, on se contente de reproduire la formule $m\pm
z_{0.975}\frac{s}{\sqrt{n}}$ :
<<ex2-3d, eval=FALSE>>=
m + c(-1,1) * qnorm(0.975) * s/sqrt(n)
@
\cmd{qnorm}\cmd{sqrt}
% FIXME:
% sans doute à supprimer car trop compliqué.

Pour représenter la distribution des tailles sous forme d'une courbe de
densité, qui ne pose pas le problème du choix de classes \emph{a priori}, la
commande à utiliser est \texttt{densityplot}.
<<ex2-3e, fig=TRUE>>=
densityplot(~ tailles)
@ 
\cmd{densityplot}

\noindent Le degré de lissage de la courbe de densité estimée à partir des
données peut être contrôlé à l'aide de l'option \texttt{bw}. L'exemple
suivant produirait une courbe présentant beaucoup moins de variations
locales, par exemple.
\begin{verbatim}
> densityplot(~ tailles, bw=4)
\end{verbatim}
\cmd{densityplot}
\end{sol}
\end{exo}
% 
% Hosmer & Lemeshow 1989
%
\begin{exo}\label{exo:2.4}
Le fichier \texttt{birthwt} est un des jeux de données fournis avec \R. Il
comprend les résultats d'une étude prospective visant à identifier les
facteurs de risque associés à la naissance de bébés dont le poids est
inférieur à la norme (2,5 kg). Les données proviennent de 189 femmes, dont
59 ont accouché d'un enfant en sous-poids. Parmi les variables d'intérêt
figurent l'âge de la mère, le poids de la mère lors des dernières
menstruations, l'ethnicité de la mère et le nombre de visites médicales
durant le premier trimestre de grossesse.\autocite{hosmer89}
Les variables disponibles sont décrites comme suit : \texttt{low} (= 1 si
poids $<2.5$ kg, 0 sinon), \texttt{age} (années), \texttt{lwt} (poids de la
mère en livres), \texttt{race} (ethnicité codée en trois classes, 1 = white,
2 = black, 3 = other), \texttt{smoke} (= 1 si consommation de tabac durant
la grossesse, 0 sinon), \texttt{ptl} (nombre d'accouchements pré-terme
antérieurs), \texttt{ht} (= 1 si antécédent d'hypertension, 0 sinon),
\texttt{ui} (= 1 si manifestation d'irritabilité utérine, 0 sinon),
\texttt{ftv} (nombre de consultations chez le gynécologue durant le premier
trimestre de grossesse), \texttt{bwt} (poids des bébés à la naissance, en
\emph{g}).
\begin{description}
\item[(a)] Recoder les variables \texttt{low}, \texttt{race},
  \texttt{smoke}, \texttt{ui} et \texttt{ht} en variables
  qualitatives, avec des étiquettes ("labels") plus informatives.
\item[(b)] Convertir le poids des mères en \emph{kg}. Indiquer la moyenne, la
  médiane et l'intervalle inter-quartile. Représenter la distribution des
  poids sous forme d'histogramme.
\item[(c)] Indiquer la proportion de mères consommant du tabac durant la
  grossesse, avec un intervalle de confiance à 95~\%. Représenter les
  proportions (en \%) fumeur/non-fumeur sous forme d'un diagramme en
  barres.
\item[(d)] Recoder l'âge des mères en trois classes équilibrées (tercilage)
  et indiquer la proportion d'enfants dont le poids est $<2500$ \emph{g}
  pour chacune des trois classes.
\item[(e)] Construire un tableau d'effectifs ($n$ et \%) pour la variable
  ethnicité (\texttt{race}).  
\item[(f)] Décrire la distribution des variables \texttt{race},
  \texttt{smoke}, \texttt{ui}, \texttt{ht} et \texttt{age} après
  stratification sur la variable \texttt{low}.  
\end{description}
\begin{sol}
Les données fournies avec \R sont dans un format accessible à partir de la
commande \texttt{data}. Il est parfois nécessaire d'indiquer à \R dans quel
package trouver les données. Pour charger les données \texttt{birthwt}, on
utilisera donc
<<ex2-4a>>=
data(birthwt, package="MASS")
str(birthwt)
head(birthwt, 5)
@ 
\cmd{data}\cmd{str}\cmd{head}
Comme on peut le vérifier dans les sorties précédentes, l'ensemble des
variables est au format numérique. Pour recoder certaines des variables en
variables qualitatives, on utilisera la commande \texttt{factor}. 
<<ex2-4b>>=
yesno <- c("No","Yes")
ethn <- c("White","Black","Other")
birthwt <- within(birthwt, {
  low <- factor(low, labels=yesno)
  race <- factor(race, labels=ethn)
  smoke <- factor(smoke, labels=yesno)
  ui <- factor(ui, labels=yesno)
  ht <- factor(ht, labels=yesno)
})
@ 
\cmd{within}\cmd{factor}
La commande \texttt{within} utilisée permet simplement d'éviter de devoir
indiquer à chaque fois le nom du tableau de données (\texttt{data.frame}
dans la terminologie \R) suivi du nom de la variable, comme par exemple dans
les instructions suivantes :
<<ex2-4c, eval=FALSE>>=
birthwt$low <- factor(birthwt$low, labels=yesno)
birthwt$race <- factor(birthwt$race, labels=ethn)
@ 
\cmd{factor}
On verra plus tard une autre construction (\texttt{with}) permettant
d'éviter de saisir le nom du tableau de données à plusieurs reprises dans
une même commande. 

La conversion des poids des mères en \emph{kg} ne pose pas vraiment de
problème puisqu'il suffit de diviser les poids exprimés en livres par 2.2 :
<<ex2-4d>>=
birthwt$lwt <- birthwt$lwt/2.2
@ 
Notons que la transformation effectuée est définitive : il n'y a plus moyen
de retrouver les valeurs d'origine de la variable \texttt{lwt} dans le
\texttt{data.frame} \texttt{birthwt} (à moins de réimporter les
données). Dans le cas présent, ce n'est pas très important, mais en règle
générale il est préférable de créer de nouvelles variables.

On peut ensuite utiliser une commande telle que \texttt{summary} pour
obtenir les principaux indicateurs de tendance centrale et de forme de la
distribution (à partir de l'étendue et des quartiles).
<<ex2-4e>>=
summary(birthwt$lwt)
@ %$ 
\cmd{summary}
La médiane et la moyenne sont de 59 et 55 \emph{kg},
respectivement. L'intervalle inter-quartile se calcule aisément comme la
différence entre le 3\ieme (63.64) et le 1\ier quartile (50.00), mais on
peut également utiliser la commande \texttt{IQR} :
<<ex2-4f>>=
IQR(birthwt$lwt)
@ 
\cmd{IQR}
De manière générale, les quartiles peuvent être retrouvés ainsi :
<<ex2-4g>>=
quantile(birthwt$lwt)
@ 
\cmd{quantile}
d'où une autre manière de calculer l'intervalle inter-quartile,
<<ex2-4h>>=
lwt.quartiles <- quantile(birthwt$lwt)
lwt.quartiles
lwt.quartiles[4] - lwt.quartiles[2]
@ %$
\cmd{quantile}

La distribution des poids est indiquée dans l'histogramme suivant, en
utilisant la commande suivante :
<<ex2-4i, fig=TRUE>>=
histogram(~ lwt, data=birthwt, type="count", xlab="Poids de la mère (kg)", ylab="Effectifs")
@ 
\cmd{histogram}

La proportion de mères ayant fumé durant leur grossesse est obtenue à partir
d'un simple tri à plat de la variable \texttt{smoke}.
<<ex2-4j>>=
table(birthwt$smoke)
round(prop.table(table(birthwt$smoke))*100, 1)
@ 
\cmd{table}\cmd{round}\cmd{prop.table}
Pour obtenir une estimation de l'intervalle de confiance à 95~\% associé à
cette proportion empirique, on peut utiliser la commande \texttt{prop.test}
qui effectue également un test d'hypothèse sur l'égalité des proportions
estimées dans différents groupes. Cependant, la commande suivante ne renvoit
pas le résultat correct car, comme on peut le vérifier dans la proportion
qui est estimée dans ce cas, \R a travaillé sur la proportion de non-fumeurs.
<<ex2-4k>>=
prop.test(table(birthwt$smoke), correct=FALSE)
@ %$
\cmd{prop.test}
On peut donc utiliser directement la forme suivante :
<<ex2-4l>>=
prop.test(74, 189, correct=FALSE)
@ 
On retrouve bien la proportion estimée à l'étape précédente (\texttt{sample
  estimates}) (39.1~\%), ainsi que l'intervalle de confiance qui, ici, vaut
$[0.325;0.463]$. 

Un diagramme en barres représentant la proportion de fumeurs/non-fumeurs
dans cet échantillon est construit à partir de la commande \texttt{barchart}
comme proposé ci-après.
<<ex2-4m, fig=TRUE>>=
barchart(prop.table(table(birthwt$smoke))*100, horizontal=FALSE, 
         ylab="Proportion (%)", ylim=c(-5,105))
@ %$ 
\cmd{barchart}\cmd{prop.table}

Pour recoder l'âge en variable qualitative à partir des terciles, on
utilisera la commande \texttt{cut} :
<<ex2-4n>>=
birthwt$age.dec <- cut(birthwt$age, breaks=quantile(birthwt$age, c(0, 0.33, 0.66, 1)), 
                       include.lowest=FALSE)
table(birthwt$age.dec)
@ 
\cmd{cmd}\cmd{table}\cmd{quantile}
Le nombre d'enfants ayant un poids $<2.5$ \emph{kg} dans chacune des
classes est obtenue à partir d'un simple croisement des deux variables
\texttt{age.dec} et \texttt{low} :
<<ex2-4o>>=
with(birthwt, table(age.dec, low))
@ 
\cmd{with}\cmd{table}
Pour obtenir la fréquence relative des enfants de faible poids, il suffit de
diviser les effectifs précédents par les totaux colonnes, ce que l'on peut
réaliser ainsi avec \R :
<<ex2-4p>>=
tab <- with(birthwt, table(age.dec, low))
prop.table(tab, 2)[,"Yes"]
@ 
\cmd{prop.table}
La dernière commande permet d'isoler la colonne \texttt{Yes} du tableau
croisé.

Concernant l'ethnicité, on utilisera également la commande \texttt{table}
pour produire un tableau des effectifs, associée à \texttt{prop.table} pour
calculer les fréquences relatives, que l'on exprimera en \%.
<<ex2-4q>>=
res <- rbind(table(birthwt$race), prop.table(table(birthwt$race))*100)
rownames(res) <- c("n","%")
round(res, 2)
@ 
\cmd{rbind}\cmd{table}\cmd{prop.table}\cmd{rownames}\cmd{round}

Pour résumer la distribution des variables selon le facteur de
stratification, on pourrait très bien utiliser la commande \texttt{summary}
sur chacune des variables (\texttt{race}, \texttt{smoke}, \texttt{ui},
\texttt{ht} et \texttt{age}). Toutefois, il existe une manière plus
"économique" d'effectuer ce type de tâche avec la commande
\texttt{summary.formula} disponible dans le package \texttt{Hmisc}.
<<ex2-4r>>=
library(Hmisc)
summary(low ~ race + smoke + ui + ht + age, data=birthwt, method="reverse")
@ 
\cmd{library}\cmd{summary.formula}
Le principe général de cette commande est le suivant : on exprime à l'aide
d'une formule, selon la terminologie \R, la relation entre les variables ;
ici, décrire selon les modalités de la variable \texttt{low} les
distributions des autres variables indiquées dans la commande
(\texttt{table} pour les variables qualitatives et \texttt{quantile} pour
les variables numériques). Les commandes \texttt{tapply} (pour les variables
numériques) et \texttt{table} (pour les variables qualitatives) restent
toutefois applicables.
\end{sol}
\end{exo}
% 
% Tri à plat, graphique en barres
%
%% \begin{exo}\label{exo:2.5}
  
%% \end{exo}

\Closesolutionfile{solutions}

\section*{Corrigés}
\input{solutions2}

\chapter*{Devoir \no 1}
\addcontentsline{toc}{chapter}{Devoir \no 1}
  
Les exercices sont indépendants. Une seule réponse est correcte pour chaque
question. Lorsque vous ne savez pas répondre, cochez la case NSP.

\section*{Exercice 1}
\noindent À partir des données sur les poids à la naissance, décrits à
l'exercice~\ref{exo:2.4} et que vous pouvez charger à l'aide de la commande
\verb|data(birthwt, package="MASS"|, veuillez indiquer les commandes
permettant de répondre aux questions suivantes :
\begin{description}
\item[\bf 1.1]  Quel est le poids moyen des bébés dont la mère est âgée de 20 ans ou
  moins ? \\[1em]
  \begin{minipage}{.6\linewidth}
    \begin{description}
    \item[A.] \verb|birthwt$lwt & birthwt$age < 20|
    \item[B.] \verb|birthwt$lwt[birthwt$age <= 20]|
    \item[C.] \verb|birthwt$lwt[age <= 20]|
    \item[D.] \verb|birthwt$lwt[birthwt$age < 20]|
    \end{description}
  \end{minipage}\hfill
  \begin{minipage}{.35\linewidth}
    Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad
    \Square{D}\quad \Square{NSP}
  \end{minipage}
\item[\bf 1.2] Combien de bébés ont une mère ayant eu des antécédents
  d'hypertension ? \\[1em]
  \begin{minipage}{.6\linewidth}
    \begin{description}
    \item[A.] \verb|table(birthwt$ht)[1]|
    \item[B.] \verb|nrow(birthwt$ht == 1)|
    \item[C.] \verb|sum(birtwht$ht == 1)|
    \item[D.] \verb|birthwt$ht[birthwt$low == 1]|
    \end{description}
  \end{minipage}\hfill
  \begin{minipage}{.35\linewidth}
    Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad
    \Square{D}\quad \Square{NSP}
  \end{minipage}
\item[\bf 1.3] On souhaite afficher la distribution des valeurs prises par
  la variable \texttt{ptv}, sous forme de pourcentage. Quelle commande est
  la plus appropriée ? \\[1em]
  \begin{minipage}{.6\linewidth}
    \begin{description}
    \item[A.] \verb|barchart(ptv, data=birthwt, type="percent")|
    \item[B.] \verb|barchart(ptv, data=birthwt, type="%")|
    \item[C.] \verb|histogram(prop.table(table(birthwt$ptv)))|
    \item[D.] \verb|barchart(table(ptv)/100, data=birthwt)|
    \end{description}  
  \end{minipage}\hfill
  \begin{minipage}{.35\linewidth}    
    Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad
    \Square{D}\quad \Square{NSP}
  \end{minipage}
\end{description}
\vskip1em

\noindent Les sorties suivantes indiquent les résultats de différentes
procédures inférentielles (estimation ou test). Quelles commandes ont été
utilisées dans chacun des cas ? 
\begin{description}
\item[\bf 1.4]
\begin{verbatim}
	1-sample proportions test without continuity correction

data:  table(birthwt$ht), null probability 0.5 
X-squared = 144.0476, df = 1, p-value < 2.2e-16
alternative hypothesis: true p is not equal to 0.5 
95 percent confidence interval:
 0.8923149 0.9633103 
sample estimates:
        p 
0.9365079 
\end{verbatim}
  \vskip1em
  
  \begin{minipage}{.6\linewidth}
    \begin{description}
    \item[A.] \verb|bin.test(birthwt$ht)|
    \item[B.] \verb|prop.test(birthwt$ht)|
    \item[C.] \verb|prop.test(table(birthwt$ht))|
    \item[D.] \verb|prop.test(table(birthwt$ht), correct=FALSE)|
    \end{description}  
  \end{minipage}\hfill
  \begin{minipage}{.35\linewidth}  
    Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C }\quad
    \Square{D }\quad \Square{NSP}
  \end{minipage}
\item[\bf 1.5] 
\begin{verbatim}  
  	One Sample t-test

data:  birthwt$bwt 
t = 55.5137, df = 188, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0 
95 percent confidence interval:
 2839.952 3049.222 
sample estimates:
mean of x 
 2944.587 
\end{verbatim}
  \vskip1em
  
  \begin{minipage}{.6\linewidth}
    \begin{description}
    \item[A.] \verb|T.test(birthwt$bwt)|
    \item[B.] \verb|t.test(birthwt$bwt, alternative=0)|
    \item[C.] \verb|t.test(bwt)|
    \item[D.] \verb|t.test(birthwt$bwt, correct=FALSE)|
    \end{description}  
  \end{minipage}\hfill
  \begin{minipage}{.35\linewidth}  
    Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad
    \Square{D}\quad \Square{NSP}
  \end{minipage}  
\end{description}
\vskip1em

\noindent Soit l'histogramme de fréquences suivant :\\
<<exo1-1-6, echo=FALSE, fig=TRUE>>=
histogram(~ bwt, data=birthwt, aspect=.6, type="percent", xlab="Poids des bébés (kg)")
@ 

\begin{description}
\item[\bf 1.6]
Quelle commande permet de reproduire cette figure (indépendemment du rapport
largeur/hauteur) ?
\begin{description}
\item[A.] \verb|histogram(~ bwt/nrow(birthwt), data=birthwt, xlab="Poids des bébés (kg)")|
\item[B.] \verb|histogram(~ bwt, data=birthwt, type="density", xlab="Poids des bébés (kg)")|
\item[C.] \verb|histogram(~ bwt, data=birthwt, type="percent", xlab="Poids des bébés (kg)")|
\item[D.] \verb|histogram(~ bwt/1000, data=birthwt, type="percent", xlab="Poids des bébés (kg)")|
\end{description}  

\noindent Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad  \Square{D}\quad \Square{NSP}
\end{description}

\section*{Exercice 2}
\noindent Soit les deux séries de mesures, $X_1$ et $X_2$, que l'on
supposera indépendantes :
\vskip1em

\begin{tabular}{l|rrrrrrrrrrrrrrr}
\texttt{x1} & 17.5 & 7.7 &16.5 &18.4 & 3.2 & 5.8 &13.5 &13.2 &12.0 & 7.8 &11.2 &13.2 & 4.1 & 2.4 & 8.0\\
\hline
\texttt{x2} & 18.6 &10.5 &15.0 & 3.0 &18.9& 13.1&  8.6& 16.9& 14.7 & 1.5 &17.6& 15.4& 17.9& 19.1&  9.8
\end{tabular}
\vskip1em

\begin{description}
\item[\bf 2.1] Quelles sont les moyennes de $X_1$ et $X_2$ ? \\[1em]
  \begin{minipage}{.6\linewidth}
    \begin{description}
    \item[A.] \verb|mean(x1,x2)|
    \item[B.] \verb|c(mean(x1),mean(x2))|
    \item[C.] \verb|mean(c(x1,x2))|
    \end{description}
  \end{minipage}\hfill
  \begin{minipage}{.35\linewidth}  
    Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad \Square{NSP}
  \end{minipage}  
  \item[\bf 2.2] Quelle est l'intervalle inter-quartile de $X_1-X_2$ ?  \\[1em]
  \begin{minipage}{.6\linewidth}
    \begin{description}
    \item[A.] \verb|mean(x1-x2)|
    \item[B.] \verb|sum(x1-x2)/14|
    \item[C.] \verb|diff(mean(c(x1,x2)))|
    \end{description}
  \end{minipage}\hfill
  \begin{minipage}{.35\linewidth}  
    Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad \Square{NSP}
  \end{minipage}    
\item[\bf 2.3] Donner un intervalle de confiance à 95~\% pour cette
  différence (en utilisant une loi de Student).
  \begin{description}
  \item[A.] \verb|t.test(x1-x2)|
  \item[B.] \verb|(x1-x2) + c(-1,1) * qt(0.975, 28)*sd(x1-x2)/sqrt(length(c(x1,x2)))|
  \item[C.] \verb|(x1-x2) + c(-1,1) * qt(0.975, 28)*sd(x1-x2)/sqrt(length(c(x1,x2)))/2-2|
  \end{description}
  Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad \Square{NSP}
\item[\bf 2.4] Convertir ces deux variables numériques en variables
  qualitatives à 4 classes équilibrées et indiquer la ou les classes modales
  dans chacun des deux cas.
  \begin{description}
  \item[A.] 
\begin{verbatim}
x1b <- cut(x1, breaks=quantile(x1))
x2b <- cut(x2, breaks=quantile(x2))
which.max(table(x1b))
which.max(table(x2b))
\end{verbatim}
  \item[B.] 
\begin{verbatim}
x1b <- cut(x1, breaks=quantile(x1), include.lowest=TRUE)
x2b <- cut(x2, breaks=quantile(x2), include.lowest=TRUE)
which.max(table(x1b))
which.max(table(x2b))
\end{verbatim}
  \item[C.]
\begin{verbatim}
x1b <- cut(x1, breaks=quantile(x1), include.lowest=TRUE)
x2b <- cut(x2, breaks=quantile(x2), include.lowest=TRUE)
max(table(x1b, x2b))
\end{verbatim}
  \end{description}
  Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad \Square{NSP}
\item[\bf 2.5] Que permettent de produire les commandes suivantes ?
\begin{verbatim}
x3 <- factor((x1-x2)>0, labels=c("+","-"))
table(x3)["+"]
\end{verbatim}
\vskip1em

\begin{description}
\item[A.] La somme des différences $X_1-X_2$ positives.
\item[B.] Le nombre de différences $X_1-X_2$ positives.
\item[C.] La proportion de valeurs de $X_1$ supérieures à celles de $X_2$.
\end{description}
  Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad \Square{NSP}  
\end{description}

\section*{Exercice 3}
\noindent Soit les données arrangées sous forme de tableau comme indiqué ci-dessous :
\vskip1em

\begin{tabular}{cccc}
\toprule  
id & age & sexe & taille \\  
\midrule
01 & 23 &  G  & 172 \\
02 & 22.5  & G & 178 \\
03 & 22.5 & F & 159 \\
04 & . & G & 182 \\
05 & 22 & F & 154 \\
06 & 21.5 & F & 161 \\
07 & 22 & G & 176 \\
08 & 21.5 & G & 191 \\
09 & 22.5 & F & 1.60 \\
10 & 23 & G & 173\\
\bottomrule
\end{tabular}
\vskip1em

\noindent Les données manquantes sont indiquées à l'aide du symbole \verb|.| (point),
le sexe des étudiants est codé \texttt{G} (garçons) ou \texttt{F} (filles),
et les tailles sont exprimées en \emph{cm}. Le tableau a été enregistré sous
\R sous le nom \texttt{dfrm}, et le résultat de la commande \verb|str(dfrm)|
est fourni ci-dessous :
<<exo1-3-1a, echo=FALSE>>=
dfrm <- data.frame(id=ifelse(1:10<10, paste("0", 1:10, sep=""), 1:10),
                   age=c(23,22.5,22.5,NA,22,21.5,22,21.5,22.5,23),
                   sexe=c("G","G","F","G","F","F","G","G","F","G"),
                   taille=c(172,178,159,182,154,161,176,191,1.6,173))
@ 
<<exo1-3-1b>>=
str(dfrm)
@ 
\begin{description}
\item[\bf 3.1] La commande \verb|mean(dfrm$age)| fournit l'âge moyen des étudiants.
  \begin{description}
  \item[A.] Vrai.
  \item[B.] Faux.
  \end{description}
  Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{NSP}  
\item[\bf 3.2] On veut remplacer l'âge manquant par l'âge moyen. Laquelle parmi
  ces commandes est correcte ?
  \begin{description}
  \item[A.] \verb|dfrm$age[4] <- mean(dfrm$age)|
  \item[B.] \verb|dfrm$age[is.na(dfrm$age)] <- mean(dfrm$age, na.rm=TRUE)|
  \item[C.] \verb|dfrm$age[dfrm$id == "04"] <- mean(dfrm$age)|  
  \end{description}
  Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad \Square{NSP}  
\item[\bf 3.3] On souhaite afficher la proportion de garçons et de filles sous
  forme d'un diagramme en barres. La commande \verb|barchart(sexe, data=dfrm)| 
  est-elle correcte ?
  \begin{description}
  \item[A.] Vrai.
  \item[B.] Faux.
  \end{description}
  Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{NSP}  
\item[\bf 3.4] Pour afficher un histogramme des tailles des garçons, quelle
  commande faut-il utiliser ?
  \begin{description}
  \item[A.] \verb|histogram(taille, subset=sexe == "G")|
  \item[B.] \verb|barchart(~ taille, data=subset(dfrm, sexe=="G"))|
  \item[C.] \verb|histogram(taille, data=dfrm, subset=sexe == "G")|
  \end{description}
  Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{C}\quad \Square{NSP}
\item[\bf 3.5] Que fait la comamnde suivante ?
\begin{verbatim}
with(subset(dfrm, sexe=="F"), mean(age) + c(-1,1) * qt(0.95, 3) * sd(age)/sqrt(4))
\end{verbatim}
  \begin{description}
  \item[A.] Elle fournit un intervalle de confiance à 95~\% pour l'âge moyen
    chez les filles.
  \item[B.] Elle fournit un intervalle de confiance à 90~\% pour l'âge moyen
    chez les filles.
  \end{description}
  Vous répondez :\\ \Square{A}\quad \Square{B}\quad \Square{NSP}  
\end{description}
  
  


%--------------------------------------------------------------- Chapter 03 --
\chapter{Comparaisons de deux variables}\label{chap:comparaisons}
\Opensolutionfile{solutions}[solutions3]

\begin{center} 
\fbox{\begin{minipage}{5in} 
\textbf{\large Objectifs :} 

\noindent
Comparaison de deux moyennes • comparaison de deux proportions • comparaison
d'une distribution empirique à une loi de probabilité • approches
non-paramétriques pour la comparaison de deux variables quantitatives ou
qualitatives 
\end{minipage}} 
\end{center}

\vspace*{3ex}

\section*{Énoncés}
%
% Everitt 2001 p. 64
%
\begin{exo}\label{exo:3.1}
On dispose des poids à la naissance d'un échantillon de 50 enfants
présentant un syndrôme de détresse respiratoire idiopathique aïgue. Ce type
de maladie peut entraîner la mort et on a observé 27 décès chez ces
enfants. Les données sont résumées dans le tableau ci-dessous et sont
disponibles dans le fichier \texttt{sirds.dat}, où les 27 premières
observations correspondent au groupe des enfants décédés au moment de
l'étude. \autocite[p.~64]{everitt01}
\begin{verbatim}
Enfants décédés :
1.050 1.175 1.230 1.310 1.500 1.600 1.720 1.750 1.770 2.275
2.500 1.030 1.100 1.185 1.225 1.262 1.295 1.300 1.550 1.820
1.890 1.940 2.200 2.270 2.440 2.560 2.730 
Enfants vivants :
1.130 1.575 1.680 1.760 1.930 2.015 2.090 2.600 2.700 2.950 
3.160 3.400 3.640 2.830 1.410 1.715 1.720 2.040 2.200 2.400 
2.550 2.570 3.005
\end{verbatim}
Un chercheur s'intéresse à l'existence éventuelle d'une différence entre le
poids moyen des enfants ayant survécu et celui des enfants décédés des
suites de la maladie. 
\begin{description}
\item[(a)] Réaliser un test $t$ de Student. Peut-on rejeter l'hypothèse nulle
  d'absence de différences entre les deux groupes d'enfants ? 
\item[(b)] Vérifier graphiquement que les conditions d'applications du test
(normalité et homogénéité des variances) sont vérifiées. 
\item[(c)] Quel est l'intervalle de confiance à 95~\% pour la différence de
  moyenne observée ?
\end{description}
\begin{sol}
Pour le chargement des données, on lit séparément les données numériques
contenues dans le fichier que l'on stocke dans une variable appelée
\texttt{poids}, et on crée une seconde variable codant pour le statut des
enfants au moment de l'étude.
<<ex3-1a>>=
poids <- scan("sirds.dat")
status <- factor(rep(c("décédé", "vivant"), c(27,23)))
@
\cmd{scan}\cmd{factor}
On réalise ensuite un test $t$ pour échantillons indépendants, en supposant
l'homogénéité des variances :
<<ex3-1b>>=
t.test(poids ~ status, var.equal=TRUE)
@ \cmd{t.test}
Le résultat du test, bilatéral par défaut (\verb|alternative="two.sided"|),
indique que l'on peut effectivement rejeter l'hypothèse nulle.

Pour vérifier l'hypothèse de normalité des populations parentes, on peut
représenter graphiquement la distribution des poids à la naissance pour
chacun des groupes à l'aide d'un histogramme. Par exemple,
<<ex3-1c, fig=TRUE>>=
histogram(~ poids | status, type="count")
@ 
\cmd{histogram}

Une alternative consiste à utiliser des graphiques de type quantile-quantile.

L'homogénéité des variances peut être évaluée à l'aide de diagrammes de type
boîtes à moustache :
<<ex3-1d, fig=TRUE>>=
bwplot(poids ~ status, xlab="status")
@ 
\cmd{bwplot}
\end{sol}
\end{exo}
%
% data(sleep)
%
\begin{exo}\label{exo:3.2}
La qualité de sommeil de 10 patients a été mesurée avant (contrôle) et après
traitement par un des deux hypnotiques suivants : (1) D. hyoscyamine
hydrobromide et (2) L. hyoscyamine hydrobromide. Le critère de jugement
retenu par les chercheurs était le gain moyen de sommeil (en heures) par
rapport à la durée de sommeil de base
(contrôle). \autocite[p.~20]{student08} Les données sont reportées
ci-dessous et figurent également parmi les jeux de données de base de R
(\verb|data(sleep)|).  
\begin{verbatim}
D. hyoscyamine hydrobromide :
0.7 -1.6 -0.2 -1.2 -0.1  3.4  3.7  0.8  0.0  2.0
L. hyoscyamine hydrobromide :
1.9  0.8  1.1  0.1 -0.1  4.4  5.5  1.6  4.6  3.4
\end{verbatim}

Les chercheurs ont conclu que seule la deuxième molécule avait réellement un
effet soporifique. 
\begin{description}
\item[(a)] Estimer le temps moyen de sommeil pour chacune des deux
  molécules, ainsi que la différence entre ces deux moyennes.
\item[(b)] Afficher la distribution des scores de différence (LHH - DHH)
  sous forme d'un histogramme, en considérant des intervalles de classe
  d'une demi-heure, et indiquer la moyenne et l'écart-type de ces scores de
  différence.
\item[(c)] Vérifier l'exactitude des conclusions à l'aide d'un test de Student.
\end{description}
\begin{sol}
On notera qu'il s'agit des mêmes patients qui sont testés dans les deux
conditions (le sujet est pris comme son propre témoin). L'aide en ligne pour
le jeu de données \texttt{sleep} indique bien que les deux premières
variables, \texttt{extra} (différentiel de temps de sommeil) et
\texttt{group} (type de médicament), sont les variables d'intérêt.
<<ex3-2a>>=
data(sleep)
mean(sleep$extra[sleep$group == 1])  # D. hyoscyamine hydrobromide
mean(sleep$extra[sleep$group == 2])  # L. hyoscyamine hydrobromide
m <- with(sleep, tapply(extra, group, mean))
m[2] - m[1]
@
\cmd{data}\cmd{mean}\cmd{with}\cmd{tapply}
Les commandes ci-dessus permettent de vérifier les moyennes de groupe et la
différence de gain de sommeil entre les molécules L. hyoscyamine
hydrobromide et D. hyoscyamine hydrobromide, en gardant à l'esprit que la
commande \verb|with(sleep, tapply(extra, group, mean))| renvoit deux valeurs
(les moyennes par type de molécule) et que la différence deux valeurs
stockées dans la variable auxiliaire \texttt{m} renvoit bien la différence
de moyennes entre les deux traitements.

Pour calculer les scores de différences, on procèdera comme dans
l'exercice~1.5 (p.~\pageref{exo:1.5}), soit
<<ex3-2b>>=
sdif <- sleep$extra[sleep$group == 2] - sleep$extra[sleep$group == 1]
c(mean=mean(sdif), sd=sd(sdif))
@ 
\cmd{mean}
Le calcul de la moyenne et de l'écart-type de ces scores de différences ne
pose pas de difficulté particulière, pas plus que l'affichage de leur
distribution sous forme d'histogramme.
<<ex3-2c, fig=TRUE>>=
histogram(~ sdif, breaks=seq(0, 5, by=0.5), xlab="LHH - DHH")
@ 

Le résultat du test $t$ est obtenu à partir de la commande \texttt{t.test}
en fournissant la variable réponse et le facteur de classification sous
forme d'une formule (variable réponse à gauche, variable explicative à
droite). Le résultat du test est reporté ci-dessous :
<<ex3-2d>>=
t.test(extra ~ group, data=sleep, paired=TRUE)
@ 
\cmd{t.test}
On notera qu'il est nécessaire d'indiquer à R où trouver les variables
d'intérêt, d'où l'usage de \verb|data=sleep|. L'option \verb|paired=TRUE|
est nécessaire pour rendre compte de l'appariement des observations. Notons
également que, par défaut, R présente la différence de moyennes entre le
premier et le second traitement, et non le second moins le premier comme
calculé précédemment.

Le résultat significatif du test et le sens de la différence de moyenne
observée (au niveau du gain horaire de sommeil) est bien en accord avec les
conclusions des chercheurs. On peut visualiser les résultats sous forme d'un
diagramme en barres :
<<3-2e, fig=TRUE>>=
barchart(with(sleep, tapply(extra, group, mean)))
@ 
\cmd{barchart}
\end{sol}
\end{exo}
%
% Hollander 1999 p. 29
%
\begin{exo}\label{exo:3.3}
Dans une étude clinique, des chercheurs se sont intéressés à l'effet d'une
certaine forme de thérapie (par administration de tranquilisants) sur
l'évolution de 9 patients souffrant d'un trouble mixte combinant anxiété et
dépression. Le niveau de dépression était mesuré à partir de l'échelle de
dépression de Hamilton à l'inclusion (première visite) et lors d'une seconde
visite (après traitement). Il s'agit d'une échelle composé de 21 items à
plusieurs modalités de réponse ordonnées à partir de laquelle on peut
calculer un score total ou moyen. Les données recueillies sont indiquées
ci-dessous :\autocite[p.~29]{hollander99}
\begin{verbatim}
1.83 0.50 1.62 2.48 1.68 1.88 1.55 3.06 1.30
0.878 0.647 0.598 2.050 1.060 1.290 1.060 3.140 1.290
\end{verbatim}
On cherche à démontrer que le traitement a bien un effet se traduisant par
une diminution des scores moyens individuels. Pour cela, on se propose de
réaliser un test non-paramétrique.
\begin{description}
\item[(a)] Représenter la distribution des scores sous forme
  d'un diagramme de dispersion (en abscisses, données à la 1\iere\ visite ; en
  ordonnées, données à la 2\ieme\ visite).
\item[(b)] Effectuer un test de Wilcoxon.
\end{description}
\begin{sol}
Pour la saisie de données, on crée deux variables quantitatives représentant
les mesures individuelles collectées à la 1\iere\ et à la 2\ieme\ visite :
<<ex3-3a>>=
occ1 <- c(1.83,0.50,1.62,2.48,1.68,1.88,1.55,3.06,1.30)
occ2 <- c(0.878,0.647,0.598,2.05,1.06,1.29,1.06,3.14,1.29)
@   

L'affichage graphique des deux séries de mesure ne pose pas de difficulté :
<<ex3-3b, fig=TRUE>>=
xyplot(occ2 ~ occ1, type=c("p","g"), xlab="Première visite", ylab="Seconde visite")
@ 
\cmd{xyplot}

Pour faciliter la lisibilité, on pourrait rajouter une droite de pente 1. On
se contentera de superposer une grille pour faciliter la comparaison des
mesures avant-après (\verb|type="g"|).

Les données étant appariées (les mêmes patients sont mesurés à deux
occasions), il faut le préciser à R à l'aide de l'option \verb|paired=TRUE|.   
<<ex3-3c>>=
wilcox.test(occ1, occ2, paired=TRUE)
@   
\cmd{wilcox.test}
Le résultat du test suggère une amélioration de l'état (diminution du
score Hamilton de \Sexpr{round(mean(occ2-occ1),2)} points en moyenne).
\end{sol}
\end{exo}
%
% Selvin 1998 p. 323
%
\begin{exo}\label{exo:3.4}
Dans un essai clinique, on a cherché à évaluer un régime supposé réduire le
nombre de symptômes associé à une maladie bénigne du sein. Un groupe de 229
femmes ayant cette maladie ont été alétoirement réparties en deux
groupes. Le premier groupe a reçu les soins courants, tandis que les
patientes du second groupe suivaient un régime spécial (variable B =
traitement). Après un an, les individus ont été évalués et ont été classés
dans l'une des deux catégories : amélioration ou pas d'amélioration
(variable A = réponse). Les résultats sont résumés dans le tableau suivant,
pour une partie de l'échantillon :\autocite[p.~323]{selvin98}
\vskip1em

\begin{tabular}{l|cc|r}
& régime & pas de régime & total \\
\hline
amélioration & 26 & 21 & 47 \\
pas d'amélioration & 38 & 44 & 82 \\
\hline
total & 64 & 65 & 129
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Réaliser un test du chi-deux.  
\item[(b)] Quels sont les effectifs théoriques attendus sous une hypothèse
  d'indépendance ?
\item[(c)] Comparer les résultats obtenus en (a) avec ceux d'un test de
  Fisher.
\item[(d)] Donner un intervalle de confiance pour la différence de
  proportion d'amélioration entre les deux groupes de patientes.
\end{description}
\begin{sol}
On ne dispose que des données concernant les effectifs tels que reportés
dans le tableau, mais il n'est pas nécessaire d'avoir les données brutes
pour réaliser le test du $\chi^2$. Celui-ci est obtenu à partir de la
commande \texttt{chisq.test} et inclut par défaut une correction de
continuité (Yates).
<<ex3-4a>>=
regime <- matrix(c(26,38,21,44), nrow=2)
dimnames(regime) <- list(c("amélioration","pas d'amélioration"), c("régime","pas de régime"))
regime
chisq.test(regime)
@ 
\cmd{matrix}\cmd{dimnames}\cmd{list}\cmd{chisq.test}
Si l'on ne souhaite pas appliquer de correction de continuit, il faut
ajouter l'option \verb|correct=FALSE|.

Les effectifs théoriques ne sont pas affichés avec le résultat du test, mais
on peut les obtenir comme suit :
<<ex3-4b>>=
chisq.test(regime)$expected
@ %$

Concernant le test de Fisher, on procède de la même manière en utilisant la
comamnde \texttt{fisher.test} à partir du tableau de contingence.
<<ex3-4c>>=
fisher.test(regime)
@ 
% FIXME:
% finir commentaires
\end{sol}
\end{exo}
%
% Agresti 2002 p. 72
%
\begin{exo}\label{exo:3.5}
Dans un essai clinique, 1360 patients ayant déjà eu un infarctus dy myocarde
ont été assignés à l'un des deux groupes de traitement suivants : prise en
charge par aspirine à faible dose en une seule prise \emph{versus}
placebo. La table ci-après indique le nombre de décès par infarctus lors de
la période de suivi de trois ans :\autocite[p.~72]{agresti02} 
\vskip1em

\begin{tabular}{lccc}
\toprule
& \multicolumn{2}{c}{Infarctus} & \\
\cmidrule(r){2-3}
& Oui & Non & Total \\
\midrule
Placebo & 28 & 656 & 684 \\
Aspirine & 18 & 658 & 676 \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Calculer la proportion d'infarctus du myocarde dans les deux
  groupes de patients.
\item[(b)] Représenter graphiquement le tableau précédent sous forme d'un
  diagramme en barres ou d'un diagramme en points ("dotplot" de Cleveland).
\item[(c)] Indiquer la valeur de l'odds-ratio ainsi que du risque relatif.
\item[(d)] À partir de l'intervalle de confiance à 95~\% pour l'odds, quelle
  conclusion peut-on tirer sur l'effet de l'aspirine dans la prévention d'un
  infarctus du myocarde ?
\end{description}
\begin{sol}
On travaillera à partir du tableau de contingence directement (aucune des
questions posées ne nécessite d'avoir accès aux données individuelles).
<<ex3-5a>>=
aspirine <- matrix(c(28,18,656,658), nrow=2) 
dimnames(aspirine) <- list(c("Placebo","Aspirine"), c("Oui","Non"))
aspirine
@ 
\cmd{matrix}\cmd{dimnames}\cmd{list}
On retiendra que par défuat R organise les données en colonnes (sauf si l'on
précise l'option \verb|byrow=TRUE|), d'où la saisie des effectifs selon la
présence ou non d'un infarctus (cf. exercice~\ref{exo:3.4}). Les effectifs
marginaux (totaux lignes et colonnes) s'obtiennent simplement en calculant
les sommes des cellules correspondantes. Par exemple, pour reproduire le
premier total ligne du tableau donné, on pourrait procéder ainsi :
<<ex3-5b>>=
sum(aspirine["Placebo",])
@ 
\cmd{sum}
ou, de manière équivalente, \verb|sum(aspirine[1,])|. Pour obtenir
simultanément les deux totaux lignes, sans répéter la commande précédente,
on peut utiliser la commande \texttt{apply}. Par exemple, pour les totaux
lignes 
<<ex3-5c>>=
apply(aspirine, 1, sum)
@
\cmd{apply}
Dans cette expression, l'option \texttt{1} signifie que l'on souhaite
travailler "par lignes". Pour obtenir les totaux colonnes, on utilisera
<<ex3-5d>>=
apply(aspirine, 2, sum)
@ 
\cmd{apply}
D'où les proportions demandées :
<<ex3-5e>>=
round(aspirine[,"Oui"]/sum(aspirine[,"Oui"]) * 100, 2)
@ 
\cmd{round}

La distribution des effectifs peut être visualisée sous forme de diagramme
en barres comme ceci :
<<ex3-5f, fig=TRUE>>=
barchart(aspirine, horizontal=FALSE, stack=FALSE, ylab="Effectifs")
@ 
\cmd{barchart}

En fait, cette représentation graphique n'est pas très informative car la
prévalence observée est très faible dans les deux groupes de traitement. Une
représentation des données plus appropriée consisterait à afficher les
événements positifs (survenue d'un infarctus pendant la période de suivi)
rapportés aux effectifs marginaux (nombre total de patients randomisés dans
chaque groupe). Une solution possible est indiquée ci-après :
<<ex3-5g, fig=TRUE>>=
prop.infarctus <- aspirine[,"Oui"]/sum(aspirine[,"Oui"])
barchart(prop.infarctus, ylab="Fréquence relative d'infarctus")
@ 
\cmd{barchart}\cmd{sum}

On peut calculer l'odds-ratio et le risque relatif comme on l'a fait plus
haut, c'est-à-dire en travaillant directement avec les valeurs des cellules
extraites du tableau \texttt{aspirine}. Par exemple, pour le risque relatif
<<ex3-5h>>=
(aspirine["Aspirine","Oui"]/sum(aspirine["Aspirine",])) / (aspirine["Placebo","Oui"]/sum(aspirine["Placebo",]))
@ 
Toutefois, il existe des librairies spécialisées pour effectuer ce genre de
calcul, et fournir une estimation de la précision de ces estimateurs sous
forme d'intervalles de confiance. Pour calculer l'odds-ratio, nous
utiliserons donc la commande disponible dans le package \texttt{vcd}, qu'il
est nécessaire de charger au préalable à l'aide de la commande
\texttt{library} : 
<<ex3-5i>>=
library(vcd)
asp.or <- oddsratio(aspirine, log=FALSE) 
print(list(or=asp.or , conf.int=confint(asp.or))) 
summary(oddsratio(aspirine))
@
\cmd{oddsratio}\cmd{vcd}\cmd{confint}
L'estimé du "vrai" OR est plutôt imprécis (l'IC à 95~\% est large et
contient la valeur 1), et le test sur le $\log(\text{OR})$ n'est pas
significatif ($p = 0.071$). On concluera que rien ne permet d'affirmer que
la probabilité de décès dûe à un infarctus diffère selon le facteur
d'exposition.
\end{sol}
\end{exo}
%
% Peat 2005 p. 235
%
\begin{exo}\label{exo:3.6}
Une étude a porté sur 86 enfants suivis dans un centre pour apprendre à
gérer leur maladie. Lors de leur arrivée, on a demandé aux enfants s'ils
savaient gérer leur maladie dans les conditions optimales (non détaillées),
c'est-à-dire s'ils savaient à quel moment il leur fallait recourir au
traitement prescrit. La même question a été posée à ces enfants à la fin de
leur suivi dans le centre. La variable mesurée est la réponse (affirmative
ou non) à cette question à l'entrée et à la sortie de l'étude. Les données,
disponibles au format SPSS dans le fichier \texttt{health-camp.sav}, sont
résumées dans le tableau ci-après :\autocite[p.~235]{peat05}  
\vskip1em

\begin{tabular}{lll...}
\toprule
& & & \multicolumn{2}{c}{Gestion maladie (sortie)} &  \\
\cmidrule(r){4-5}
& & & Non & Oui & Total \\
\midrule
Gestion maladie (entrée) & Non & Effectif & 27 & 29 & 56 \\
& & Fréquence & 31.4~\% & 33.7~\% & 65.1~\% \\
& Oui & Effectif & 6 & 24 & 30 \\
& & Fréquence & 7.0~\% & 27.9~\% & 34.9~\% \\
Total & & Effectif & 33 & 53 & 86 \\
& & Fréquence & 38.4~\% & 61.6~\% & 100.0~\% \\
\bottomrule
\end{tabular}
\vskip1em

On se demande si le fait d'avoir suivi le programme de formation proposé
dans le centre a augmenté le nombre d'enfants ayant une bonne connaissance
de leur maladie et de sa gestion quotidienne.
\begin{description}
\item[(a)] Reproduire le tableau d'effectifs et de fréquences relatives
  précédent à partir des données brutes.
\item[(b)] Indiquer le résultat d'un test de McNemar.
\item[(c)] Comparer le résultat du test réalisé en (b) mais sans continuité
  de correction avec le résultat obtenu à partir d'un test binomial.
\end{description}
\begin{sol}
Il n'est pas vraiment nécessaire d'importer le tableau de données brutes et
l'on pourrait se contenter de travailler avec les effectifs présentés dans
le tableau, comme dans l'exercice~\ref{exo:3.5}. Toutefois, pour charger le
fichier SPSS, il est nécessaire d'utiliser une commande \R spécifique et
disponible dans le package \texttt{foreign}.
<<ex3-6a>>=
library(foreign)
hc <- read.spss("health-camp.sav", to.data.frame=TRUE)
@ 
\cmd{foreign}\cmd{read.spss}
L'option \verb|to.data.frame=TRUE| est importante car c'est celle qui assure
qu'à l'issue de l'importation les données seront bien stockées dans un
tableau où les lignes figurent les observations et les colonnes les
variables. Si l'on regarde comment les données sont représentées dans le
tableau importé, on constate que l'information est un peu vague :
<<ex3-6b>>=
head(hc)
@ 
\cmd{head}
Ceci s'explique par la perte des labels associés aux variables dans SPSS. On
peut retrouver cette informaiton en interrogeant la base de données \R avec
la commande \texttt{str} :
<<ex3-6c>>=
str(hc)
@ 
\cmd{str}
À partir de la ligne \verb|attr(*, "variable.labels")|, on voit donc que les
colonnes \texttt{BEFORE} et \texttt{AFTER} correspondent à la question
portant sur la connaissance de la maladie, alors que les colonnes
\texttt{BEFORE2} et \texttt{AFTER2} correspondent à la question portant sur
le traitement.

Finalement, on peut reproduire le tableau initial (effectifs et fréquences
relatives) ainsi :
<<ex3-6d>>=
table(hc[,c("BEFORE","AFTER")])
round(prop.table(table(hc[,c("BEFORE","AFTER")])), 2)
@ 
\cmd{table}\cmd{prop.table}
Pour obtenir les distributions marginales, on peut utiliser la commande
\texttt{margin.table}. 
<<ex3-6e>>=
margin.table(table(hc[,c("BEFORE","AFTER")]), 1)
margin.table(table(hc[,c("BEFORE","AFTER")]), 2)
@ 
\cmd{margin.table}

Pour réaliser le test de McNemar, on peut utiliser directement le tableau de
contingence construit précédemment. Par souci de commodité, on stockera ce
dernier dans une variable \R appelé \texttt{hc.tab}.
<<ex3-6f>>=
hc.tab <- table(hc[,c("BEFORE","AFTER")])
mcnemar.test(hc.tab)
@

On peut comparer les résultats du test de McNemar sans appliquer la
correction de continuité avec ceux d'un test binomial (test exact).
<<ex3-6g>>=
binom.test(6, 6+29)
mcnemar.test(hc.tab, correct=FALSE)
@ 
\end{sol}
\end{exo}

\Closesolutionfile{solutions}
\section*{Corrigés}
\input{solutions3}

%--------------------------------------------------------------- Devoir 02 ---
\chapter*{Devoir \no 2}
\addcontentsline{toc}{chapter}{Devoir \no 2}

%--------------------------------------------------------------- Chapter 04 --
\chapter{Analyse de variance et plans d'expérience}\label{chap:anova}
\Opensolutionfile{solutions}[solutions4]

\begin{center} 
\fbox{\begin{minipage}{5in} 
\textbf{\large Objectifs :} 

\noindent
Analyse de variance sur k groupes indépendants • Estimation ponctuelle et
 par intervalle de moyennes de groupe • Approche non-paramétrique pour la
 comparaison de k groupes indépendants • Comparaisons multiples • Test de
 tendance linéaire 
\end{minipage}} 
\end{center}

\vspace*{3ex}

\section*{Énoncés}
%
% Dupont 2009 p. 326
%
\begin{exo}\label{exo:4.1}
Dans une étude sur le gène du récepteur à \oe strogènes, des généticiens se
sont intéressés à la relation entre le génotype et l'âge de diagnostic du
cancer du sein. Le génotype était déterminé à partir des deux allèles d'un
polymorphisme de restriction de séquence (1.6 et 0.7 kb), soit trois groupes
de sujets : patients homozygotes pour l'allèle 0.7 kb (0.7/0.7), patients
homozygotes pour l'allèle 1.6 kb (1.6/1.6), et patients hétérozygotes
(1.6/0.7). Les données ont été recueillies sur 59 patientes atteintes d'un
cancer du sein, et sont disponibles dans le fichier
\texttt{polymorphism.dta} (fichier Stata). Les données moyennes sont
indiquées ci-dessous :\autocite[p.~327]{dupont09}
\vskip1em

\begin{tabular}{lrrrr}
\toprule
& \multicolumn{3}{c}{Génotype} & \\
\cmidrule(r){2-4}
& 1.6/1.6 & 1.6/0.7 & 0.7/0.7 & Total \\
\midrule
Nombre de patients & 14 & 29 & 16 & 59 \\
\emph{Âge lors du diagnostic} & & & & \\
\quad Moyenne & 64.64 & 64.38 & 50.38 & 60.64 \\
\quad Écart-type & 11.18 & 13.26 & 10.64 & 13.49 \\
\quad IC 95~\% & (58.1–71.1) & (59.9–68.9) & (44.3–56.5) & \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Tester l'hypothèse nulle selon laquelle l'âge de diagnostic ne varie
  pas selon le génotype à l'aide d'une ANOVA. Représenter sous forme
  graphique la distribution des âges pour chaque génotype.
\item[(b)] Les intervalles de confiance présentés dans le tableau ci-dessus ont
  été estimés en supposant l'homogénéité des variances, c'est-à-dire en
  utilisant l'estimé de la variance commune ; donner la valeur de ces
  intervalles de confiance sans supposer l'homoscédasticité. 
\item[(c)] Estimer les différences de moyenne correspondant à l'ensemble des
  combinaisons possibles des trois génotypes, avec une estimation de
  l'intervalle de confiance à 95~\% associé et un test paramétrique
  permettant d'évaluer le degré de significativité de la différence
  observée.
\end{description}
\begin{sol}
Pour charger les données, il est nécessaire d'importer la librairie
\texttt{foreign} qui permet de lire les fichiers enregistrés par Stata.
<<ex4-1a>>=
library(foreign)
polymsm <- read.dta("polymorphism.dta")
head(polymsm)
@ 
\cmd{read.dta}\cmd{library}\cmd{head}  
Notons que \texttt{polymsm} est un \texttt{data.frame}, ce qui sera utile
pour utiliser les commandes graphiques ou réaliser l'ANOVA. La première
colonne contient une série d'identifiants uniques pour les individus ; elle
ne sera pas utile dans le cas présent. Pour calculer les moyennes et
écart-types de chaque groupe, on peut procéder comme suit : 
<<ex4-1b>>=
with(polymsm, tapply(age, genotype, mean))
with(polymsm, tapply(age, genotype, sd))
@ 
\cmd{tapply}\cmd{mean}\cmd{sd}
La distribution des âges selon le génotype est indiquée dans le diagramme en
boîtes à moustaches suivant.
<<ex4-1c, fig=TRUE>>=
bwplot(age ~ genotype, data=polymsm)
@ 
\cmd{bwplot}

Il est également possible d'utiliser des histogrammes, en utilisant
\texttt{histogram} au lieu de \texttt{bwplot}.
<<ex4-1d, fig=TRUE>>=
histogram(~ age | genotype, data=polymsm)
@ 
\cmd{histogram}

Le modèle d'ANOVA est réalisé à l'aide de la commande \texttt{aov} :
<<ex4-1e>>=
aov.res <- aov(age ~ genotype, data=polymsm)
summary(aov.res)
@ 
\cmd{aov}\cmd{summary.aov} 
La statistique de test $F$ est reportée dans la colonne \texttt{F value},
avec le degré de significativité associé dans la colonne suivante
(\texttt{Pr(>F)}). L'ANOVA indique qu'au moins une paire de moyennes est
significativement différente, en considérant un rique d'erreur de 5~\%.

On peut vérifier l'exactitude des intervalles de confiance reportés dans le
tableau présenté plus haut. Pour cela, il nous faut une estimation de
l'erreur résiduelle, qui est simplement la racine carrée du carré moyen
associé au terme d'erreur dans le tableau d'ANOVA ci-dessus.
<<ex4-1f>>=
mse <- unlist(summary(aov.res))["Mean Sq2"]
se <- sqrt(mse)
@ 
\cmd{unlist}\cmd{sqrt}
Soit, une estimation de la racine de la variance commune de
\Sexpr{round(se,2)}. À partir de là, on peut construire les intervalles de
confiance à 95~\% pour chaque moyenne de groupe comme suit :
<<ex4-1g>>=
ni <- table(polymsm$genotype)
m <- with(polymsm, tapply(age, genotype, mean))
lci <- m - qt(0.975, n-3) * se / sqrt(ni)
uci <- m + qt(0.975, n-3) * se / sqrt(ni)
rbind(lci, uci) 
@ %$ 
\cmd{table}\cmd{mean}\cmd{qt}\cmd{sqrt}\cmd{rbind}
\end{sol}
\end{exo}
%
% METHO p. 87
%
\begin{exo}\label{exo:4.2}
On a mesuré en fin de traitement chez 18 patients répartis par tirage au
sort en trois groupes de traitement A, B, et C, un paramètre biologique dont
on sait que la distribution est normale. Les résultats sont les suivants :
\vskip1em

\begin{tabular}{ccc}
\toprule
A & B & C \\
\midrule
19.8 & 15.9 & 15.4 \\
20.5 & 19.7 & 17.1 \\
23.7 & 20.8 & 18.2 \\
27.1 & 21.7 & 18.5 \\
29.6 & 22.5 & 19.3 \\
29.9 & 24.0 & 21.2 \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Réaliser une ANOVA à un facteur.
\item[(b)] Selon le résultat du test, procéder aux comparaisons par paire de
  traitement des moyennes, en appliquant une correction simple de Bonferroni
  (c'est-à-dire où les degrés de significativité estimé sont multipliés par
  le nombre de comparaisons effectuées). Comparer avec de simples tests de
  Student non corrigés pour les comparaisons multiples. 
\item[(c)] D'après des études plus récentes, il s'avère que la normalité des
  distributions parentes peut-être remise en question. Effectuer la
  comparaison des trois groupes par une approche non-paramétrique.
\end{description}
\begin{sol}
Dans un premier temps, il s'agit de saisir les données sous un format
approprié pour leur traitement sous \R. Plutôt que de construire un tableau
à trois colonnes, il est préférable de construire un \texttt{data.frame},
contenant une colonne avec l'ensemble des mesures biologiques (soit $3\times
6=18$ observations) et une autre colonne codant pour le type de traitement
(trois niveaux, répétés 6 fois chacun).
<<ex4-2a>>=
pb <- c(19.8,20.5,23.7,27.1,29.6,29.9,
        15.9,19.7,20.8,21.7,22.5,24.0,
        15.4,17.1,18.2,18.5,19.3,21.2)
tx <- gl(3, 6, labels=c("A","B","C"))
dfrm <- data.frame(pb, tx)
head(dfrm, 8)
@ 
\cmd{gl}\cmd{data.frame}\cmd{head}
On peut produire un rapide résumé numérique (moyenne et variance) des
mesures enregistrées par groupe de traitement comme suit :
<<ex4-2b>>=
with(dfrm, tapply(pb, tx, mean))
with(dfrm, tapply(pb, tx, var))
@
\cmd{tapply}\cmd{mean}\cmd{var}
Enfin, on peut visualiser la distribution des mesures individuelles par
groupe de traitement à l'aide d'un diagramme de type boîte à moustaches.
<<ex4-2c, fig=TRUE>>=
bwplot(pb ~ tx, data=dfrm)
@ 
\cmd{bwplot}

À présent, il est possible de réaliser l'ANOVA avec la commande \texttt{aov}.
<<ex4-2d>>=
aov.res <- aov(pb ~ tx, data=dfrm)
summary(aov.res)
@ 
\cmd{aov}\cmd{summary.aov}
Le résultat du test significatif indique qu'au moins une paire de moyennes
peut être considérée comme significativement différente au seuil 5~\%. Pour
comparer l'ensemble des traitements deux à deux, en se protégeant de
l'inflation du risque d'erreur, il est possible d'utiliser la commande
\texttt{pairwise.t.test} pour réaliser des tests de Student, avec l'option
\verb|p.adjust="bonf"| pour appliquer une correction de Bonferroni.
<<ex4-2e>>=
pairwise.t.test(dfrm$pb, dfrm$tx, p.adjust="bonf")
@ 

Si l'hypothèse de normalité n'est pas réaliste ou peut être remise en
question, on peut réaliser une ANOVA en considérant les rangs des
observations avec la commande \texttt{kruskal.test} :
<<ex4-2f>>=
kruskal.test(pb ~ tx, data=dfrm)
@
La comparaison des paires de traitements peut être effectuée à l'aide de
simples tests de Wilcoxon pour échantillons indépendants, par exemple 
\texttt{wilcox.test}. 
<<ex4-2g, eval=FALSE>>=
wilcox.test(pb ~ tx, data=dfrm, subset=tx!="C")
@ 
ou bien
<<ex4-2h>>=
with(dfrm, wilcox.test(pb[tx=="A"], pb[tx=="B"]))
@ 
\end{sol}
\end{exo}
%
% Peat 2005 p. 113
%
\begin{exo}\label{exo:4.3}
Un service d'obstétrique s'intéresse au poids de nouveaux-nés nés à terme et
âgés de 1 mois. Pour cet échantillon de 550 bébés, on dispose également
d'une information concernant la parité (nombre de frères et soeurs), mais on
sait qu'il n'y aucune relation de gemellité parmi les enfants ayant des
frères et soeurs. L'objet de l'étude est de déterminer si la parité (4
classes) influence le poids des nouveaux-nés à 1 mois. Les données sont
résumées dans le tableau suivant, et elles sont disponibles dans un fichier
SPSS, \texttt{weights.sav}.\autocite[p.~113]{peat05}
\vskip1em

\begin{tabular}{lrrrrr}
\toprule
& \multicolumn{4}{c}{Nombre de frères et soeurs} & Total \\
& 0 & 1 & 2 & $\ge 3$ & \\
\midrule
\emph{Échantillon} & & & & \\ 
Effectif & 180 & 192 & 116 & 62 & 550 \\
Fréquence & 32.7 & 34.9 & 21.1 & 11.3 & 100.0 \\
\emph{Poids (kg)} & & & & \\
Moyenne & 4.26 & 4.39 & 4.46 & 4.43 & \\
Écart-type & 0.62 & 0.59 & 0.61 & 0.54 & \\
(Min–Max) & (2.92–5.75) & (3.17–6.33) & (3.09–6.49) & (3.20–5.48) & \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Vérifier les données reportées dans le tableau précédent.
\item[(b)] Procéder à une analyse de variance à un facteur. Conclure sur la
  significativité globale et indiquer la part de variance expliquée par le
  modèle.
\item[(c)] Afficher la distribution des poids selon la parité. Procéder à un
  test d'homogénéité des variances (rechercher dans l'aide en ligne le test
  de Levenne). 
\item[(d)] On décide de regrouper les deux dernières catégories (2 et $\ge
  3$). Refaire l'analyse et comparer aux résultats obtenus en (b).
\item[(e)] Réaliser un test de tendance linéaire (par ANOVA) sur les données
  recodées en trois niveaux pour la parité.
\end{description}
\begin{sol}
Pour charger les données, il est nécessaire d'importer la librairie
\texttt{foreign} qui permet de lire les fichiers enregistrés par SPSS.
<<ex4-3a>>=
library(foreign)
weights <- read.spss("weights.sav", to.data.frame=TRUE)
str(weights)
@
\cmd{foreign}\cmd{read.spss}
Comme on l'a vu dans l'exercice~\ref{exo:3.6}, l'option
\verb|to.data.frame=TRUE| est importante car c'est elle qui permet de
stocker les données lues sous forme de \texttt{data.frame} (variable en
colonnes, individus en lignes).

Dans un premier temps, procédons au résumé numérique uni- et bivarié des
données d'intérêt (variables \texttt{WEIGHT} et \texttt{PARITY}). Pour la
variable qualitative, les tableaux d'effectifs et de fréquences relatives
sont obtenus ainsi :
<<ex4-3b>>=
table(weights$PARITY)
round(prop.table(table(weights$PARITY))*100, 1)
@ 
\cmd{table}\cmd{prop.table}\cmd{round}
Pour la variable quantitative, les moyennes et écart-types par type de
parité sont obtenus ainsi :
<<ex4-3c>>=
round(with(weights, tapply(WEIGHT, PARITY, mean)), 2)
round(with(weights, tapply(WEIGHT, PARITY, sd)), 2)
@ 
\cmd{round}\cmd{mean}\cmd{sd}

Pour l'analyse de variance, on utilise le même principe qu'à
l'exercice~\ref{exo:4.2}. 
<<ex4-3d>>=
aov.res <- aov(WEIGHT ~ PARITY, data=weights)
summary(aov.res)
@
\cmd{aov}\cmd{summary.aov}
Le test $F$ est significatif, donc on peut rejeter l'hypothèse nulle
d'égalité des quatre moyennes. La part de variance expliquée est simplement
le rapport entre la somme des carrés (\texttt{Sum Sq}) associée au facteur
d'étude (\texttt{PARITY}), soit 3.48, et la somme des carrés totaux, soit
3.48+3.48+195.36 : on obtient 0.018, soit environ 2~\%.

Pour afficher la distribution des poids, on peut bien sûr utiliser des
boîtes à moustaches, comme dans les exercices précédents. Si l'on souhaite
visualiser directement les données individuelles, un diagramme de dispersion
conditionné sur les groupes est également intéressant.
<<ex4-3e, fig=TRUE>>=
stripplot(WEIGHT ~ PARITY, data=weights, ylab="Poids (kg)", jitter.data=TRUE)
@ 
\cmd{stripplot}

L'autre alternative consiste à utiliser des histogrammes pour chaque groupe.
<<ex4-3f, fig=TRUE>>=
histogram(~ WEIGHT | PARITY, data=weights)
@ 

On notera que l'on a ajouté un léger décalage aléatoire des données (sur
l'axe horizontal uniquement) en utilisant l'option \verb|jitter.data=TRUE|,
ce qui permet d'éviter le chevauchement total des points.

Le test de Levene n'est pas disponible dans les commandes de base de R, mais
on peut installer le package \texttt{car} qui fournit la commande
\texttt{leveneTest} :
<<ex4-3g>>=
library(car)
leveneTest(WEIGHT ~ PARITY, data=weights)
@ 

Pour regrouper les deux dernières catégories, on peut créer une nouvelle
variable et générer manuellement les nouvelles modalités associés, ou plus
simplement "recoder" la variable qualitative \texttt{PARITY} en une nouvelle
variable :
<<ex4-3h>>=
PARITY2 <- weights$PARITY
levels(PARITY2)[3:4] <- "2 siblings or more"
@ %$
Le modèle d'analyse de variance est construit de la même manière que
précédemment :
<<ex4-3i>>=
aov.res2 <- aov(WEIGHT ~ PARITY2, data=weights)
summary(aov.res2)
@ 

Enfin, pour réaliser un test de tendance centrale, il y a deux solutions :
soit par la méthode des contrastes, soit par une approche de régression
linéaire. Ces deux approches fournissent des résultats identiques, et
suppose généralement que les niveaux du facteur de classification sont
équi-espacés (variation du même nombre d'unités entre chaque niveau du
facteur). Dans le cas présent, on utlisera la commande \texttt{lm} pour
réaliser une régression linéaire simple (voir \ref{chap:reg},
p.~\pageref{chap:reg}, pour plus de détails). 
<<exo4-3j>>=
levels(PARITY2)
lm.res <- lm(WEIGHT ~ as.numeric(PARITY2), data=weights)
summary(lm.res)
@ 
Le test de la tendance linéaire pour l'ANOVA correspond au test de la pente
de la droite de régression, qui ici est significatif suggérant une
augmentation du poids moyen avec la taille de la fratrie.
De manière équivalente, on peut utiliser l'approche consistant à recoder le
facteur de classification en facteur à modalités ordonnées (ou niveaux) à
l'aide la commande \texttt{as.ordered}. Le test pour la tendance linéaire
correspondant au contraste nommé \texttt{as.ordered(PARITY2).L} dans la
sortie suivante.
<<exo4-3k>>=
levels(as.ordered(PARITY2))
summary(lm(WEIGHT ~ as.ordered(PARITY2), data=weights))
@
\end{sol}
\end{exo}
%
% ANOVA two-way parallel group
%
%% \begin{exo}
%%   ANOVA two-way parallel group
%% \end{exo}
%
% ANOVA two-way cross-over
%
%% \begin{exo}
%% Les données inclues dans le fichier \texttt{headache.txt} ont été collectées
%% dans un plan randomisé en essais croisés (cross-over) avec trois bras de
%% traitement visant à comparer deux analgésiques (A et B) et un placebo (P)
%% pour le traitement des céphalées.\autocite[p.~617]{fitzmaurice04} La
%% comparaison principale porte sur les deux traitements actifs (l'un des deux
%% incluant en plus de la caffeine). On notera qu'il n'y a que deux périodes,
%% donc les patients n'ont reçu que deux des trois traitements, par
%% randomisation. La réponse mesurée est la diminution moyenne de douleur. Le
%% fichier de données comporte un descriptif détaillé de l'étude ainsi que le
%% nom des variables.

%% Par souci de simplicité, on ne va s'intéresser qu'aux deux bras actifs, A et
%% B, d'où les deux facteurs d'intérêt suivants : période (1 et 2) et
%% traitements séquentiels (AB ou BA). Les résultats moyens sont résumés dans
%% le tableau suivant (Tableau 21.1, p.~618) :
%% \vskip1em

%% \begin{tabular}{lrrrrr}
%% \toprule
%% & & \multicolumn{2}{c}{Période 1} & \multicolumn{2}{c}{Période 2} \\
%% \cmidrule(r){3-6}
%% Séquence & N & Moyenne & DS & Moyenne & DS \\
%% \midrule
%% AB & 126 & 10.196 & 3.347 & 9.153 & 3.429 \\
%% BA & 127 & 9.581 & 3.881 & 10.791 & 3.530 \\
%% \bottomrule
%% \end{tabular}
%% \vskip1em

%% \begin{description}
%% \item[(a)] Après avoir importé les données et restreint le tableau de
%%   données aux seuls comparaisons AB et BA, reonstruire le tableau de
%%   synthèse ci-dessus.
%% \item[(b)] Résumer l'effet traitement dans un graphique, en prenant en
%%   considération les deux périodes et la séquence.
%% \item[(c)] Tester l'effet séquence (traitement) et l'éffet période à l'aide
%%   de tests t de Student. Conclure au risque $\alpha$ de 5~\%.
%% \item[(d)] Réaliser une ANOVA en considérant ces deux mêmes facteurs, et
%%   leur interaction. Existe-t-il un effet rémanent (carry-over) ? Conclure
%%   sur l'effet global du traitement. 
%% \end{description}
%% \begin{sol}
%% Le fichier de données \texttt{headache.txt} contient un en-tête de 35 lignes
%% qui comprend la description de l'étude et le nom des variables. Les données
%% réelles sont ensuite arrangées sous forme de 7 colonnes (lignes
%% 36--881). Pour importer ces données, il conviendra donc de lire le fichier à
%% partir de la 36\ieme\ ligne. La 6\ieme\ colonne est identique à la 3\ieme\ qui
%% peut donc être supprimée.

%% <<exo4-5a>>=
%% headache <- read.table("headache.txt", header=FALSE, skip=35)
%% headache <- headache[,-3]
%% varnames <- c("ID", "Center", "Sequence", "Period", "Treatment", "Response")
%% names(headache) <- varnames
%% head(headache)
%% @ 
%% Pour restreindre les résultats aux seuls comparaisons AB et BA (séquences 1
%% et 2), on utilise \texttt{subset} de la manière suivante :
%% <<exo4-5b>>=
%% headache <- subset(headache, Sequence == 1 | Sequence == 2)
%% headache$Sequence <- factor(headache$Sequence, levels=1:2, labels=c("BA","AB"))
%% headache$Period <- factor(headache$Period, levels=0:1, labels=1:2)
%% @
%% Pour reconstruire le tableau de synthèse (moyenne et écart-type par
%% séquence et période), on peut utiliser la commande \texttt{aggregate}. La
%% subtilité consiste à correctement identifier les séquences AB et BA, ce qui
%% a été fait à l'étape précédente en renommant les étiquettes associées à
%% cette variable qualitative.
%% <<exo4-5c>>=
%% resultats <- aggregate(Response ~ Sequence + Period, data=headache, mean)
%% xtabs(Response ~ Sequence + Period, resultats)
%% @ 
%% On procèdera de même pour les écart-types en remplaçant la commande
%% \texttt{mean} par \texttt{sd}.

%% Ces résultats peuvent être résumés dans un graphique d'interaction où l'on
%% représente les moyennes calculées ci-dessus.
%% <<exo4-5d, fig=TRUE>>=
%% xyplot(Response ~ Period, data=resultats, groups=Sequence, 
%%        type=c("b","g"), auto.key=list(corner=c(0,1)))
%% @ 

%% On peut tester séparément les effets séquence (traitement) et période à
%% l'aide de simples tests $t$ pour échantillons indépendants.
%% <<exo4-5e>>=
%% t.test(Response ~ Period, data=headache, var.equal=TRUE)
%% t.test(Response ~ Sequence, data=headache, var.equal=TRUE)
%% @ 
%% Ces résultats suggère que l'ordre d'administration des traitements
%% n'influence pas le niveau moyen de la réponse, mais en même temps on ne met
%% pas en évidence d'effet global du traitement (indépendemment de la
%% période). 

%% Le modèle d'ANOVA considérant les deux facteurs, \texttt{Sequence} (AB ou
%% BA) et \texttt{Period} (1 et 2), se construit sur la base de la formule
%% \verb|Response ~ Period + Sequence + Period:Sequence|, le dernier terme
%% correspondant à l'effet d'interaction. Ce modèle peut s'abbréger sous la
%% forme \verb|Response ~ Period * Sequence|.
%% <<exo4-5f>>=
%% aov.res <- aov(Response ~ Period * Sequence, data=headache)
%% summary(aov.res)
%% @ 
%% À l'évidence, on retrouve un effet d'interaction suggérant que les deux
%% effets de séquence et période sont inter-dépendants, sans qu'aucun des deux ne
%% soit significatifs à 5~\%. Ceci suggère 
%% \end{sol}
%% \end{exo}
%
% ANCOVA
%
%% \begin{exo}
%%   ANCOVA
%% \end{exo}
\Closesolutionfile{solutions}
\section*{Corrigés}
\input{solutions4}

\chapter*{Devoir \no 3}
\addcontentsline{toc}{chapter}{Devoir \no 3}

\chapter{Corrélation et régression linéaire}\label{chap:reg}
\Opensolutionfile{solutions}[solutions5]

\begin{center} 
\fbox{\begin{minipage}{5in} 
\textbf{\large Objectifs :} 

\noindent
Coefficient de corrélation linéaire de Bravais-Pearson et corrélation de
 rangs • Diagramme de dispersion • Régression linéaire simple • Intervalles
 de confiance pour une pente • Tests d'hypothèse sur deux pentes   
\end{minipage}} 
\end{center}

\vspace*{3ex}

\section*{Énoncés}
%
% Everitt 2011 p. 184
%
\begin{exo}\label{exo:5.1}
Une étude a porté sur une mesure de malnutrition chez 25 patients âgés de 7
à 23 ans et souffrant de fibrose kystique. On disposait pour ces patients de
différentes informations relatives aux caractéristiques antropométriques
(taille, poids, etc.) et à la fonction pulmonaire. \autocite[p.~180]{everitt01}
Les données sont disponibles dans le fichier \texttt{cystic.dat}.
\begin{description}
\item[(a)] Calculer le coefficient de corrélation linéaire entre les
  variables \texttt{PEmax} et \texttt{Weight}, ainsi que son intervalle de
  confiance à 95~\%.
\item[(b)] Tester si le coefficient de régression calculé en (a) peut être
  considéré comme significativement différent de 0.3 au seuil 5~\%.
\item[(c)] Afficher l'ensemble des données numériques sous forme de
  diagrammes de dispersion, soit 45 graphiques arrangés sous forme d'une
  "matrice de dispersion".
\item[(d)] Calculer l'ensemble des corrélations de Pearson et de Spearman
  entre les variables numériques. Reporter les coefficients de
  Bravais-Pearson supérieurs à 0.7 en valeur absolue.
\item[(e)] Calculer la corrélation entre \texttt{PEmax} et \texttt{Weight},
  en contrôlant l'âge (\texttt{Age}) (corrélation partielle). Représenter
  graphiquement la covariation entre \texttt{PEmax} et \texttt{Weight} en
  mettant en évidence les deux terciles les plus extrêmes pour la variable
  \texttt{Age}. 
\end{description}
\begin{sol}
Les données étant disponibles dans un format texte où les valeurs ("champs")
sont séparées par des tabulations, on utilisera la commande
\texttt{read.table} pour les importer dans \R. Comme la première ligne dans
le fichier permet d'identifier les variables, on ajoutera l'option
\verb|header=TRUE|. 
<<ex5-1a>>=
cystic <- read.table("cystic.dat", header=TRUE)
str(cystic)
summary(cystic)
@ 
\cmd{read.table}\cmd{str}\cmd{summary}
On voit d'emblée que le sexe des patients n'est pas codé sous la forme d'une
variable qualitative mais d'un nombre (0/1). Bien que cela ne soit pas
fondamentalement nécessaire dans cet exercice, il est toujours préférable de
convertir les variables dans le bon format.
<<ex5-1b>>=
cystic$Sex <- factor(cystic$Sex, labels=c("M","F"))
table(cystic$Sex)
@ %$
\cmd{factor}\cmd{table}

La corrélation linéaire entre \texttt{PEmax} et \texttt{Weight} est obtenue
à l'aide de la commande \texttt{cor}, qui par défaut calcule un coefficient
de corrélation de Bravais-Pearson :
<<ex5-1c>>=
with(cystic, cor(PEmax, Weight))
@ 
\cmd{with}\cmd{cor}
La construction un peu étrange \texttt{with(cystic, ...} permet de ne pas avoir
à répéter le nom du \texttt{data.frame}, \texttt{cystic}, pour désigner les
variables d'intérêt. Autrement, on aurait écrit : \verb|cor(cystic$PEmax, cystic$Weight)|. 
La commande \texttt{cor} ne permet que l'estimation ponctuelle du
paramètre. Pour obtenir l'intervalle de confiance associé, on utilisera
directement la commande \texttt{cor.test} qui répond en même temps au test
d'hypothèse sur la nullité du coefficient dans la population.
<<ex5-1d>>=
with(cystic, cor.test(PEmax, Weight))
@
\cmd{with}\cmd{cor.test}

Pour tester si ce coefficient de corrélation est significativement différent
d'une autre valeur que 0, il est nécessaire d'utiliser la commande
\texttt{r.test} du package \texttt{psych}.
Pour tester l'hypothèse $H_0:\, \rho=0.300$ au seuil 5~\%, on indiquera la
taille de l'échantillon, la corrélation dans la population et la corrélation
observée, soit :
<<ex5-1e>>=
library(psych)
r.test(25, 0.300, 0.6363)
@ 
\cmd{r.test}

Pour afficher l'ensemble des diagrammes de dispersion, on peut utiliser la
commande de base \texttt{pairs} ou bien la solution suivante :
<<ex5-1f, fig=TRUE>>=
splom(cystic[,-3], varname.cex=0.7, cex=.8)
@
\cmd{splom}

Notons que nous avons exclus la variable \texttt{Sex} qui figure dans la
3\ieme colonne du tableau de données. Les deux autres options
(\texttt{varname.cex} et \texttt{cex}) permettent de contrôler la taille des
polices et des points.

Pour calculer les corrélations de toutes les paires de variables, on peut
toujours utiliser la commande \texttt{cor}. Par contre, il ne sera pas
possible de tester ces corrélations avec la commande \texttt{cor.test} ; il
faudrait pour cela utiliser par exemple la commande \texttt{corr.test} du
package \texttt{psych}.
<<ex5-1g>>=
round(cor(cystic[,-3]), 3)
@
\cmd{round}\cmd{cor}
Pour les corrélations de Spearman, il suffit d'ajouter l'option
\verb|method="spearman"| comme ci-dessous :
<<ex5-1h>>=
round(cor(cystic[,-3], method="spearman"), 3)
@ 
\cmd{round}\cmd{cor}

La question du filtrage des corrélations les plus élevées en valeur absolue
est plus délicate car la matrice de corrélation contient deux fois la même
information puisqu'elle est symétrique. Il est relativement aisé d'isoler
les corrélations situées dans la diagonale supérieure ; par exemple, les
commandes suivantes nous donnent l'ensemble de ces corrélations (sans
considération de la paire de variables en question) :
<<ex5-1i>>=
cor.mat <- cor(cystic[,-3])
cor.mat[upper.tri(cor.mat)]
@ 
\cmd{cor}\cmd{upper.tri}
En revanche, il est plus élégant de transformer la matrice de corrélation en
un tableau à 3 colonnes indiquant pour chaque paire de variables la
corrélation de Pearson associée :
<<ex5-1j>>=
cor.mat[upper.tri(cor.mat, diag=TRUE)] <- NA
library(reshape)
subset(melt(cor.mat), abs(value) > 0.7)
@ 
\cmd{upper.tri}\cmd{library}\cmd{subset}\cmd{melt}\cmd{abs}
Pour éviter les doublons, on a recodé en valeurs manquantes les éléments
diagonaux et sous-diagonaux de la matrice de corrélation. Ensuite, on a
filtré les valeurs supérieures à 0.7 en valeur absolue.

Enfin, pour estimer la corrélation partielle entre \texttt{PEmax},
\texttt{Weight} et \texttt{Age}, on utilisera la commande \texttt{pcor.test}
du package \texttt{ppcor} :
<<ex5-1k>>=
library(ppcor)
with(cystic, pcor.test(PEmax, Weight, Age))
@ 
\cmd{library}\cmd{with}\cmd{pcor.test}
On voit que la corrélation entre \texttt{PEmax} et \texttt{Weight} diminue
fortement lorsque l'on tient compte de la corrélation entre ces deux
variables et l'âge.

<<ex5-1l, fig=TRUE>>=
cystic$Age.ter <- cut(cystic$Age, breaks=quantile(cystic$Age, c(0,0.33,0.66,1)), 
                      include.lowest=TRUE)
cystic2 <- subset(cystic, as.numeric(Age.ter) %in% c(1,3))
cystic2$Age.ter <- factor(cystic2$Age.ter)
xyplot(PEmax ~ Weight, data=cystic2, groups=Age.ter, auto.key=list(corner=c(0,1)))
@ %$
\cmd{cut}\cmd{subset}\cmd{factor}\cmd{xyplot}\cmd{quantile}

\noindent Les commandes utilisées ont réalisé, dans l'ordre : la création d'une
variable \texttt{Age.ter} représentant trois classes, déterminées à partir
des trois déciles ([7,12], (12,17] et (17,23]) ; la restriction du jeu de
données initiale aux seules observations comprises dans les premier et
troisième déciles de la variable \texttt{Age} ; la suppression de la
deuxième classe d'âge devenue inutile au niveau des labels associés à la
variable \texttt{Age.ter} ; et enfin, un diagramme de dispersion de
\texttt{PEmax}, en fonction de \texttt{Weight}, en soulignant à l'aide de
symboles différents les observations situées dans chacun des deux terciles
retenus. 
\end{sol}
\end{exo}
%
% METHO TD8 Exo 1
%
\begin{exo}\label{exo:5.2}
Les données disponibles dans le fichier \texttt{quetelet.csv} renseignent
sur la pression artérielle systolique (\texttt{PAS}), l'indice de Quetelet
(\texttt{QTT}), l'âge (\texttt{AGE}) et la consommation de tabac
(\texttt{TAB}=1 si fumeur, 0 sinon) pour un échantillon de 32 hommes de plus
de 40 ans. 
\begin{description}
\item[(a)] Indiquer la valeur du coefficient de corrélation linéaire entre
  la pression artérielle systolique et l'indice de Quetelet, avec un
  intervalle de confiance à 90~\%.
\item[(b)] Donner les estimations des paramètres de la droite de régression
  linéaire de la pression artérielle sur l'indice de Quetelet.
\item[(c)] Tester si la pente de la droite de régression est différente de 0
  (au seuil 5~\%).
\item[(d)] Représenter graphiquement les variations de pression artérielle
  en fonction de l'indice de Quetelet, en faisant apparaître distinctement
  les fumeurs et les non-fumeurs avec des symboles ou des couleurs
  différentes, et tracer la droite de régression dont les paramètres ont été
  estimés en (b). 
\item[(e)] Refaire l'analyse (b-c) en restreignant l'échantillon aux
  fumeurs.
\end{description}
\begin{sol}
Les données ont été exportées à partir d'un tableur de type Excel, avec
comme séparateur de champ le ";". On pourrait utiliser la commande
\texttt{read.table} et spécifier les options adéquates (\texttt{sep} et
\texttt{header}). Heureusement, \R offre des commandes qui simplifient cette
tâche : \texttt{read.csv} lit des fichiers dans lesquels le séparateur de
champ est "," tandis que \texttt{read.csv2} traite les fichiers avec un
séparateur de type ";".
<<ex5-2a>>=
dfrm <- read.csv2("quetelet.csv")
head(dfrm)
dfrm$TAB <- factor(dfrm$TAB, labels=c("NF","F"))
summary(dfrm[,-1])
@ 
\cmd{read.csv2}\cmd{head}\cmd{factor}\cmd{summary}
On a profité de l'inspection rapide des premières observations indiquant que
la variable "consommation de tabac" était représentée sous forme de nombres
(0/1) pour la recoder en variable qualitative avec labels plus informatifs
(\texttt{NF}=non-fumeur, \texttt{F}=fumeur).
  
L'estimation du coefficient de corrélation liénaire et son intervalle de
confiance à 90~\% ne pose pas de problème particulier : on utilisera la même
commande qu'à l'exercice~\ref{exo:5.1}, \texttt{cor.test}, en négligeant le
test d'hypothèse mais en modifiant l'option \texttt{conf.level} qui, par
défault, est fixée à 0.95.
<<ex5-2b>>=
with(dfrm, cor.test(PAS, QTT, conf.level=0.90))
@ 
\cmd{with}\cmd{cor.test}

La commande pour effectuer une régression linéaire, simple ou multiple, est
\texttt{lm} (pour \underline{l}inear \underline{m}odel). La forme générale
du modèle est symbolisée comme suit : variable réponse ~ variable
explivative. Les commandes \texttt{summary} et \texttt{coef} permettent
d'obtenir, respectivement, un tableau résumant les coefficients de
régression estimés à partir des données et leur degré de significativité (à
partir d'un test $t$ de Student), ainsi que d'autres informations que nous
discuterons plus loin, ou simplement les coefficients de régression
(ordonnée à l'origine et pente de la droite de régression).
<<ex5-2c>>=
reg.res <- lm(PAS ~ QTT, data=dfrm)
summary(reg.res)
coef(reg.res)
@ 
\cmd{lm}\cmd{summary.lm}\cmd{coef}
Le test sur la pente est directement accessible à partir du tableau de
régression ; ici $t=6.062$ et $p<0.001$. Le coefficient associé à
\texttt{QTT} reflète l'augmentation de \texttt{PAS} (21.5 points) lorsque
\texttt{QTT} varie d'une unité.

Pour représenter les données sous forme graphique, on utilisera
\texttt{xyplot} qui permet de représenter les variations d'une variable en
fonction des valeurs prises par une autre variable. Les options utilisées
ci-après permettent d'afficher les observations avec des symboles ou des
couleurs différentes selon le statut realtif à la consommation de tabac
(\texttt{group=TAB}) ainsi que les droites de régression estimées sur ces
deux sous-échantillons (option \verb|type="r"|).
<<ex5-2d, fig=TRUE>>=
xyplot(PAS ~ QTT, data=dfrm, groups=TAB, type=c("p", "g", "r"),
       auto.key=list(points=FALSE, lines=TRUE))
@ 
\cmd{xyplot}

\noindent Il existe une autre manière de représenter la droite de régression
associée estimée sur l'ensemble de l'échantillon ou un ou plusieurs
sous-groupes. Ici, l'option \verb|type="r"| simplifie beaucoup les choses
puisqu'elle tient compte de l'option de groupement, \texttt{groups=}.

Pour restreindre l'analyse aux seules observations pour lesquelles
\verb|TAB="F"| (les fumeurs), on peut utiliser la commande \texttt{subset}
pour filtrer les lignes du tableau de données. Toutefois, cette commande
peut également être utilisée sous forme d'une option lorsque l'on utilise la
commande \texttt{lm} pour le modèle de régression linéaire.
<<ex5-2e>>=
reg.res2 <- lm(PAS ~ QTT, data=dfrm, subset=TAB=="F")
summary(reg.res2)
coef(reg.res2)
@ 
\cmd{lm}\cmd{summary.lm}\cmd{coef}
\end{sol}
\end{exo}
%
% Dupont 2009 p. 63
%
\begin{exo}\label{exo:5.3}
Dans l'étude Framingham, on dispose de donnée sur la pression artérielle
systolique (\texttt{sbp}) et l'indice de masse corporelle (\texttt{bmi}) de
2047 hommes et 2643 femmes.\autocite[p.~63]{dupont09} On s'intéresse à la
relation entre ces deux variables (après transformation logarithmique) chez
les hommes et chez les femmes séparément.
Les données sont disponibles dans le fichier \texttt{Framingham.csv}.
\begin{description}
\item[(a)] Représenter graphiquement les variations entre pression
  artérielle et IMC chez les hommes et chez les femmes.
\item[(b)] Les coefficients de corrélation linéaire estimés chez les hommes
  et chez les femmes sont-ils significativement différents à 5~\% ?
\item[(c)] Estimer les paramètres du modèle de régression linéaire
  considérant la pression artérielle comme variable réponse et l'IMC comme
  variable explicative, pour ces deux sous-échantillons. Donner un
  intervalle de confiance à 95~\% pour l'estimé des pentes respectives.
\item[(d)] Tester l'égalité des deux coefficients de régression associés à
  la pente (au seuil 5~\%).
\end{description}
\begin{sol}
Cette fois, les données ont été générées à partir d'un tableur (Excel ou
autre) mais le séparateur de champ est la ",", d'où l'usage de
\texttt{read.csv} au lieu de \texttt{read.csv2} comme dans
l'exercice~\ref{exo:5.2}. 
<<ex5-3a>>=
fram <- read.csv("Framingham.csv")
head(fram)
str(fram)
@ 
\cmd{read.csv}\cmd{head}\cmd{str}
La variable \texttt{sex} est traitée comme une variable quantitative (1/2)
et pour faciliter l'interprétation, nous la recodons d'emblée en variable
qualitative. 
<<ex5-3b>>=
table(fram$sex)
fram$sex <- factor(fram$sex, labels=c("M","F"))
@ %$
\cmd{table}\cmd{factor}

Avant de répondre à la question concernant les variations entre pression
artérielle et IMC, on peut vouloir vérifier la présence de valeurs
manquantes. Cela ne gêne en rien l'estimation des moyennes et écart-types,
des paramètres de la droite de régression ou la représentation graphique des
données, mais cela permet de connaître le nombre de "cas complets" sur les
données d'intérêt.
<<ex5-3c>>=
apply(fram, 2, function(x) sum(is.na(x)))
@ 
\cmd{apply}\cmd{sum}\cmd{is.na}\cmd{function}
On utilise une commande \texttt{apply} pour répéter une même opération pour
chaque variable, cette opération consistant à compter (\texttt{sum}) le
nombre de valeurs manquantes (\texttt{is.na(x)}). On constate que pour l'une
des variables de notre modèles, l'IMC (\texttt{bmi}), 9 observations sont
manquantes. D'où la distribution par sexe suivante :
<<ex5-3d>>=
with(fram, table(sex[!is.na(bmi)]))
@
\cmd{with}\cmd{table}

Pour représenter les données dans un diagramme de dispersion, on prendra
garde au fait qu'en raison des "grands" effectifs par sous-groupe (plus de
2000 observations), il risque d'y avoir beaucoup de points qui se
chevauchent. Une possibilité est d'utiliser la semi-transparence et de
réduire la taille des points.
<<ex5-3e, fig=TRUE>>=
xyplot(sbp ~ bmi | sex, data=fram, type=c("p","g"), alpha=0.5, cex=0.7, pch=19)
@ 
\cmd{xyplot}

Les corrélations entre les variables \texttt{sbp} et \texttt{bmi} chez les
hommes et chez les femmes peuvent être obtenues à partir de la commande
\texttt{cor}, en restreignant bien sûr l'analyse à chacun des deux
sous-échantillons :
<<ex5-3f>>=
with(subset(fram, sex=="M"), cor(sbp, bmi, use="pair"))
with(subset(fram, sex=="F"), cor(sbp, bmi, use="pair"))
@ 
\cmd{with}\cmd{subset}\cmd{cor}
On notera que l'on a rajouté l'option \verb|use="pair"| (l'option complète
se lit \verb|"pairwise.complete.obs"| mais il est possible d'abréger les
options lorsque cela ne pose pas de problème d'ambiguïté) pour calculer les
corrélations sur l'ensemble des données observées. Pour tester si ces deux
coefficients estimés à partir des données peuvent être considérés comme
significativement différents au seuil 5~\%, il faut utiliser la commande
\texttt{r.test} du package \texttt{psych}.
<<ex5-3g>>=
library(psych)
r.test(n=2047, r12=0.23644, n2=2643, r34=0.37362)
@
\cmd{library}\cmd{r.test}

Pour vérifier l'effet de la transformation logarithmique sur la distribution
des variables \texttt{sbp} et \texttt{bmi}, on peut procéder de deux
manières : soit créer un \texttt{data.frame} contenant les valeurs de
densité de chaque histogramme, associées au type de variable (\texttt{bmi},
\texttt{log(bmi)}, \texttt{sbp} et \texttt{log(sbp)}), soit construire
séparément les quatre histogrammes et les combiner en une seule figure à
l'aide de la commande \texttt{grid.arrange} du package
\texttt{gridExtra}. Voici ce qui est obtenu avec cette deuxième solution :
<<ex5-3h, fig=TRUE>>=
library(gridExtra)
p1 <- histogram(~ bmi, data=fram)
p2 <- histogram(~ log(bmi), data=fram)
p3 <- histogram(~ sbp, data=fram)
p4 <- histogram(~ log(sbp), data=fram)
grid.arrange(p1, p2, p3, p4)
@ 
\cmd{library}\cmd{grid.arrange}

Le modèle de régression dans chaque sous groupe est établi comme suit :
<<ex5-3i>>=
reg.resM <- lm(log(sbp) ~ log(bmi), data=fram, subset=sex=="M")
reg.resF <- lm(log(sbp) ~ log(bmi), data=fram, subset=sex=="F")
summary(reg.resM)   # Hommes
confint(reg.resM)
summary(reg.resF)   # Femmes
confint(reg.resF)
@
\cmd{lm}\cmd{summary.lm}\cmd{confint}
Pour les intervalles de confiance, on utilise la commande
\texttt{confint}. On peut regrouper tous les résultats qui nous intéressent
dans un même tableau. Par exemple,
<<ex5-3j>>=
res <- data.frame(pente=c(coef(reg.resM)[2], coef(reg.resF)[2]), 
                  rbind(confint(reg.resM)[2,], confint(reg.resF)[2,]))
rownames(res) <- c("M","F")
colnames(res)[2:3] <- c("2.5 %", "97.5 %")
round(res, 3)
@
\cmd{data.frame}\cmd{rownames}\cmd{colnames}\cmd{round}\cmd{rbind}
\cmd{confint}\cmd{coef}
% FIXME: test de l'égalité des pentes
\end{sol}
\end{exo}
%
% Hosmer & Lemeshow 1989
%
\begin{exo}
À partir des données sur les poids à la naissance décrites dans
l'exercice~\ref{exo:2.4}, on cherche à étudier la relation entre le poids des
bébés (traité en tant que variable numérique, \texttt{bwt}) et deux
caractéristiques de la mère : son poids (\texttt{lwt}) et son origine
ethnique (\texttt{race}).
\begin{description}
\item[(a)] Représenter graphiquement la relation entre poids des bébés et
  poids des mères, en fonction de l'ethnicité des mères.
\item[(b)] Estimer les paramètres de la régression linéaire en considérant
  les poids des bébés comme variable réponse et les poids des mères centrés
  sur leur moyenne comme variable explicative. La pente estimée est-elle
  significative au seuil usuel de 5~\% ?
\item[(c)] Estimer les paramètres de la régression linéaire où cette fois la
  variable explicative est l'ethnicité des mères, la variable réponse
  restant le poids des bébés. Comparer la significativité du modèle dans son
  ensemble avec les résultats obtenus à partir d'une ANOVA à un facteur
  (ethnicité). 
%% \item[(d)] Refaire l'analyse de régression décrite en (c) après avoir
%%   modifié la manière dont \R génère les contrastes pour les variables
%%   qualitatives en tapant ceci à l'invite de commande \R :
%% \begin{verbatim}
%% > options(contrasts=c("contr.sum", "contr.poly"))
%% \end{verbatim}
%% Comparer avec les résultats précédents.
\end{description}
\begin{sol}
Les données peuvent être importées et recodées exactement comme dans les
solutions proposées pour l'exercice~\ref{exo:2.4}, soit les séries de
commandes \R suivantes :
<<ex5-4a, eval=FALSE>>=
<<ex2-4a>>
<<ex2-4b>>  
@ 

On peut représenter la relation entre poids des bébés et poids des mères à
l'aide d'un simple diagramme de dispersion. Ici, on a choisi de stratifier
sur l'ethnicité des mères et d'afficher trois graphiques séparés.
<<ex5-4c, fig=TRUE>>=
xyplot(bwt ~ lwt | race, data=birthwt, layout=c(3,1), type=c("p","g"), aspect=0.8)
@ 
\cmd{xyplot}

\noindent L'option \texttt{aspect=0.8} permet de modifier le rapport
largeur/hauteur des graphiques. L'option \verb|layout=c(3,1)| permet
d'afficher les trois graphiques côte à côte (1 ligne, 3 colonnes). En
modifiant légèrement la commande précédente, il serait tout à fait possible
d'afficher toutes les observations dans le même graphique mais en utilisant
des symboles ou couleurs différents selon l'origine ethnique :
\begin{verbatim}
> xyplot(bwt ~ lwt, data=birthwt, groups=race)
\end{verbatim}
\cmd{xyplot}

Les paramètres du modèle de régression linéaire sont estimés à partir de la
commande \texttt{lm}, sachant que la commande \texttt{scale} permet de
centrer et/ou réduire une série de mesures. Ici, on souhaite uniquement
centrer les poids des mères sur leur moyenne, pas les standardiser par
unités d'écart-type.
<<ex5-4b>>=
reg.res <- lm(bwt ~ scale(lwt, scale=FALSE), data=birthwt)
summary(reg.res)
@
\cmd{lm}\cmd{summary.lm}
Le test $t$ évaluant la nullité de l'hypothèse nulle pour la pente est
significatif si l'on considère un risque de première espèce de 5~\%
($p=0.011$).

Si l'on considère l'ethnicité comme variable explicative, le modèle de
régression est évalué de la même manière :
<<ex5-4d>>=
reg.res2 <- lm(bwt ~ race, data=birthwt)
summary(reg.res2)
@
\cmd{lm}\cmd{summary.lm}
L'ANOVA nous donne :
<<ex5-4e>>=
aov.res <- aov(bwt ~ race, data=birthwt)
summary(aov.res)
@
\cmd{aov}\cmd{summary.aov}
En fait, il est tout à fait possible d'obtenir le tableau de régression
précédent en utilisant \texttt{summary.lm} au lieu de \texttt{summary},
comme on peut le vérifier dans la sortie suivante.
<<ex5-4f>>=
summary.lm(aov.res)
@
\cmd{summary.lm}
Réciproquement, on peut tout à fait afficher un tableau d'ANOVA
correspondant au modèle de régression précédent :
<<ex5-4g>>=
anova(reg.res2)
@ 
\cmd{anova}
On peut vérifier que la statistique F est identique dans les deux cas (elle
vaut 4.913, pour 2 et 186 degrés de liberté).

Les contrastes utilisés dans le modèle de régression (\texttt{reg.res2})
sont appelés contrastes de traitement et ils permettent de tester la
différence entre les scores moyens (ici, l'âge) de deux catégories d'une
variable qualitative (ici, l'ethnicité), l'une des deux catégories servant
de catégorie dite \og de référence\fg. Avec R, la catégorie de référence est
toujours le premier niveau du facteur, en suivant l'ordre lexicographique,
soit dans le cas présent le niveau \texttt{White}. Les coefficients de
régression représentent alors la différence de poids moyen entre les
bébés des mères de la catégorie \texttt{Black} \emph{versus} \texttt{White}
(-383.03), et \texttt{Other} \emph{versus} \texttt{White} (-297.44). On peut
le vérifier en calculant manuellement ces différences de moyennes :
<<ex5-4h>>=
grp.means <- with(birthwt, tapply(bwt, race, mean))
grp.means[2:3] - grp.means[1]     # 1er modèle de régression (reg.res2)
@ 
%% Si l'on change la manière de coder les contrastes, par exemple en utilisant
%% des contrastes de 
%% <<ex5-4h>>=
%% op <- options(contrasts=c("contr.sum", "contr.poly"))
%% reg.res3 <- lm(bwt ~ race, data=birthwt)
%% summary(reg.res3)
%% options(op)
%% @ 
%% \cmd{options}\cmd{lm}\cmd{sumamry.lm}
%% On peut le vérifier à partir des moyennes de groupe :
%% <<ex5-4i>>=
%% grp.means <- with(birthwt, tapply(bwt, race, mean))
%% grp.means[2:3] - grp.means[1]     # 1er modèle de régression (reg.res2)
%% grp.means[1:2] - mean(grp.means)  # 2ème modèle de régression (reg.res3)
%% @ 
%% \cmd{with}\cmd{tapply}
%% Dans le premier modèle, l'intercept vaut \texttt{grp.means[1]}, tandis
%% que dans le second modèle il s'agit de la moyenne des moyennes de
%% groupe (\texttt{mean(grp.means)}). Les deux coefficients associés aux
%% pentes représentent dans le premier cas les déviations entre
%% \texttt{Black} et \texttt{Other} par rapport à \texttt{White}, et dans
%% le second cas entre \texttt{White} et \texttt{Black} et la moyenne des
%% trois groupes.
% \url{http://bit.ly/LFkFBg}.
\end{sol}
\end{exo}
\Closesolutionfile{solutions}
\section*{Corrigés}
\input{solutions5}

\chapter*{Devoir \no 4}
\addcontentsline{toc}{chapter}{Devoir \no 4}

\chapter{Mesures d'association en épidémiologie et régression logistique}\label{chap:logistic}
% FIXME:
% exo kappa ici ou dans chapitre comparaison deux groupes ?
\Opensolutionfile{solutions}[solutions6]

\begin{center} 
\fbox{\begin{minipage}{5in} 
\textbf{\large Objectifs :} 

\noindent
Calculer les principales grandeurs et mesures d'association à partir de
tableaux $2\times 2$ • Calculer un intervalle de confiance pour l'odds-ratio
• Construire un modèle de régression logistique et interpréter les
coefficients de régression • Établir les valeurs pronostiques et
diagnostiques d'un test 
\end{minipage}} 
\end{center}

\vspace*{3ex}


\section*{Énoncés}
%
% RC TD3 exo 2
%
\begin{exo}\label{exo:6.1}
On étudie l'effet d'un traitement prophylactique d'un macrolide à faibles
doses (Traitement A) sur les épisodes infectieux chez des patients atteints
de mucoviscidose dans un essai randomisé multicentrique contre placebo
(B). Les résultats sont les suivants :
\vskip1em

\begin{tabular}{lccc}
\toprule
& \multicolumn{2}{c}{Infection} & \\
\cmidrule(r){2-3}
& Non & Oui & Total \\
\midrule
Traitement (A) & 157 & 52 & 209 \\
Placebo (B) & 119 & 103 & 222 \\
Total & 276 & 155 & 431 \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] À partir d'un test du $\chi^2$, que peut-on répondre à la
  question : le traitement permet-il de prévenir la survenue d'épisodes
  infectieux (au seuil $\alpha=0.05$) ? Vérifier que les effectifs
  théoriques sont bien tous supérieurs à 5.
\item[(b)] Conclut-on de la même manière à partir de l'intervalle de
  confiance de l'odds-ratio associé à l'effet traitement ?
\item[(c)] On souhaite vérifier s'il existe une disparité du point de vue
  des pourcentages d'épisodes infectieux en fonction du centre. Les données
  par centre sont indiquées dans le tableau ci-après. Conclure à partir d'un
  test du $\chi^2$.

  \begin{table}[!htb] \hskip40pt
  \begin{minipage}[b]{0.33\linewidth}
  \scalebox{0.65}{\begin{tabular}{|l|r|r|r|}
    \multicolumn{1}{c}{} & \multicolumn{2}{c}{Infection} &  \multicolumn{1}{c}{} \\
    \cline{2-4}
    \multicolumn{1}{c|}{} & Non & Oui & Total \\
    \hline
    Traitement (A) & 51 & 8 & 59 \\
    \hline
    Placebo (B) & 47 & 19 & 66 \\
    \hline
    Total & 98 & 27 & 125 \\
    \hline
    \multicolumn{4}{c}{Centre 1}
  \end{tabular}} 
  \end{minipage} \hspace{0.1cm}
  \begin{minipage}[b]{0.3\linewidth}
  \scalebox{0.65}{\begin{tabular}{|l|r|r|r|}
    \multicolumn{1}{c}{} & \multicolumn{2}{c}{Infection} &  \multicolumn{1}{c}{} \\
    \cline{2-4}
    \multicolumn{1}{c|}{} & Non & Oui & Total \\
    \hline
    Traitement (A) & 91 & 35 & 126 \\
    \hline
    Placebo (B) & 61 & 71 & 132 \\
    \hline
    Total & 152 & 106 & 258 \\
    \hline
    \multicolumn{4}{c}{Centre 2}
  \end{tabular}} 
  \end{minipage} \hspace{0.1cm}
  \begin{minipage}[b]{0.3\linewidth}
  \scalebox{0.65}{\begin{tabular}{|l|r|r|r|}
    \multicolumn{1}{c}{} & \multicolumn{2}{c}{Infection} &  \multicolumn{1}{c}{} \\
    \cline{2-4}
    \multicolumn{1}{c|}{} & Non & Oui & Total \\
    \hline
    Traitement (A) & 15 & 9 & 24 \\
    \hline
    Placebo (B) & 11 & 13 & 24 \\
    \hline
    Total & 26 & 22 & 48 \\
    \hline
    \multicolumn{4}{c}{Centre 3}
  \end{tabular}}
  \end{minipage}
  \end{table}
\item[(d)] À partir du tableau précédent, on cherche à vérifier si l'effet
  traitement est indépendent du centre ou non. On se propose de réaliser un
  test de comparaison entre les deux traitements ajustés sur le centre (test
  de Mantel-Haenszel). Indiquer le résultat du test ainsi que la valeur de
  l'odds-ratio ajusté.
\end{description}
\begin{sol}
Dans un premier temps, il faut créer le tableau de données sous \R.
<<ex6-1a>>=
macrolid <- matrix(c(157,119,52,103), nr=2, 
                   dimnames=list(traitement=c("A","B"), 
                     infection=c("Non","Oui")))
macrolid
@ 
\cmd{matrix}

On peut représenter graphiquement les données en considérant les proportions
relatives d'épisodes infectieux selon le type de traitement (52/209 contre
103/222) sous forme de diagrammes en barres :
<<ex6-1b, fig=TRUE>>=
barchart(prop.table(macrolid, 1), xlab="Fréquence relative", 
         auto.key=list(space="top", column=2, title="Infection"))
@ 
\cmd{barchart}\cmd{prop.table}

Le test du $\chi^2$ est simple à mettre en \oe uvre à l'aide de la commande
\texttt{chisq.test} :
<<ex6-1c>>=
chisq.test(macrolid, correct=FALSE)
@
\cmd{chisq.test}
On a désactivé la correction de continuité proposée par défaut par \R avec
l'option \verb|correct=FALSE|. Le résultat du test suggère l'existence d'une
association entre le traitement par macrolides et la survenue d'un épisode
infectieux au seuil usuel de 5~\%. On peut obtenir les effectifs théoriques
à partir de la même commande :
<<ex1-6d>>=
chisq.test(macrolid, correct=FALSE)$expected
@ %$
\cmd{chisq.test}
En règle générale, \R signalera que l'approximation par la distribution du
$\chi^2$ est incorrecte dans le cas où les effectifs théoriques sont trop
petits. 

L'odds-ratio peut se calculer manuellement, en tenant compte de l'ordre de
présentation des modalités des variables dans le tableau (il faut
intervertir l'ordre des colonnes) :
<<ex1-6e>>=
(52*119)/(103*157)
macrolid.bis <- matrix(nc=2, nr=2)
macrolid.bis[,1:2] <- macrolid[,2:1]  # échange colonne 1 et 2
or <- (macrolid.bis[1,1]*macrolid.bis[2,2]) / (macrolid.bis[2,1]*macrolid.bis[1,2])
or
@ 
\cmd{matrix}
de même que son intervalle de confiance
<<ex1-6f>>=
se <- sqrt(sum(1/macrolid.bis))  # erreur standard
or * exp(qnorm(0.025)*se)        # borne inf. 95 %
or * exp(qnorm(0.975)*se)        # borne sup. 95 %
@ 
\cmd{sqrt}\cmd{sum}\cmd{exp}\cmd{qnorm}
Mais en règle générale on préfèrera utiliser la commande \texttt{oddsratio}
du package \texttt{vcd}.
<<ex1-6g>>=
library(vcd)
oddsratio(macrolid.bis, log=FALSE)
confint(oddsratio(macrolid.bis, log=FALSE))
@ 
\cmd{library}\cmd{oddsratio}\cmd{confint}
On concluerait donc de la même manière qu'avec le test $\chi^2$ puisque
l'intervalle de confiance à 95~\% pour l'odds-ratio ne contient pas la
valeur 1 (signifiant l'absence d'association entre les deux variables
étudiées). 

Pour vérifier une éventuelle disparité entre les centres, il nous faut dans
un premier temps calculer les pourcentages d'épisodes infectieux dans chaque
centre. On peut le faire manuellement à partir des tableaux données dans
l'énoncé (par exemple, pour le centre 1 il s'agit de 27/125), auquel cas il
nous suffirait de créer un tableau avec les totaux colonnes :
<<ex1-6h>>=
macrolid.centre <- matrix(c(98,27,152,106,26,22), nr=3, byrow=TRUE)
rownames(macrolid.centre) <- paste("Centre", 1:3, sep=":")
colnames(macrolid.centre) <- c("Non","Oui")
macrolid.centre
@
\cmd{matrix}\cmd{rownames}\cmd{colnames}\cmd{paste}
Le test du $\chi^2$ est ensuite facile à réaliser :
<<ex1-6i>>=
chisq.test(macrolid.centre)
@ 
\cmd{chisq.test}
Mais comme à la question suivante on aura besoin de l'ensemble des données,
on peut également décider de travailler avec les trois tableaux. Dans ce
cas, pour répondre à la question de l'hétérogénéité entre centre du point de
vue du critère principal, il nous faudra recalculer les effectifs marginaux.
<<ex1-6j>>=
tab1 <- matrix(c(8,19,51,47), nr=2)
tab2 <- matrix(c(35,71,91,61), nr=2)
tab3 <- matrix(c(9,13,15,11), nr=2)
colnames(tab1) <- colnames(tab2) <- colnames(tab3) <- c("Oui","Non")
rownames(tab1) <-rownames(tab2) <- rownames(tab3) <- c("A","B")
@
\cmd{matrix}\cmd{colnames}\cmd{rownames}
Une fois les trois tableaux saisis, on calcule et on assemble les marges ainsi :
<<ex1-6k, eval=FALSE>>=
macrolid.centre <- rbind(centre1=apply(tab1, 2, sum), 
                         centre2=apply(tab2, 2, sum), 
                         centre3=apply(tab3, 2, sum))
chisq.test(macrolid.centre)
@ 
\cmd{rbind}\cmd{chisq.test}
La dernière commande produit bien évidemment le même résultat que celui
affiché précédemment.

Enfin, pour réaliser un test de Mantel-Haenszel, il faut réarranger les
données de sorte que chacun des trois tableaux construits à l'étape
précédent soit bien interprété par \R comme désignant une strate. Pour cela,
on utilise la commande \texttt{array} qui permet de généraliser la commande
\texttt{matrix} à des tableaux à plus de deux dimensions.
<<ex1-6l>>=
macrolid2 <- array(c(tab1, tab2, tab3), dim=c(2,2,3))
dimnames(macrolid2) <- list(c("A","B"), c("Oui","Non"), paste("Centre", 1:3, sep=":"))
macrolid2
mantelhaen.test(macrolid2)
@ 
\cmd{array}\cmd{dimnames}\cmd{mantelhaen.test}
La valeur de l'odds-ratio ajusté figure à la fin du résultat produit par \R
: dans ce cas, il est estimé à 0.362, avec un intervalle de confiance à
95~\% de [0.238–0.551]. Le résultat du test indique bien entendu que
l'odds-ratio ajusté peut être considéré comme significativement différent de
1 (absence d'association).

Le graphique suivant montre les trois odds-ratio et leurs intervalles de
confiance à 95~\%.
<<ex1-6m, fig=TRUE>>=
tab1.or <- oddsratio(tab1, log=FALSE)
tab2.or <- oddsratio(tab2, log=FALSE)
tab3.or <- oddsratio(tab3, log=FALSE)
dfrm <- data.frame(centre=factor(1:3), or=c(tab1.or, tab2.or, tab3.or), 
                   rbind(confint(tab1.or), confint(tab2.or), confint(tab3.or)))
dfrm
# -- %< -----
prepanel.ci <- function(x, y, lx, ux, subscripts, ...) {
  x <- as.numeric(x)
  lx <- as.numeric(lx[subscripts])
  ux <- as.numeric(ux[subscripts])
  list(ylim = range(x, ux, lx, finite=TRUE))
}
panel.ci <- function(x, y, lx, ux, subscripts, pch=16, ...) {
  x <- as.numeric(x)
  y <- as.numeric(y)
  lx <- as.numeric(lx[subscripts])
  ux <- as.numeric(ux[subscripts])
  panel.abline(h = 1, col = "black", lty = 2)
  panel.abline(v = unique(x), col = "grey")
  panel.arrows(x, lx, x, ux, col = 'black',
               length = 0.05, unit = "native",
               angle = 90, code = 3)
  panel.xyplot(x, y, pch = pch, ...)
}   
# -- %< -----
xyplot(or ~ centre, data=dfrm, pch=20, ylab="Odds-ratio (IC 95 %)", 
       lx=dfrm$lwr, ux=dfrm$upr, prepanel=prepanel.ci, panel=panel.ci)
@
\cmd{xyplot}\cmd{oddsratio}\cmd{data.frame}
\end{sol}
\end{exo}
%
% Pepe 2004 p. 22
%
\begin{exo}\label{exo:6.2}
Voici les résultats d'une étude de cohorte visant à déterminer, entre
autres, l'intérêt d'utliser comme outil de screening une mesure de test
d'effort physique (EST), pour lequel un résultat de type réussite/échec peut
être dérivé, lors du diagnostic d'une maladie coronarienne
(CAD).\autocite{pepe04}  
\vskip1em

\begin{tabular}{l|l|c|c|c}
\multicolumn{2}{c}{}&\multicolumn{2}{c}{CAD}&\\
\cline{3-4}
\multicolumn{2}{c|}{}&Non-malade&Malade&\multicolumn{1}{c}{Total}\\
\cline{2-4}
\multirow{2}{*}{EST}& Négatif & 327 & 208 & 535\\
\cline{2-4}
& Positif & 115 & 815 & 930\\
\cline{2-4}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{Total} & \multicolumn{1}{c}{442} & 
\multicolumn{1}{c}{1023} & \multicolumn{1}{c}{1465}\\
\end{tabular}
\vskip1em
On fera l'hypothèse qu'il n'y a pas de biais de vérification.

\begin{description}
\item[(a)] À partir de cette matrice de confusion, indiquer les valeurs
  suivantes (avec intervalles de confiance à 95~\%) : sensibilité et
  spécificité, valeur prédictive positive et négative. 
\item[(b)] Quelle est la valeur de l'aire sous la courbe pour les données
  reportées ?
\end{description}
\begin{sol}
La création du tableau de données peut se faire comme indiqué ci-après, à
partir d'un tableau de type \texttt{matrix} :
<<ex6-2a>>=
tab <- as.table(matrix(c(815,115,208,327), nrow=2, byrow=TRUE, 
                       dimnames=list(EST=c("+","-"), CAD=c("+","-"))))
tab
@   
\cmd{matrix}
On notera que le tableau a été légèrement réorganisé de manière à
correspondre aux notations habituelles, avec les événements \og positifs\fg\
sur la première ligne (typiquement, l'exposition) et la première colonne
(typiquement, la maladie) du tableau.

Il serait tout à fait possible de calculer en très peu d'opérations
arithmétiques avec \R toutes les quantités demandées. Cependant, le package
\texttt{epiR} contient toutes les commandes nécessaires pour répondre aux
questions épidémiologiques sur les tableaux $2\times 2$. Ainsi, la commande
\texttt{epi.tests} fournit les valeurs de prévalence,
sensibilité/spécificité, et valeurs prédictive positive et négative.
<<ex6-2b>>=
library(epiR)
epi.tests(tab)
@ 
Concernant l'aire sous la courbe, on peut également utiliser une commande
externe, \texttt{roc.from.table}, dans le package \texttt{epicalc}.
<<>>=
library(epicalc)
roc.from.table(tab, graph=FALSE)
@ 
\cmd{library}\cmd{roc.from.table}
\end{sol}
\end{exo}
%
% RC TD8 diagnostic cutoff
%
\begin{exo}\label{exo:6.3}
On dispose de données issues d'une étude cherchant à établir la validité
pronostique de la concentration en créatine kinase dans l'organisme sur la
prévention de la survenue d'un infarctus du myocarde.\autocite[p.~115]{rabe-hesketh04}

Les données sont disponibles dans le fichier \texttt{sck.dat} : la première
colonne correspond à la variable créatine kinase (\texttt{ck}), la deuxième
à la variable présence de la maladie (\texttt{pres}) et la dernière à la
variable absence de maladie (\texttt{abs}).
\begin{description}
\item[(a)] Quel est le nombre total de sujets ?
\item[(b)] Calculer les fréquences relatives malades/non-malades, et
  représenter leur évolution en fonction des valeurs de créatine kinase à
  l'aide d'un diagramme de dispersion (points + segments reliant les points).
\item[(c)] À partir d'un modèle de régression logistique dans lequel on
  cherche à prédire la probabilité d'être malade, calculer la valeur de
  \texttt{ck} à partir de laquelle ce modèle prédit que les personnes
  présentent la maladie en considérant une valeur seuil de 0.5
  (si $P(\text{malade})\ge 0.5$ alors \texttt{malade=1}).
\item[(d)] Représenter graphiquement les probabilités d'être malade prédites
  par ce modèle ainsi que les proportions empiriques en fonction des valeurs
  \texttt{ck}. 
\item[(e)] Établir la courbe ROC correspondant au seuil 0.5, et reporter la
  valeur de l'aire sous la courbe. Quelle est le taux de classification
  correcte pour le seuil de probabilité choisi en (c).
\item[(f)] Quelle est la valeur de seuil optimisant le compromis
  sensibilité/spécificité ?
\end{description}
\begin{sol}
Comme le nom des variables ne figure pas dans le fichier de données, il
faudra les attribuer aussitôt après avoir importé les données.
<<ex6-3a>>=
sck <- read.table("sck.dat", header=FALSE)
names(sck) <- c("ck", "pres", "abs")
summary(sck)
@ 
\cmd{read.table}\cmd{names}\cmd{summary}

Le nombre total de sujets correspond à la somme des effectifs pour les deux
variables \texttt{pres} et \texttt{abs}, soit
<<ex6-3b>>=
sum(sck[,c("pres","abs")])
@ 
\cmd{sum}
ou de manière équivalente : \verb|sum(sck$pres) + sum(sck$abs)| (mais pas
\verb|sck$pres + sck$abs| !).

Pour calculer les fréquences relatives de ces deux variables, il est
nécessaire de connaître les effectifs totaux par variable. Ceux-ci peuvent
être obtenu en utilisant la commande \texttt{apply} et en opérant par
colonnes :
<<ex6-3c>>=
ni <- apply(sck[,c("pres","abs")], 2, sum)
@ 
\cmd{apply}
À partir de là, il suffit de diviser chaque valeur des variables
\texttt{pres} et \texttt{abs} par les nombres calculés ci-dessus. On
stockera les valeurs obtenues dans deux nouvelles variables, dans le même
tableau de données.
<<ex6-3d>>=
sck$pres.prop <- sck$pres/ni[1]
sck$abs.prop <- sck$abs/ni[2]
@ 
On peut vérifier que les calculs sont corrects : la somme des valeurs pour
chaque variable doit maintenant valoir 1 :
<<ex6-3e>>=
apply(sck[,c("pres.prop","abs.prop")], 2, sum)
@ 
\cmd{apply}
Il suffit ensuite de représenter les proportions obtenues dans un même
graphique, en considérant les valeurs de la variable \texttt{ck} comme
abscisses.
<<ex6-3f, fig=TRUE>>=
xyplot(pres.prop + abs.prop ~ ck, data=sck, type=c("b", "g"),
       auto.key=TRUE, ylab="Fréquence")
@ 
\cmd{xyplot}

L'instruction \verb|type=c("b", "g")| signifie que l'on souhaite afficher
des points reliés par des lignes (\verb|"b"|=\verb|"o"|+\verb|"l"|)
ainsi qu'un quadrillage (\verb|"g"|).

Le modèle de régression pour données groupées
<<ex6-3g>>=
glm.res <- glm(cbind(pres, abs) ~ ck, data=sck, family=binomial)
summary(glm.res)
@ 
\cmd{glm}\cmd{summary.glm}

Les prédictions, exprimées sous forme de probabilités et non sur l'échelle
log-odds, sont obtenues à l'aide de la commande \texttt{predict} en
précisant l'option \verb|type="response"| comme ci-dessous :
<<ex6-3h>>=
glm.pred <- predict(glm.res, type="response")
names(glm.pred) <- sck$ck
@ %$
\cmd{predict}\cmd{names}
En considérant que des probabilités $\ge 0.5$ signifie que les individus
sont malades, on obtient donc la répartition suivante :
<<ex6-3i>>=
glm.pred[glm.pred >= 0.5]
@ 
On en conclut que les personnes seront considérées malades, selon ce modèle,
pour des valeurs \texttt{ck} de 80 ou plus.

Cela se vérifie aisément sur un graphique dans lequel on reporte les
probabilités prédites en fonction des valeurs de la variable \texttt{ck}.
<<ex6-3j, fig=TRUE>>=
sck$malade <- sck$pres/(sck$pres+sck$abs)
xyplot(glm.pred ~ sck$ck, type="l", 
       ylab="Probabilité", xlab="ck", 
       panel=function(...) {
         panel.xyplot(...)
         panel.xyplot(sck$ck, sck$malade, pch=19, col="grey")
       })
@ %$
\cmd{xyplot}

Dans un premier temps, il faut "décompacter" les données groupées et crée un
tableau avec deux colonnes : la première représentant la variable
\texttt{ck} et la seconde représentant la présence ou absence de maladie. On
utilisera les effectifs par sous-groupe calculé précédemment et disponible
dans la variable \texttt{ni}.
% FIXME:
% voir epitools::expand.table
<<ex6-3k>>=
sck.expand <- data.frame(ck=c(rep(sck$ck, sck$pres), rep(sck$ck, sck$abs)), 
                         malade=c(rep(1, ni[1]), rep(0, ni[2])))
table(sck.expand$malade)
with(sck.expand, tapply(malade, ck, sum))
@ %$
\cmd{data.frame}\cmd{table}\cmd{tapply}
Les deux dernières commandes visent à s'assurer que l'on retombe bien sur
les mêmes effectifs et que la distribution des personnes malades par valeur
de \texttt{ck} est correcte. On peut également vérifier que l'on obtient
bien les mêmes résultats concernant la régression logistique, et en profiter
pour ajouter les valeurs prédites au tableau de données précédent (on
pourrait procéder comme précédemment et répliquer les prédictions, mais cela
est plus simple ainsi)
<<ex6-3l>>=
glm.res2 <- glm(malade ~ ck, data=sck.expand, family=binomial)
sck.expand$prediction <- ifelse(predict(glm.res2, type="response") >= 0.5, 1, 0)
with(sck.expand, table(malade, prediction))
@  %$
\cmd{glm}\cmd{ifelse}\cmd{predict}\cmd{with}\cmd{table}
La dernière commande permet d'afficher une matrice de confusion dans
laquelle on croise les diagnostiques réels et ceux prédits par le modèle de
régression. On peut ainsi comparer les taux de classification correcte
lorsque l'on varie le seuil de référence :
<<ex6-3m>>=
classif.tab <- with(sck.expand, table(malade, prediction))
sum(diag(classif.tab))/sum(classif.tab)     # seuil à 0.5
classif.tab2 <- table(sck.expand$malade, ifelse(predict(glm.res2, type="response") >= 0.15, 1, 0))
sum(diag(classif.tab2))/sum(classif.tab2)   # seuil à 0.15
@ %$
\cmd{with}\cmd{sum}\cmd{diag}\cmd{table}\cmd{ifelse}\cmd{predict}
Clairement, en considérant un seuil plus bas, on augmente le taux de faux
positifs et on diminue le taux de classification correcte (qui passe de
91.4~\% à 87.8~\%). 

Pour afficher la courbe ROC, on utilisera le package \texttt{ROCR}.
<<ex6-3n, fig=TRUE>>=
library(ROCR)
pred <- prediction(predict(glm.res2, type="response"), sck.expand$malade)
perf <- performance(pred, "tpr","fpr")
plot(perf, ylab="Sensibilité", xlab="1-Spécificité")
grid()
abline(0, 1, lty=2)
@ %$
\cmd{library}\cmd{prediction}\cmd{performance}\cmd{plot}\cmd{grid}\cmd{abline}

La valeur de l'aire sous la courbe est obtenue comme suit :
<<ex6-3o>>=
performance(pred, "auc")@"y.values"
@ 

% FIXME:
% for previous exercice
% Une autre solution consiste à utiliser la commande \texttt{lroc} dans le
% package \texttt{epicalc}, mais celle-ci offre moins de
% facilités. L'avantage, en revanche, est que cette commande permet de
% travailler directement avec des matrices de confusion avec des modèles de
% régression logistique estimés à partir de données groupées.
% <<ex6-3p, eval=FALSE, fig=FALSE>>=
% library(epicalc)
% lroc(glm.res, line.col="black", grid.col="grey70", auc.coords=c(0.4,0.2))
% @ 

Enfin, pour représenter le compromis sensibilité/specificité en fonction du
choix 
\end{sol}
\end{exo}
% 
% Everitt 2001 p. 208
%
\begin{exo}\label{exo:6.4}
Le tableau suivant résume la proportion d'infarctus du myocarde observée
chez des hommes âgés de 40 à 59 ans et pour lesquels on a relevé le niveau
de tension artérielle et le taux de cholesterol, considérées sous forme de
classes ordonnées.
\vskip1em

\begin{tabular}{l///////}
\toprule
& \multicolumn{7}{c}{Cholesterol (mg/100 ml)} \\
\cmidrule(r){2-8}
TA & \multicolumn{1}{c}{$<200$} & \multicolumn{1}{c}{$200-209$} & \multicolumn{1}{c}{$210-219$} & \multicolumn{1}{c}{$220-244$} & \multicolumn{1}{c}{$245-259$} & \multicolumn{1}{c}{$260-284$} & \multicolumn{1}{c}{$>284$} \\
$<117$ & 2/53 & 0/21 & 0/15 & 0/20 & 0/14 & 1/22 & 0/11 \\
$117-126$ & 0/66 & 2/27 & 1/25 & 8/69 & 0/24 & 5/22 & 1/19 \\
$127-136$ & 2/59 & 0/34 & 2/21 & 2/83 & 0/33 & 2/26 & 4/28 \\
$137-146$ & 1/65 & 0/19 & 0/26 & 6/81 & 3/23 & 2/34 & 4/23 \\
$147-156$ & 2/37 & 0/16 & 0/6 & 3/29 & 2/19 & 4/16 & 1/16 \\
$157-166$ & 1/13 & 0/10 & 0/11 & 1/15 & 0/11 & 2/13 & 4/12 \\
$167-186$ & 3/21 & 0/5 & 0/11 & 2/27 & 2/5 & 6/16 & 3/14 \\
$>186$ & 1/5 & 0/1 & 3/6 & 1/10 & 1/7 & 1/7 & 1/7 \\
\bottomrule
\end{tabular}
\vskip1em

Les données sont disponibles dans le fichier \texttt{hdis.dat} sous forme
d'un tableau comprenant 4 colonnes indiquant, respectivement, la pression
artérielle (8 catégories, notées 1 à 8), le taux de cholesterol (7
catégories, notées 1 à 7), le nombre d'infarctus et le nombre total
d'individus. On s'intéresse à l'association entre la pression artérielle et
la probabilité d'avoir un infarctus du myocarde.
\begin{description}
\item[(a)] Calculer les proportions d'infarctus pour chaque niveau de
  pression artérielle et les représenter dans un tableau et sous forme
  graphique.
\item[(b)] Exprimer les proportions calculées en (a) sous forme de
  \emph{logit}. 
\item[(c)] À partir d'un modèle de régression logistique, déterminer s'il
  existe une association significative au seuil $\alpha=0.05$ entre la
  pression artérielle, traitée en tant que variable quantitative en
  considérant les centres de classe, et la probabilité d'avoir un
  infarctus.
\item[(d)] Exprimer en unités \emph{logit} les probabilités d'infarctus
  prédites par le modèle pour chacun des niveaux de pression artérielle.
\item[(e)] Afficher sur un même graphique les proportions empiriques et la
  courbe de régression logistique en fonction des valeurs de pression
  artérielle (centres de classe).
\end{description}
% FIXME:
% Ajouter question sur prédiction pour une certaine valeur
% (niveau observé, et niveau non observé)
\begin{sol}
L'importation des données \texttt{hdis.dat} se fait comme suit :
<<ex6-4a>>=
bp <- read.table("hdis.dat", header=TRUE)
str(bp)
@ 
\cmd{read.table}\cmd{str}
Comme on peut le constater, aucun label n'est associé aux modalités de
variables d'intérêt (\texttt{bpress} pour la pression artérielle,
\texttt{chol} pour le taux de cholesterol). Pour générer et associer les
labels, on peut utiliser les commandes suivantes :
<<ex6-4b>>=
blab <- c("<117","117-126","127-136","137-146",
          "147-156","157-166","167-186",">186")
clab <- c("<200","200-209","210-219","220-244",
          "245-259","260-284",">284")
bp$bpress <- factor(bp$bpress, labels=blab)
bp$chol <- factor(bp$chol, labels=clab)
@ 
\cmd{factor}
La dernière commande convertit les variables d'origine en variables
qualitatives et associe à leurs modalités les labels définis par
\texttt{blab} et \texttt{clab}. Pour vérifier que la base de données est
bien sous la forme désirée, les commandes suivantes sont toujours utiles :
<<ex6-4c>>=
str(bp)
summary(bp)
@ 
\cmd{str}\cmd{summary}

On peut maintenant reproduire le tableau de fréquences relatives fourni dans
l'énoncé :
<<ex6-4d>>=
round(xtabs(hdis/total ~ bpress + chol, data=bp), 2)
@
\cmd{round}\cmd{xtabs}

Puisque l'on ne va s'intéresser qu'à la relation entre pression artérielle
et infarctus, il est nécessaire d'aggréger les données sur le taux de
cholestérol. En d'autres termes, il est nécessaire de cumuler les effectifs
pour chaque niveau de pression artérielle, tous niveaux de cholesterol
confondus. On en profitera également pour renommer les niveaux de la
variable \texttt{bpress} en utilisant les centres des intervalles de classe.
<<ex6-4e>>=
blab2 <- c(111.5,121.5,131.5,141.5,151.5,161.5,176.5,191.5)
# levels(bp$bpress) <- blab2
# bp$bpress <- as.numeric(as.character(bp$bpress))
bp$bpress <- rep(blab2, each=7)
dfrm <- aggregate(bp[,c("hdis","total")], list(bpress=bp[,"bpress"]), sum)
@ %$
\cmd{levels}\cmd{aggregate}\cmd{sum}
C'est la dernière commande, \texttt{aggregate}, qui permet d'aggréger les
données : on additionne tous les effectifs (\texttt{sum}) de la variable
\texttt{chol} qui ne figure pas dans la liste des variables que l'on
souhaite conserver pour l'analyse. On en profite pour stocker les résultats
dans une nouvelle base de données nommée \texttt{dfrm}. Un aperçu des
données aggrégées est fourni ci-après :
<<ex6-4f>>=
head(dfrm, 5)
@ 
\cmd{head}

Lorsque l'on dispose d'une proportion, $p$, sa valeur sur une échelle dont
les unités sont des \emph{logit} est donnée par la relation
$\log(p/(1-p))$. D'où les commandes suivantes pour convertir les
proportions, calculées comme \verb|hdis/total| en unités logit :
<<ex6-4g>>=
logit <- function(x) log(x/(1-x))
dfrm$prop <- dfrm$hdis/dfrm$total
dfrm$logit <- logit(dfrm$hdis/dfrm$total)
@ %$
\cmd{function}\cmd{log}
On remarquera que l'on a défini une petite fonction permettant de convertir
des valeurs \texttt{x}, qui ici sont supposées être des proportions, en leur
équivalent $\log(x/(1-x))$. On aurait pu écrire de manière équivalente :
<<ex6-4h, eval=FALSE>>=
log((dfrm$hdis/dfrm$total)/(1-dfrm$hdis/dfrm$total))
@ %$
\cmd{log}
Le résultat de ces calculs est reproduit ci-après.
<<ex6-4i>>=
dfrm
@ 

Le modèle de régression logistique s'écrit de la manière suivante :
<<ex6-4j, eval=FALSE>>=
glm(cbind(hdis, total-hdis) ~ bpress, data=dfrm, family=binomial)
@ 
\cmd{glm}
La formulation utilisée, \verb|cbind(hdis, total-hdis) ~ bpress|, tient
compte du fait que nous disposons de données groupées, et non des réponses
individuelles. La commande \texttt{glm} avec l'option \verb|family=binomial|
correspond à une régression logistique, qui, sans vouloir entrer trop dans
les détails, utilise par défaut la fonction \texttt{logit} comme lien
canonique. 
% On préférera toutefois utiliser la commande équivalente
% \texttt{lrm} du package \texttt{rms}, car celle-ci fournit en règle
% générale beaucoup plus d'informations sur la qualité du modèle, en plus des
% informations que l'on pourrait obtenir à partir de la commande
% \texttt{glm}. La formulation du modèle reste identique :
% <<ex6-4k>>=
% library(rms)
% lrm(cbind(hdis, total-hdis) ~ bpress, data=dfrm)
% @ 
On obtient donc les résultats suivants :
<<ex6-4k>>=
summary(glm(cbind(hdis, total-hdis) ~ bpress, data=dfrm, family=binomial))
@ 
\cmd{glm}\cmd{summary.glm}
Le résultat précédent comprend les informations essentielles pour répondre à
la question de la significativité statistique de l'association entre
pression artérielle et probabilité d'un infarctus : le coefficient de
régression (sur l'échelle log-odds) vaut 0.024 et est significatif au seuil
usuel de 5~\% (cf. colonne \verb+Pr(>|z|)+).

La probabilité d'avoir un infarctus selon les différents niveaux de pression
artérielle considérés est obtenue comme suit :
<<ex6-4l>>=
glm.res <- glm(cbind(hdis, total-hdis) ~ bpress, data=dfrm, family=binomial)
predict(glm.res)
@
\cmd{glm}\cmd{predict}
Notons que l'on a stocké les résultats intermédiaires générés par \R sous le
nom \texttt{glm.res} avant d'utiliser la commande \texttt{predict}. Les
prédictions générées par \R sont exprimées sous forme de \emph{logit}, et
l'on peut comparer les logit observés et prédits.
<<ex6-4m>>=
cbind(dfrm, logit.predit=predict(glm.res))
@ 
\cmd{cbind}\cmd{predict}

Pour représenter graphiquement les proportions d'infarctus observées et
prédites en fonction du niveau de pression artérielle, on a pratiquement
tous les éléments. Il nous manque les prédictions du modèle exprimées sous
forme de proportions, et non sur l'échelle log-odds. Ensuite, on peut
vouloir tracer la courbe de prédiction, c'est-à-dire la probabilité d'avoir
un infarctus en fonction de la pression artérielle, sans limiter cette
dernière aux 8 valeurs observées pour la variable \texttt{bpress}. Voici une
solution possible :
<<ex6-4n, fig=TRUE>>=
dfrm$prop.predit <- predict(glm.res, type="response") 
f <- function(x) 1/(1+exp(-(coef(glm.res)[1]+coef(glm.res)[2]*x)))
xyplot(hdis/total ~ bpress, data=dfrm, aspect=1.2, cex=.8,
       xlab="Pression artérielle", ylab="Probabilité infarctus",
       panel=function(x, y, ...) {
         panel.xyplot(x, y, col="gray30", pch=19, ...)
         panel.curve(f, lty=3, col="gray70")
         panel.points(x, dfrm$prop.predit, col="gray70", ...)
       })
@ 
\cmd{predict}\cmd{function}\cmd{exp}\cmd{coef}\cmd{xyplot}
\end{sol}
\end{exo}
%
% Dupont 2009 p. 138
%
\begin{exo}\label{exo:6.5}
Une enquête cas-témoin a porté sur la relation entre la consommation
d'alcool et de tabac et le cancer de l'oesophage chez l'homme (étude "Ille
et Villaine"). Le groupe des cas était composé de 200 patients atteints d'un
cancer de l'oesophage et diagnostiqué entre janvier 1972 et avril 1974. Au
total, 775 témoins de sexe masculin ont été sélectionnés à partir des listes
électorales. Le tableau suivant indique la répartition de l'ensemble des
sujets selon leur consommation journalière d'alcool, en considérant qu'une
consommation supérieure à 80 g est considérée comme un facteur de
risque.\autocite{breslow80} 
\vskip1em

\begin{tabular}{lccc}
\toprule
& \multicolumn{2}{c}{Consommation d'alcool (g/jour)} & \\
\cmidrule(r){2-3}
& $\ge 80$ & $<80$ & Total \\
\midrule
Cas & 96 & 104 & 200 \\
Témoins & 109 & 666 & 775 \\
Total & 205 & 770 & 975 \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Quelle est la proportion de personnes considérées à risque dans
  cet échantillon ?
\item[(b)] Quelle est la valeur de l'odds-ratio et son intervalle de
  confiance à 95~\% (méthode de Woolf) ? Est-ce une bonne estimation du
  risque relatif ? 
\item[(c)] Est-ce que la proportion de consommateurs à risque est la même
  chez les cas et chez les témoins (considérer $\alpha=0.05$) ?
\item[(d)] Construire le modèle de régression logistique permettant de
  tester l'association entre la consommation d'alcool et le statut des
  sujets. Le coefficient de régression est-il significatif ?
\item[(e)] Retrouvez la valeur de l'odds-ratio observé, calculé en (b), et
  son intervalle de confiance à partir des résultats de l'analyse de
  régression.
\end{description}
\begin{sol}
Comme les données brutes (individuelles) ne sont pas disponibles, il faut
travailler directement avec le tableau d'effectifs fourni dans l'énoncé. 
<<ex6-5a>>=
alcool <- matrix(c(666,104,109,96), nr=2, dimnames=list(c("Témoin","Cas"), c("<80",">=80")))
alcool
@
\cmd{matrix}

Concernant l'odds-ratio, on utilisera la commande \texttt{oddsratio} du
package \texttt{vcd} :
<<ex6-5c>>=
library(vcd)
oddsratio(alcool, log=FALSE)
@ 
\cmd{oddsratio}
% FIXME:
% Introduire epicalc?
% cc(NULL, NULL, cctable=make2x2(96,109,104,666))
L'option \texttt{log=FALSE} nous assure que le résultat renvoyé correspond
bien à un odds-ratio et non au log odds-ratio. Pour obtenir un intervalle de
confiance asymptotique, on utilise 
<<ex6-5d>>=
confint(oddsratio(alcool, log=FALSE))
@ 
\cmd{confint}\cmd{oddsratio}
De même, on pourrait utiliser \verb|summary(oddsratio(alcool))| pour
effectuer un test d'hypothèse sur le log odds-ratio
($H_0:\,\log(\hat\theta)=0$). 

Pour tester l'hypothèse que la proportion de personnes avec une consommation
journalière est $\ge 80$ g est identique chez les cas et les témoins, on
peut utiliser la commande \texttt{prop.test} en indiquant les effectifs
observés à partir du tableau croisé donnée dans l'énoncé.
<<ex6-5f>>=
prop.test(c(96,109), c(200,775), correct=FALSE)
@ 
\cmd{prop.test}
Ce test est exactement équivalent à un test $Z$ pour tester la différence
entre deux proportions estimées à partir des données (si l'on n'utilise pas
de correction de continuité).

Pour le modèle de régression logistique, on a besoin de transformer le
tableau de contingence en un tableau de données où l'on fait clairement
apparaître les deux variables qualitatives (maladie et exposition),
c'est-à-dire un \texttt{data.frame}. Cela peut être réalisé en utilisant une
commande qui permet de transformer les données sous forme tabulaire en
format "long" : la commande \texttt{melt} du package \texttt{reshape}. On en
profitera pour recoder les niveaux de la variable maladie
en 0 (témoins) et 1 (cas), même si ce n'est pas vraiment nécessaire, et
considérer le niveau 0 comme la catégorie de référence (ce qui facilite
l'interprétation des résultats).
<<ex6-5g>>=
library(reshape)
alcool.df <- melt(alcool)
names(alcool.df) <- c("maladie", "exposition", "n")
levels(alcool.df$maladie) <- c(1,0)
alcool.df$maladie <- relevel(alcool.df$maladie, "0")
@ %$
\cmd{library}\cmd{melt}\cmd{names}\cmd{levels}\cmd{relevel}

Le modèle de régression logistique
<<ex6-5h>>=
glm.res <- glm(maladie ~ exposition, data=alcool.df, family=binomial, weights=n)
summary(glm.res)
@
\cmd{glm}\cmd{summary.glm}
Le résultat qui nous intéresse est la ligne associée à
\verb|exposition>=80|, puisqu'elle nous renseigne sur la valeur du
coefficient de régression associé à l'exposition et estimé par \R, avec son
erreur standard, ainsi que la valeur de la statistique de test. Ici, le
coefficient de régression s'interprète comme le log de l'odds-ratio.  Notons
que l'on obtiendrait exactement les mêmes résultats en intervertissant le
rôle des variables dans la formulation précédente, \verb|exposition ~ maladie|.

On peut retrouver l'odds-ratio calculé plus haut à partir du coefficient de
régression associé au facteur d'intérêt (\texttt{exposition}), ainsi que son
intervalle de confiance à 95~\% :
<<ex6-5i>>=
exp(coef(glm.res)[2])
exp(confint(glm.res)[2,])
@ 
\cmd{exp}\cmd{coef}\cmd{confint}

Une deuxième solution consiste à considérer le nombre de cas et le nombre
total d'individus, comme dans l'exercice~\ref{exo:6.4}.
<<ex6-5j>>=
alcool2 <- data.frame(expos=c("<80",">=80"), cas=c(104,96), total=c(104+666, 96+109))
summary(glm(cbind(cas, total-cas) ~ expos, data=alcool2, family=binomial))
@ 
\cmd{data.frame}\cmd{glm}\cmd{summary.glm}

Voici enfin une troisième manière de procéder, toujours avec la même
structure de données.
<<ex6-5k, eval=FALSE>>=
summary(glm(cas/total ~ expos, data=alcool2, family=binomial, weights=total))
@
\cmd{summary.glm}\cmd{glm}
Notons que le nombre total de sujets selon l'exposition n'est pas vraiment
utile dans ce dernier cas (on aurait pu utiliser directement le nombre de
témoins). 

% FIXME:
% Autre solution avec rms::lrm
% d <- datadist(alcool.df)
% options(datadist="d")
% lrm(X1 ~ X2, data=alcool.df, weights=value)
% summary(lrm(X1 ~ X2, data=alcool.df, weights=value))
\end{sol}
\end{exo}
\Closesolutionfile{solutions}
\section*{Corrigés}
\input{solutions6}

\chapter*{Devoir \no 5}
\addcontentsline{toc}{chapter}{Devoir \no 5}

\chapter{Données de survie}\label{chap:survival}
\Opensolutionfile{solutions}[solutions7]

\begin{center} 
\fbox{\begin{minipage}{5in} 
\textbf{\large Objectifs :} 

\noindent

\end{minipage}} 
\end{center}

\vspace*{3ex}

\section*{Énoncés}

%
% 
%
\begin{exo}\label{exo:7.1}
Dans un essai contre placebo sur la cirrhose biliaire, la D-penicillamine
(DPCA) a été introduite dans le bras actif sur une cohorte de 312
patients. Au total, 154 patients ont été randomisés dans le bras actif
(variable traitement, \texttt{rx}, 1=Placebo, 2=DPCA). Un ensemble de
données telles que l'âge, des données biologiques et signes cliniques variés
incluant le niveau de bilirubine sérique (\texttt{bilirub}) sont disponibles
dans le fichier \texttt{pbc.txt}.\autocite{vittinghoff05} Le status du
patient est enregistré dans la variable \texttt{status} (0=vivant, 1=décédé)
et la durée de suivi (\texttt{years}) représente le temps écoulé en années
depuis la date de diagnostic.
\begin{description}
\item[(a)] Combien dénombre-t-on d'individus décédés? Quelle proportion de
  ces décès retrouve-t-on dans le bras actif ?  
\item[(b)] Afficher la distribution des durées de suivi des 312 patients, en
  faisant apparaître distinctement les individus décédés. Calculer le temps
  médian (en années) de suivi pour chacun des deux groupes de
  traitement. Combien y'a-t-il d'événements positifs au-delà de 10.5 années
  et quel est le sexe de ces patients ?
\item[(c)] Les 19 patients dont le numéro (\texttt{number}) figure parmi la
  liste suivante ont subi une transplantation durant la période de suivi.
\begin{verbatim}  
5 105 111 120 125 158 183 241 246 247 254 263 264 265 274 288 291
295 297 345 361 362 375 380 383
\end{verbatim}   
  Indiquer leur âge moyen, la distribution selon le sexe et la durée médiane
  de suivi en jours jusqu'à la transplantation.
\item[(d)] Afficher un tableau résumant la distribution des événements à
  risque en fonction du temps, avec la valeur de survie associée.
\item[(e)] Afficher la courbe de Kaplan-Meier avec un intervalle de
  confiance à 95~\%, sans considérer le type de traitement.
\end{description}
\begin{sol}
Le chargement des données ne pose pas de difficultés :
<<ex7-1a>>=
pb <- read.table("pbc.txt", header=TRUE)
names(pb)[1:20]
@ 
Pour faciliter la lecture des résultats, recodons d'emblée les variables
traitement (\texttt{rx}) et sexe (\texttt{sex}) comme variables qualitatives
: 
<<ex7-1b>>=
pb$rx <- factor(pb$rx, labels=c("Placebo", "DPCA"))
table(pb$rx)
pb$sex <- factor(pb$sex, labels=c("M","F"))
@ 
Le nombre de patients décédés peut être obtenu directement à partir d'un
tableau d'effectif sur la variable \texttt{status} :
<<ex7-1c>>=
prop.table(table(pb$status))
@ 
La proportion de décès par groupe de traitement s'obtient en croisant les
deux variables \texttt{status} et \texttt{rx} et en calculant les fréquences
relatives par ligne :
<<ex7-1c>>=
prop.table(with(pb, table(status, rx)), 1)
@
Pour afficher la distribution des temps de suivi, on peut utiliser un simple
diagramme de dispersion dans lequel les coordonnées des points à afficher
sont définis par le numéro d'observation et la durée de suivi depuis le
diagnostique. 
<<ex7-1d, fig=TRUE>>=
xyplot(number ~ years, data=pb, pch=pb$status, cex=.8)
@ 

Le temps médian de suivi par groupe de traitement est obtenu à partir d'une
commande de type \texttt{tapply} en considérant la variable \texttt{rx}
comme variable de classification, soit :
<<ex7-1e>>=
with(pb, tapply(years, rx, median))
@ 
Le nombre de patients décédés au-delà de 10.5 ans est obtenu comme suit :
<<ex7-1f>>=
with(pb, table(status[years > 10.5]))
with(pb, table(sex[years > 10.5 & status == 1]))
@ 
soit 4 patients, dont 2 patients de sexe féminin. Notons que la dernière
commande pourrait également s'écrire 
<<ex7-1g>>=
subset(pb, years > 10.5 & status == 1, sex)
@ 
ce qui a l'avantage de fournir les numéros d'observation.

Les patients transplantés figurent normalement parmi les individus vivants à
la date de point. On peut le vérifier aisément à l'aide d'un simple tri à
plat du \texttt{status} de ces patients.
<<ex7-1h>>=
idx <- c(5,105,111,120,125,158,183,241,246,247,254,263,264,265,274,288,291,295,
         297,345,361,362,375,380,383)
table(pb$status[pb$number %in% idx])
@ 
Pour calculer les quantités demandées (âge moyen, sexe, durée de suivi), on
peut dans un premier temps réduire le tableau de données initial à ces seuls
patients :
<<ex7-1i>>=
pb.transp <- subset(pb, number %in% idx, c(age, sex, years))
@ 
Ensuite, il s'agit simplement d'appliquer les commandes \texttt{mean},
\texttt{table} et \texttt{median} aux variables sélectionnées pour ces 19
patients.
<<ex7-1j>>=
mean(pb.transp$age)
table(pb.transp$sex)
median(pb.transp$years * 365)
@ 

Pour obtenir un tableau des événements avec la survie associée, il est
nécessaire d'utiliser la commande \texttt{Surv} du package
\texttt{survival}. La commande de base pour recoder des données
événements/temps en données de survie est \texttt{Surv} dont voici un
exemple d'utilisation :
<<ex7-1k>>=
library(survival)
head(with(pb, Surv(time=years, event=status)))
@ 
Les données censurées (patients vivant) apparaissent suffixées avec un
\texttt{+}. Un tableau des événements en fonction du temps peut être
construit à l'aide de la commande \texttt{survfit}. On notera que lorsque
l'on ne considère aucun facteur de groupe ou de stratification, il faut
utiliser la syntaxe un peu particulière \verb|Surv(time, event) ~ 1|.
<<ex7-1l, results=hide>>=
s <- survfit(Surv(years, status) ~ 1, data=pb)
summary(s)
@ 

Une fois que le tableau de survie a été construit, il est très simple
d'afficher l'estimateur de la survie sous la forme d'une courbe de
Kaplan-Meier à l'aide de la commande \texttt{plot}.
<<ex7-1m, fig=TRUE>>=
plot(s)
@ 
\end{sol}
\end{exo}
%
% Everitt p. 348
%
\begin{exo}\label{exo:7.2}
Dans un essai randomisé, on a cherché à comparer deux traitements pour le
cancer de la prostate. Les patients prenaient chaque jour par voie orale
soit 1 mg de diethylstilbestrol (DES, bras actif) soit un placebo, et le
temps de survie est mesuré en mois.\autocite{collett94} La question
d'intérêt est de savoir si la survie diffère entre les deux groupes de
patients, et on négligera les autres variables présentes dans le fichier de
données \texttt{prostate.dat}. 
\begin{description}
\item[(a)] Calculer la médiane de survie pour l'ensemble des patients, et
  par groupe de traitement.
\item[(b)] Quelle est la différence entre les proportions de survie dans les
  deux groupes à 50 mois ?
\item[(c)] Afficher les courbes de survie pour les deux groupes de patients.
\item[(d)] Effectuer un test du log-rank pour tester l'hypothèse selon
  laquelle le traitement par DES a un effet positif sur la survie des
  patients. 
\end{description}
\begin{sol}
Pour importer les données, on utilisera la commande \texttt{read.table}. On
profitera de l'inspection préalable des données pour recoder la variable
\texttt{Treatment} en variable qualitative.
<<ex7-2a>>=
prostate <- read.table("prostate.dat", header=TRUE)
str(prostate)
head(prostate)
prostate$Treatment <- factor(prostate$Treatment)
table(prostate$Status)
@ 
Il n'est pas vraiment nécessaire de recoder \texttt{Status} en variable
qualitative puisque l'on va utiliser cette variable conjointement avec
\texttt{Time} pour l'analyse de survie. Dans un premier temps, pour
transformer les données en données de survie, incluant la censure, il est
nécessaire de passer par la commande \texttt{Surv} du package \texttt{survival}.
<<ex7-2b>>=
library(survival)
with(prostate, Surv(time=Time, event=Status))
@ 

La médiane de survie s'obtient à l'aide de \texttt{survfit}, qui une
commande générale permettant de manipuler les données de survie par la
méthode de Kaplan-Meier ou le modèle de Cox. Si l'on ne tient pas compte des
groupes de traitement (et plus généralement d'un quelconque co-facteur), on
l'utilisera ainsi : 
<<ex7-2c>>=
survfit(Surv(Time, Status) ~ 1, data=prostate)
@
Pour stratifier l'analyse selon le traitement, on utilisera en revanche :
<<ex7-2d>>=
survfit(Surv(Time, Status) ~ Treatment, data=prostate)
@ 

Pour afficher les courbes de survie correspondant aux deux groupes de
traitement, on procèdera comme suit :
<<ex7-2e, fig=TRUE>>=
plot(survfit(Surv(Time, Status) ~ Treatment, data=prostate))
@ 

Le test du log-rank peut être obtenu de deux manières différentes. La plus
simple consiste à utiliser la commande \texttt{survdiff} pour comparer deux
courbes de survie à partir de l'estimateur de Kaplan-Meier.
<<ex7-2f>>=
survdiff(Surv(time=Time, event=Status) ~ Treatment, data=prostate)
@ 
L'autre solution consiste à utiliser une régression de Cox
<<ex7-2f>>=
summary(coxph(Surv(time=Time, event=Status) ~ Treatment, data=prostate))
@ 
\end{sol}
\end{exo}
%
% time 1/2 (interval data)
%

% 
% Stata hip fracture
%
\begin{exo}\label{exo:7.3}
% FIXME: reformuler énoncé
Une étude a visé à évaluer le bénéfice d'un nouvel appareil de protection
pneumatique pour prévenir les fractures de hanche occasionnées par la chute
des personnes âgés. Ce dispositif doit être porté en permanence par la
personne, mais on espère qu'à terme cela permettra de réduire l'incidence
des fractures de hanche. Au total, 48 femmes âgées de plus de 60 ans ont
participé à cette étude, parmi lesquelles 28 ont été sélectionnées par
tirage aléatoire pour utiliser ce nouveau dispositif et 20 ont été choisies
comme contrôles (pas de dispositif). Des dosages réguliers de calcium ont
été effectués tous les 5 mois et la durée d'observation avant la première
fracture a été enregistrée en mois.\autocite{cleves10} Les données sont
disponibles au format Stata dans le fichier \texttt{hip.dta}. L'événement
est défini dans la variable appelée \texttt{fracture}, et les variables
\texttt{time0} et \texttt{time1} indiquent le début et la fin des
intervalles de monitoring des sujets.
\begin{description}
\item[(a)] Vérifier la structure du tableau de données et remédier aux
  problèmes des valeurs manquantes.
\item[(b)] Afficher un tableau des principaux indicateurs de survie (nombre
  d'individus à risque, nombre d'événements).
\item[(c)] Calculer la médiane de survie et son intervalle de confiance à
  95~\% pour chaque groupe de sujets et afficher les courbes de survie
  correspondantes.
\item[(d)] Effectuer un test du log-rank. Comparer avec un test de Wilcoxon.
\item[(e)] Effectuer un test du log-rank sur le facteur d'intérêt
  (\texttt{protect}) en stratifiant sur l'âge. On considèrera trois groupe
  d'âge : 65 ans ou moins, entre 65 et 75 ans inclus, plus de 75 ans.
\item[(f)] Construire un modèle de régression (Cox) en tenant compte du
  groupe de traitement et donner la valeur du hazard ratio.
\item[(g)] Vérifier les conditions d'applications du modèle.
\end{description}

\begin{sol}
Le chargement des données
<<ex7-3a>>=
library(foreign)
hip <- read.dta("hip.dta")
summary(hip)
subset(hip, id==34)
tail(hip[do.call(order, hip),])
@   
\end{sol}
\end{exo}

\Closesolutionfile{solutions}
\section*{Corrigés}
\input{solutions7}

\begin{footnotesize}
  \printindex
\end{footnotesize}

\part{Analyses statistiques avec Stata}

\chapter{Élements du langage et statistiques descriptives}

Dans cette partie sont proposés les corrigés des exercices 1.1, 1.2, 1.5, 2.1,
2.3 et 2.4 dont les énoncés figurent dans les chapitres 1
(p.~\pageref{chap:langage}) et 2 (p.~\pageref{chap:descriptive}).


\section*{Corrigés}
\soln{\ref{exo:1.1}} Stata dispose d'un éditeur de données qui se présente
sous la forme d'un tableur (à l'image d'Excel), mais on peut saisir
dirctement des données à l'aide de la commande \texttt{input}. Après avoir
donné le nom de la ou des variables (lorsqu'il y a plusieurs variables, on
sépare leur nom par un espace), on presse sur la touche \textsf{Entrée}
et on saisit les observations (idem, lorsqu'il y a plusieurs variables on
saisit les observations ou valeurs observées en les séparant par un
espace). Lorsque la saisie est terminé, on saisit le mot \texttt{end} et on
presse sur \textsf{Entrée} pour indiquer à Stata que la saisie est
terminée. \label{para:edit}
\begin{verbatim}
. input x

             x
  1. 3.68
  2. 2.21
  3. 2.45
  4. 8.64
  5. 4.32
  6. 3.43
  7. 5.11
  8. 3.87
  9. end
\end{verbatim}
On peut vérifier la saisie en affichant les valeurs 3 ou 4 premières valeurs
de \texttt{x} :
\begin{verbatim}
. list in 1/4

     +------+
     |    x |
     |------|
  1. | 3.68 |
  2. | 2.21 |
  3. | 2.45 |
  4. | 8.64 |
     +------+
\end{verbatim}
Les commandes \texttt{describe} ou \texttt{summarize} fournissent toutes les
deux le nombre d'observations contenues dans la variable \texttt{x} (comme
il n'y a qu'une seule variable, il n'est même pas nécessaire de spécifier le
nom de la variable lorsque l'on utilise ces commandes).
\begin{verbatim}
. summarize

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
           x |         8     4.21375    2.019526       2.21       8.64
\end{verbatim}
Pour corriger la valeur erronée (8.64), on utilisera \texttt{replace} en
sépcifiant la position de l'observation dans la variable (son rang
numérique) :
\begin{verbatim}
. replace x = 3.64 in 4
(1 real change made)
\end{verbatim}
On procèdera de même pour recoder la 7\ieme\ observation en valeur
manquante, sachant que les valeurs manquantes simples sont codées à l'aide
d'un point :
\begin{verbatim}
. replace x = . in 7   
(1 real change made, 1 to missing)
\end{verbatim}
D'où le résumé numérique final suivant :
\begin{verbatim}
. summarize

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
           x |         7    3.371429    .7656246       2.21       4.32
\end{verbatim}
% 
%
%
\soln{\ref{exo:1.2}} On peut utiliser la même solution qu'à l'exercice
précédent pour saisir les données, c'est-à-dire utiliser la commande
\texttt{input} ou bien se servir de léditeur de données qui se présente sous
la forme d'un tableur :

\includegraphics{./figs/stata_editX}

On en profitera pour nommer la variable (\texttt{var1} par défaut pour la
première colonne du tableur). Le seuil de détection en logarithme vaut :
\begin{verbatim}
. display log10(50)
1.69897
\end{verbatim}
On peut donc compter le nombre d'observation ne remplissant pas la condition
$X>\log(50)$ à l'aide de la commande \texttt{count}.
\begin{verbatim}
. count if X <= log10(50)
    4
\end{verbatim}
D'où le calcul de la charge virale médiane en ne considérant que les données
au-dessus de la limite de détection :
\begin{verbatim}
. egen Xm = median(10^X) if X > log10(50)
. display round(Xm)
12980
\end{verbatim}
Pour calculer la médiane des $X_i$ remplissant la condition $X_i>\log(50)$,
on a utilisé directement la commande \texttt{egen} qui fournit un certain
nombre de fonctions de transformations et de calcul de base (voir l'aide en
ligne, \verb|help egen|).
%
%
%
\soln{\ref{exo:1.5}}
Pour lire les données contenues dans le fichier \texttt{anorexia.data} dont
un aperçu est fourni ci-dessous
\begin{verbatim}
Group Before After
g1 80.5  82.2
g1 84.9  85.6
g1 81.5  81.4
g1 82.6  81.9
g1 79.9  76.4
\end{verbatim}
on va utiliser la commande \texttt{infile} à partir d'un fichier \og
dictionnaire\fg\ qui décrit la structure des données. Ce fichier porte
généralement le même nom que le fichier source de données, et possède
l'extension \texttt{dct}. En voici le listing complet
(\texttt{anorexia.dct}) :
\begin{verbatim}
infile dictionary using anorexia.dat {
  _first(2)
  str2 Group "Type de therapie"
  double Before "Avant"
  double After "Apres"
}
\end{verbatim}
Dans celui-ci, on indique que les observations commencent à la 2\ieme\ ligne
(on ignore la ligne d'en-tête), et que l'on a trois variables,
\texttt{Group} (variable qualitative), \texttt{Before} et \texttt{After}
(variables numériques). Notons que l'on a associé des étiquettes de
description pour ces trois variables. Il suffit ensuite d'utiliser la
commande \texttt{infile} en fournissant le nom de ce fichier dictionnaire
(inutile de préciser l'extension du fichier).
\begin{verbatim}
. infile using anorexia

infile dictionary using anorexia.dat {
  _first(2)
  str2 Group "Type de therapie"
  double Before "Avant"
  double After "Apres"
}

(72 observations read)
\end{verbatim}
On peut ensuite vérifier que les données ont été importées correctement à
l'aide de \texttt{describe}. Cette commande fournit par ailleurs le nombre
d'observations disponibles dans le tableau de données ($N=72$).
\begin{verbatim}
. describe

Contains data
  obs:            72                          
 vars:             3                          
 size:         1,296                          
----------------------------------------------------------------------------------------
              storage  display     value
variable name   type   format      label      variable label
----------------------------------------------------------------------------------------
Group           str2   %9s                    Type de therapie
Before          double %10.0g                 Avant
After           double %10.0g                 Apres
----------------------------------------------------------------------------------------
Sorted by:  
     Note:  dataset has changed since last saved
\end{verbatim}
Pour obtenir les effectifs par type de thérapie, on utilise un simple tri à
plat à l'aide de la commande \texttt{tabulate}.
\begin{verbatim}
. tabulate Group

    Type de |
   therapie |      Freq.     Percent        Cum.
------------+-----------------------------------
         g1 |         29       40.28       40.28
         g2 |         26       36.11       76.39
         g3 |         17       23.61      100.00
------------+-----------------------------------
\end{verbatim}
La transformation d'unités pour les poids ne pose pas de problème
spécifique, mais il faut décider si l'on crée de nouvelles variables
(\texttt{generate}) ou si l'on remplace les valeurs existantes
(\texttt{replace}). Ici, on remplacera les valeurs existantes :
\begin{verbatim}
. replace Before = Before/2.2
(72 real changes made)
. replace After = After/2.2
(72 real changes made)
\end{verbatim}
Pour les scores de différences, on crée cette fois-ci une nouvelles variable
à l'aide de la commande \texttt{generate}.
\begin{verbatim}
. generate diff = After - Before
. summarize diff

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
        diff |        72    1.256313    3.628908  -5.545455   9.772727
\end{verbatim}

La commande \texttt{summarize} peut être utilisée pour fournir un résumé
numérique pour chacun des groupes de traitement, par exemple 
\verb|by Group, sort: summarize diff|. Mais, pour calculer spécifiquement
certains indicateurs descriptifs, il est plus commode d'utiliser la comamnde
\texttt{tabstat} à laquelle on fournit la variable réponse et le facteur de
classification, ainsi que les statistiques recherchées via
\texttt{stats()}. \label{para:tabstat}
\begin{verbatim}
. tabstat diff, by(Group) stats(mean min max)

Summary for variables: diff
     by categories of: Group (Type de therapie)

 Group |      mean       min       max
-------+------------------------------
    g1 |  1.366771 -4.136364       9.5
    g2 | -.2045454 -5.545455  7.227273
    g3 |  3.302139 -2.409091  9.772727
-------+------------------------------
 Total |  1.256313 -5.545455  9.772727
--------------------------------------
\end{verbatim}
%
%
%
\soln{\ref{exo:2.1}} Pour la saisie des données, on procéder de la même
manière qu'à l'exercice~1.1, p.~\pageref{para:edit}, ou tout simplement
saisir les données dans un fichier au format texte. Supposons que les
données aient été entrées dans un fichier appelé \verb|saisie_x.txt| dont
voici un aperçu :
\begin{verbatim}
24.9 25.0 25.0 25.1 25.2 ...
\end{verbatim}
Les valeurs de $X$ sont simplement séparées par un espace. Dans ce cas, les
données peuvent être lues et importées dans Stata avec la commande \texttt{infile}.
\begin{verbatim}
. infile x using "saisie_x.txt"             
(26 observations read)
\end{verbatim}
On n'oubliera pas d'ajouter l'option \texttt{clear} si des données existe
déjà dans l'espace de travail de Stata.

La commande \texttt{tabstat} peut être utilisée pour calculer la moyenne et
la médiane des observations. On utilisera par contre la commande
\texttt{egen} avec la fonction \texttt{mode} pour calculer les valeurs
modales de $X$. Comme il y a plusieurs modes, on demandera à Stata de
renvoyer la valeur la plus basse des deux modes. Notons que la commande
\texttt{egen} pourrait également être utilisée pour calculer la moyenne et
la médiane de $X$. 
\begin{verbatim}
. tabstat x, stats(mean median)

    variable |      mean       p50
-------------+--------------------
           x |  25.44615     25.45
----------------------------------
. egen xmode = mode(x), minmode
. display xmode
25.4
\end{verbatim}
Concernant la variance, on a directement accès à l'écart-type à partir de 
\verb|summarize x|, mais on peut également la calculer avec \texttt{egen}
comme le carré de l'écart-type estimé à partir de l'échantillon, puis
l'afficher sur la console des résultats :
\begin{verbatim}
. egen varx = sd(x)
. di varx^2
.0793846
\end{verbatim}
Pour le recodage en 4 classes d'intervalles pré-définis pour la variable
\texttt{x}, on peut toujours utiliser la fonction \texttt{cut} avec
\texttt{egen}. On peut ensuite vérifier que les intervalles de classe sont
bien respectés à l'aide de \texttt{table} qui est plus souple que la
commande \texttt{summarize} et permet de préciser la liste des valeurs
statistiques à afficher.
\begin{verbatim}
. egen xc = cut(x), at(24.8,25.2,25.5,25.8,26.1) label
. table xc, contents(min x max x)

----------------------------------
       xc |     min(x)      max(x)
----------+-----------------------
    24.8- |       24.9        25.1
    25.2- |       25.2        25.4
    25.5- |       25.5        25.8
    25.8- |       25.9          26
----------------------------------
\end{verbatim}
En revanche, le tableau d'effectifs peut s'obtenir directement avec
\texttt{tabulate}, par exemple :
\begin{verbatim}
. tabulate xc, plot

         xc |      Freq.
------------+------------+-----------------------------------------------------
      24.8- |          4 |****
      25.2- |          9 |*********
      25.5- |         11 |***********
      25.8- |          2 |**
------------+------------+-----------------------------------------------------
      Total |         26
\end{verbatim}
Enfin, pour la représentation sous forme d'histogramme, Stata dispose de ses
propres algorithmes de calcul pour la détermination du nombre de classes à
construire, tout comme R. Tout est géré à partir de la commande
\texttt{histogram}. 
\begin{verbatim}
. histogram x, frequency
(bin=5, start=24.9, width=.22000008)
\end{verbatim}
Ne pas oublier l'option \texttt{frequency} si l'on souhaite afficher les
effectifs plutôt que la densité (qui est le choix par défaut).

\includegraphics{./figs/stata_histx}
%
%
%
\soln{\ref{exo:2.3}} Pour importer les données stockées dans un simple
fichier texte, on utilise la commande \verb|- infile -|.
\begin{verbatim}
. infile tailles using "elderly.dat", clear
\end{verbatim}
Ici, on notera que l'on ne précise pas la manière dont sont codées les
valeurs manquantes car le "." est le format utilisé par défaut par Stata
(pour les variables numériques uniquement).

Il existe plusieurs commandes qui facilitent la détection et
l'identification des valeurs manquantes. Parmi celles disponibles par défaut
sur Stata, on distingue \verb|- codebook -| qui fournit un résumé de la
variable, ainsi que les fonctions de base qui permettent de tabuler ou
compter les observations répondant à un certain critère.
\begin{verbatim}
. count if tailles == .
    5
\end{verbatim}

Pour obtenir la taille moyenne et son intervalle de confiance à 95~\%
associé, il suffit d'entrer la commande suivante :
\begin{verbatim}
. ci tailles

    Variable |        Obs        Mean    Std. Err.       [95% Conf. Interval]
-------------+---------------------------------------------------------------
     tailles |        346    159.8324    .3232401        159.1966    160.4681
\end{verbatim}

Enfin, pour afficher la distribution des tailles sous forme d'une courbe de
densité, on utilise la commande \texttt{histogram} avec l'option
\texttt{kdensity}. Le degré de lissage peut être contrôlé à l'aide de
l'option \texttt{kdenopts} ; par exemple, ajouter \verb|kdenopts(gauss width(1))| 
à la commande ci-dessous produirait une courbe \og moins lisse\fg\
(c'est-à-dire s'ajustant plus aux données).
\begin{verbatim}
. histogram tailles, kdensity
(bin=18, start=142, width=2)
\end{verbatim}

\includegraphics{./figs/stata_kdens}

%
%
%
\soln{\ref{exo:2.4}} Les données sur les poids à la naissance de Hosmer \&
Lemeshow (1989) peuvent être obtenues directement au format Stata grâce à la
commande \verb|webuse lbw|, mais par souci de simplicité on utilisera les
mêmes données que celles traitées avec \R. Celles-ci ont été exportées au
format texte dans le fichier \texttt{birthwt.dat}, et elles peuvent être
importées ainsi : 
\begin{verbatim}
. infile low age lwt race smoke ptl ht ui ftv bwt using "birthwt.dat", clear
\end{verbatim}

On peut vérifier avec la commande \verb|- describe -| que les variables sont
bien toutes au format numérique (colonne \texttt{storage type}).
\begin{verbatim}
. describe

Contains data
  obs:           189                          
 vars:            10                          
 size:         7,560                          
-----------------------------------------------------------------------------------------
              storage  display     value
variable name   type   format      label      variable label
-----------------------------------------------------------------------------------------
low             float  %9.0g                  
age             float  %9.0g                  
lwt             float  %9.0g                  
race            float  %9.0g                  
smoke           float  %9.0g                  
ptl             float  %9.0g                  
ht              float  %9.0g                  
ui              float  %9.0g                  
ftv             float  %9.0g                  
bwt             float  %9.0g                  
-----------------------------------------------------------------------------------------
Sorted by:  
     Note:  dataset has changed since last saved
\end{verbatim}
Pour créer de nouvelles étiquettes aux variables \texttt{low},
\texttt{race}, \texttt{smoke}, \texttt{ui} et \texttt{ht}, il suffit de
définir des "labels" et de les associer aux variables en question (cela ne
change le format de représentation des variables en mémoire, qui restent
stockées sous forme de nombre).
\begin{verbatim}
. label define yesno 0 "no" 1 "yes"
. label define ethn 1 "White" 2 "Black" 3 "Other"
. label values low smoke ui ht yesno
. label values race ethn
\end{verbatim}
On peut vérifier que le tableau de données a bien été mis à jour
\begin{verbatim}
. list in 1/5

     +---------------------------------------------------------------+
     | low   age   lwt    race   smoke   ptl   ht    ui   ftv    bwt |
     |---------------------------------------------------------------|
  1. |  no    19   182   Black      no     0   no   yes     0   2523 |
  2. |  no    33   155   Other      no     0   no    no     3   2551 |
  3. |  no    20   105   White     yes     0   no    no     1   2557 |
  4. |  no    21   108   White     yes     0   no   yes     2   2594 |
  5. |  no    18   107   White     yes     0   no   yes     0   2600 |
     +---------------------------------------------------------------+
\end{verbatim}

La conversion du poids des mères en \emph{kg} ne pose pas de problème
particulier, et ici on remplacera directement les données disponibles à
l'aide de la commande \texttt{replace}
\begin{verbatim}
. replace lwt = lwt/2.2
(189 real changes made)
\end{verbatim}
Les indicateurs de tendance centrale et de dispersion relative sont obtenus
à partir de la commande \verb|summarize|.
\begin{verbatim}
. summarize lwt, detail

                             lwt
-------------------------------------------------------------
      Percentiles      Smallest
 1%     38.63636       36.36364
 5%     42.72727       38.63636
10%     44.54546       38.63636       Obs                 189
25%           50       40.45454       Sum of Wgt.         189

50%           55                      Mean           59.00673
                        Largest       Std. Dev.      13.89972
75%     63.63636       104.0909
90%     77.27273       106.8182       Variance       193.2022
95%     85.90909       109.5455       Skewness       1.390855
99%     109.5455       113.6364       Kurtosis       5.309181
\end{verbatim}
On a ajouté l'option \texttt{detail} car la commande \verb|summarize lwt| ne
fournit pas la médiane. On a cependant vu une alternative possible avec la
commande \texttt{tabstat} dans l'exercice~1.5
(p.~\pageref{para:tabstat}). Pour obtenir l'intervalle inter-quartile, on
peut le calculer ainsi :
\begin{verbatim}
. egen lwtiqr = iqr(lwt)
. di lwtiqr
13.636364
\end{verbatim}
Enfin, un histogramme du poids des mères est construit à l'aide de la
commande \verb|histogram|. Ici, sans autre option un histogramme de
densité sera construit.
\begin{verbatim}
. histogram lwt
(bin=13, start=36.363636, width=5.9440557)
\end{verbatim}

\includegraphics{./figs/stata_birthwt}

Concernant la proportion de mères ayant fumé durant la grossesse et
le calcul de l'intervalle de confiance à 95~\% associé, on peut utiliser la
commande \verb|- prtest -|, qui comme dans le cas de la commande \R
\texttt{prop.test} suppose de grands échantillons :
\begin{verbatim}
. tabulate smoke

      smoke |      Freq.     Percent        Cum.
------------+-----------------------------------
         no |        115       60.85       60.85
        yes |         74       39.15      100.00
------------+-----------------------------------
      Total |        189      100.00
. prtest smoke == .5

One-sample test of proportion                  smoke: Number of obs =      189
------------------------------------------------------------------------------
    Variable |       Mean   Std. Err.                     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       smoke |   .3915344   .0355036                      .3219487    .4611201
------------------------------------------------------------------------------
    p = proportion(smoke)                                         z =  -2.9823
Ho: p = 0.5

     Ha: p < 0.5                 Ha: p != 0.5                   Ha: p > 0.5
 Pr(Z < z) = 0.0014         Pr(|Z| > |z|) = 0.0029          Pr(Z > z) = 0.9986
\end{verbatim}
Le test utilisé par Stata diffère du $\chi^2$ de \R mais ces deux tests
fournissent essentiellement les mêmes résultats. Ici, la proportion estimée
vaut 0.392 avec un intervalle de confiance de $[0.322;0.461]$. Si l'on ne
souhaite pas utiliser l'approximation pour grands échantillons, on peut
utiliser \verb|- bintest -| à la place.

La commande \verb|- graph bar -| permet de construire assez facilement des
diagrammes en barres. 
\begin{verbatim}
. gen one = 1
. graph bar (sum) one, over(smoke) ytitle("Effectif")
\end{verbatim}

\includegraphics{./figs/stata_birthwt2}

Pour générer les terciles, on utilise la commande \verb|- xtile -| en
spécifiant le nombre de quantile désiré, \texttt{nq(3)}. 
\begin{verbatim}
. xtile ageter = age, nq(3)
. tabulate ageter

3 quantiles |
     of age |      Freq.     Percent        Cum.
------------+-----------------------------------
          1 |         69       36.51       36.51
          2 |         66       34.92       71.43
          3 |         54       28.57      100.00
------------+-----------------------------------
      Total |        189      100.00
\end{verbatim}
On notera que Stata n'attribue pas automatiquement des labels aux catégories
générées, mais simplement un rang (ici, ${1,2,3}$). On peut toutefois
obtenir les bornes des intervalles des terciles de deux manières. La
première solution, 
\begin{verbatim}
. _pctile age, n(3)
. return list

scalars:
                 r(r1) =  20
                 r(r2) =  25
\end{verbatim}
renvoit les bornes inférieures (exclues) des deux derniers intervalles, d'où
les trois intervalles : (14,20], (20,25] et (25,45], la borne inférieure du
1\ier intervalle étant obtenue comme le \texttt{min} de la variable
\texttt{age}. La seconde méthode consiste à fournir un résumé plus détaillé
de la variable \texttt{ageter} :
\begin{verbatim}
. table ageter, contents(freq min age min age)

----------------------------------------------
3         |
quantiles |
of age    |      Freq.    min(age)    min(age)
----------+-----------------------------------
        1 |         69          14          14
        2 |         66          21          21
        3 |         54          26          26
----------------------------------------------
\end{verbatim}

On notera qu'une commande telle que \texttt{cut} permettrait de générer des
groupes approximativement équivalents en termes d'effectifs, mais cela ne
correspond pas tout à fait au résultat souhaité.
\begin{verbatim}
. egen ageter = cut(age), group(3) label
\end{verbatim}

Le croisement de cette nouvelle variable avec la variable indicatrice de
sous-poids donne les résultats suivants, exprimées en termes de proportions
(par colonne) :
\begin{verbatim}
. tabulate ageter low, col nofreq

         3 |
 quantiles |          low
    of age |        no        yes |     Total
-----------+----------------------+----------
         1 |     35.38      38.98 |     36.51 
         2 |     33.08      38.98 |     34.92 
         3 |     31.54      22.03 |     28.57 
-----------+----------------------+----------
     Total |    100.00     100.00 |    100.00
\end{verbatim}

Un tri à plat de la variable \texttt{race} est effectuée de la même manière
à partir de la comamnde \verb|- tabulate -|.
\begin{verbatim}
. tabulate race

       race |      Freq.     Percent        Cum.
------------+-----------------------------------
      White |         96       50.79       50.79
      Black |         26       13.76       64.55
      Other |         67       35.45      100.00
------------+-----------------------------------
      Total |        189      100.00
\end{verbatim}


Enfin, pour résumer la distribution des variables selon la variable
indicatrice \texttt{low}, on peut procéder en deux temps. D'abord, on résume
les variables quantitatives :
\begin{verbatim}
. by low, sort: summarize age lwt ptl ftv

-------------------------------------------------------------------------------------------------------------------
-> low = no

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         age |       130    23.66154    5.584522         14         45
         lwt |       130    60.59091    14.42001   38.63636   113.6364
         ptl |       130    .1307692    .4556019          0          3
         ftv |       130    .8384615    1.069694          0          6

-------------------------------------------------------------------------------------------------------------------
-> low = yes

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         age |        59    22.30508    4.511496         14         34
         lwt |        59    55.51618     12.0724   36.36364   90.90909
         ptl |        59    .3389831    .5448875          0          2
         ftv |        59    .6949153    1.038139          0          4
\end{verbatim}
Puis, on procède de même avec les variables qualitatives :
\begin{verbatim}
. by low, sort: tab1 race smoke ht ui
\end{verbatim}

\chapter*{Devoir \no 6}
\addcontentsline{toc}{chapter}{Devoir \no 6}

\chapter{Mesures d'association, comparaison de moyennes et de proportions
  pour deux échantillons ou plus}  

Dans cette partie sont proposés les corrigés des exercices~3.1, 3.2, 3.4,
3.5, 4.1, 4.2 et 4.3 dont les énoncés figurent dans les chapitres 3
(p.~\pageref{chap:comparaisons}) et 4 (p.~\pageref{chap:anova}).

\section*{Corrigés}
%
%
%
\soln{\ref{exo:3.1}} On rappelle que seuls les valeurs numériques des poids
à la naissance sont disponibles dans le fichier \texttt{sirds.dat}, et qu'il
nous faut contruire la variable de groupement (enfants décédés \emph{versus}
vivants). Une manière de procéder consiste à générer une variable prenant
deux valeurs puis à associer des "labels" aux valeurs distinctes prises par
cette variable. Par exemple,
\begin{verbatim}
. infile poids using "sirds.dat", clear
. gen status = 0
. replace status = 1 if _n>27
. label define labstatus 0 "décédé" 1 "vivant"
. label values status labstatus
\end{verbatim}

Il est possible d'obtenir un résumé numérique de la variable \texttt{poids}
pour chaque modalité de la variable \texttt{status} nouvellement créée à
l'aide de la commande \verb|- summarize -|.
\begin{verbatim}
. by status, sort: summarize poids

-> status = décédé

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
       poids |        27    1.691741    .5176473       1.03       2.73

-----------------------------------------------------------------------------------------
-> status = vivant

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
       poids |        23    2.307391    .6647229       1.13       3.64
\end{verbatim}

Pour réaliser un histogramme de la distribution des poids à la naissance en
fonction du status clinique, on peut procéder ainsi :
\begin{verbatim}
. histogram poids, frequency by(status)
\end{verbatim}
\includegraphics{./figs/stata_sirds}

Enfin, le test de Student est réalisé en utilisant la commande
\texttt{ttest}, qui par défaut suppose l'homogénéité des variances vérifiée :
\begin{verbatim}
. ttest poids, by(status)

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
  décédé |      27    1.691741    .0996213    .5176473    1.486966    1.896515
  vivant |      23    2.307391    .1386043    .6647229    2.019944    2.594839
---------+--------------------------------------------------------------------
combined |      50     1.97494    .0934493     .660786    1.787147    2.162733
---------+--------------------------------------------------------------------
    diff |           -.6156506    .1673084               -.9520467   -.2792545
------------------------------------------------------------------------------
    diff = mean(décédé) - mean(vivant)                            t =  -3.6797
Ho: diff = 0                                     degrees of freedom =       48

    Ha: diff < 0                 Ha: diff != 0                 Ha: diff > 0
 Pr(T < t) = 0.0003         Pr(|T| > |t|) = 0.0006          Pr(T > t) = 0.9997
\end{verbatim}
Stata reporte les degrés de significativité correspondant aux deux types de
tests, uni- et bilatéral. Ici, pour le test bilatéral, le degré de
significativité estimé est de 0.0006 ; ce résultat est arrondi, mais il est
possible d'obtenir une valeur plus précise à l'aide de la commande suivante :
\begin{verbatim}
. display r(p)
.00059019
\end{verbatim}
La commande ci-dessus doit être entrée immédiatement après l'appel à la
commande \verb|- ttest -|.
%
%
%
\soln{\ref{exo:3.2}} Les données de l'étude sur le sommeil servant de base à
l'article de Student ne sont pas disponibles sous Stata. On peut en revanche
les saisir manuellement à l'aide de la commande \verb|input|, comme on l'a
vu p.~\pageref{para:edit}. Dans le cas présent, on va donc créer deux
variables, \texttt{DHH} et \texttt{LHH}, correspondant aux temps de sommeil
enregistrées pour les molécules D. hyoscyamine hydrobromide et
L. hyoscyamine hydrobromide, respectivement. 
\begin{verbatim}
. clear all
. input DHH LHH

           DHH        LHH
  1. 0.7 1.9
  2. -1.6 0.8
  3. -0.2 1.1
  4. -1.2 0.1
  5. -0.1 -0.1
  6. 3.4 4.4
  7. 3.7 5.5
  8. 0.8 1.6
  9. 0.0 4.6
 10. 2.0 3.4
 11. end
\end{verbatim}
On peut vérifier que la saisie donne bien les résultats attendus en
affichant les 5 premières observations :
\begin{verbatim}
. list in 1/5

     +------------+
     |  DHH   LHH |
     |------------|
  1. |   .7   1.9 |
  2. | -1.6    .8 |
  3. |  -.2   1.1 |
  4. | -1.2    .1 |
  5. |  -.1   -.1 |
     +------------+
\end{verbatim}

Les moyennes par groupe de traitement sont obtenues à l'aide de
\texttt{tabstat}, qui sans autre option renvoit la moyenne des variables
listées dans la commande.
\begin{verbatim}
. tabstat DHH LHH, save

   stats |       DHH       LHH
---------+--------------------
    mean |       .75      2.33
------------------------------
\end{verbatim}
L'option \texttt{save} utilisée ci-dessus permet de stocker temporairement
les résultats renvoyés par la commande \texttt{tabstat}, ce qui permet de
les réutiliser pour calculer la différence de moyennes entre les deux
molécules. 
\begin{verbatim}
. return list

matrices:
          r(StatTotal) :  1 x 2
. matrix m = r(StatTotal)
. mat li m

m[1,2]
            DHH        LHH
mean  .75000001       2.33
. display m[1,2] - m[1,1]
1.58
\end{verbatim}
Comme on l'a vu avec R, la manipulation de la variable auxiliaire
\texttt{m}, dans laquelle on a stocké les deux moyennes de groupe, se fait
en appelant ses éléments par numéro de position (la moyenne pour le groupe
LHH se trouve en seconde position, donc peut être utilisée comme
\verb|m[1,2]|). 

On calculera les scores de différences par simple soustraction, et on
stockera les résultats dans une nouvelle variable comme indiqué ci-après.
\begin{verbatim}
. gen sdif = LHH - DHH
. tabstat sdif, stats(mean sd)

    variable |      mean        sd
-------------+--------------------
        sdif |      1.58  1.229995
----------------------------------
\end{verbatim}
Enfin, pour afficher la distribution des scores de différences sous forme
d'un histogramme en imposant des intervalles de classe de 0.5 heure, il
faudra ajouter les options \texttt{bin(8)} (8 intervalles au total) et
\texttt{start(0)} (en débutant à 0).
\begin{verbatim}
. histogram sdif, percent bin(8) start(0)
(bin=8, start=0, width=.57499999)
\end{verbatim}

\includegraphics{./figs/stata_sdif}

Pour réaliser un test $t$ pour données appariées, on utilise toujours la
commande \verb|- ttest -|, avec cette fois-ci une syntaxe légèrement
différente :
\begin{verbatim}
. ttest DHH == LHH

Paired t test
------------------------------------------------------------------------------
Variable |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
     DHH |      10         .75    .5657345     1.78901   -.5297804     2.02978
     LHH |      10        2.33    .6331666    2.002249    .8976776    3.762322
---------+--------------------------------------------------------------------
    diff |      10       -1.58    .3889587    1.229995   -2.459886   -.7001143
------------------------------------------------------------------------------
     mean(diff) = mean(DHH - LHH)                                 t =  -4.0621
 Ho: mean(diff) = 0                              degrees of freedom =        9

 Ha: mean(diff) < 0           Ha: mean(diff) != 0           Ha: mean(diff) > 0
 Pr(T < t) = 0.0014         Pr(|T| > |t|) = 0.0028          Pr(T > t) = 0.9986
\end{verbatim}

Enfin, on peut représenter les gains moyens de temps de sommeil pour
chaque molécule à l'aide d'un diagramme en barres.
\begin{verbatim}
. graph hbar DHH LHH, bargap(20)
\end{verbatim}
\includegraphics{./figs/stata_student}

Ici, on a choisi de représenter les données sous forme de barres
horizontales (\texttt{hbar} au lieu de \texttt{bar}), sachant que par défaut
Stata calcule automatiquement les moyennes conditionnelles. Pour afficher la
médiane, on utiliserait la commande \verb|graph hbar (median) DHH LHH|.
% 
%
%
\soln{\ref{exo:3.4}} Stata dispose de commandes appelées "immédiates"
permettant de calculer les statistiques de test associées à des données
saisies directement à l'invite de commande, c'est-à-dire sans passer par
l'importation d'un fichier de données ou la saisie d'un tableau
d'effectifs. Dans le cas présent, on peut répondre aux trois questions
posées à partir de la même commande, \texttt{tabi} (à ne pas confondre
avec la commande externe \texttt{chitesti} qui réalise des tests
d'ajustement à une loi donnée).  
\begin{verbatim}
. tabi 26 21\ 38 44, exact chi2 expected

+--------------------+
| Key                |
|--------------------|
|     frequency      |
| expected frequency |
+--------------------+

           |          col
       row |         1          2 |     Total
-----------+----------------------+----------
         1 |        26         21 |        47 
           |      23.3       23.7 |      47.0 
-----------+----------------------+----------
         2 |        38         44 |        82 
           |      40.7       41.3 |      82.0 
-----------+----------------------+----------
     Total |        64         65 |       129 
           |      64.0       65.0 |     129.0 

          Pearson chi2(1) =   0.9632   Pr = 0.326
           Fisher's exact =                 0.364
   1-sided Fisher's exact =                 0.212
\end{verbatim}
\emph{À noter} : Stata ne fournit pas de correction de continuité de Yates pour ce
type de test parmi les commandes de base.  
%
%
%
\soln{\ref{exo:3.5}} Comme dans l'exercice précédent, on pourrait travailler
à l'aide de commandes "immédiates". Toutefois, il est parfois utile de
recréer directement le tableau d'effectif sous Stata. Voici une façon de
procéder. \label{exo:3.5stata}

La première étape consiste à créer trois variables qui permettront de
stocker, pour chaque combinaison de traitement (placebo ou aspirine) et de
réponse (infarctus ou pas d'infarctus), les effectifs observés. Les deux
variables \texttt{traitement} et \texttt{infarctus} sont des variables
binaires (deux modalités, codées 0 et 1). 
\begin{verbatim}
. clear all
. input traitement infarctus N

     traitem~t  infarctus          N
  1. 0 1 28
  2. 0 0 656
  3. 1 1 18
  4. 1 0 658
  5. end
\end{verbatim}
Ensuite, on associe des labels aux modalités des variables nouvellement
créées. On peut ensuite vérifier que les données ont bien été enregistrées
dans le format attendu.
\begin{verbatim}
. label define tx 0 "Placebo" 1 "Aspirine"
. label values traitement tx
. label define ouinon 0 "Non" 1 "Oui"
. label values infarctus ouinon
. list

     +---------------------------+
     | traite~t   infarc~s     N |
     |---------------------------|
  1. |  Placebo        Oui    28 |
  2. |  Placebo        Non   656 |
  3. | Aspirine        Oui    18 |
  4. | Aspirine        Non   658 |
     +---------------------------+
\end{verbatim}

Les diagrammes en barres sont prouits avec la commande \verb|- graph bar -|.
\begin{verbatim}
. graph bar (asis) N, over(infarctus) asyvars over(traitement) legend(title("Infarctus"))
\end{verbatim}
L'option \texttt{asyvars} insérée entre les deux variables de classification
est indispensable si l'on souhaite que les barres aient des couleurs
différentes selon le type de variable considéré.

\includegraphics{./figs/stata_infarctus}
% FIXME:
% display as percent
% catplot infarctus traitement [fw=N], percent recast(bar)

Pour représenter les proportions d'infarctus selon le type de traitement, on
peut utiliser une commande externe de Stata (à installer de la manière
suivante : \verb|ssc install catplot|) qui est bien commode pour la
représentation graphique des données catégorielles. Son usage est le suivant :
\begin{verbatim}
. catplot infarctus traitement [fw=N] if infarctus==1, percent
\end{verbatim}

\includegraphics{./figs/stata_infarctus2}

On verra dans le chapitre consacré aux données épidémiologiques comment
estimer l'odds-ratio et son intervalle de confiance à partir de la commande
\texttt{cc}. Dans le cas présent, on se contentera d'utiliser
\texttt{tabodds} qui fournit une estimation des odds et de
l'odds-ratio. Pour ce dernier, il faut préciser l'option \texttt{or} comme
indiqué ci-après ainsi que la catégorie de référence (\texttt{base}).
\begin{verbatim}
. tabodds infarctus traitement [fweight=N], or base(2)

---------------------------------------------------------------------------
   infarctus |  Odds Ratio       chi2       P>chi2     [95% Conf. Interval]
-------------+-------------------------------------------------------------
         Non |           .          .           .              .          .
         Oui |           .          .           .              .          .
---------------------------------------------------------------------------
Test of homogeneity (equal odds): chi2(1)  =     2.13
                                  Pr>chi2  =   0.1446

Score test for trend of odds:     chi2(1)  =     2.13
                                  Pr>chi2  =   0.1446
\end{verbatim}
%
%
%
\soln{\ref{exo:4.1}} Le chargement des données ne pose pas vraiment de
problème puisque les données sont déjà disponibles au format Stata.
\begin{verbatim}
. use polymorphism.dta
. by genotype: summarize age

-----------------------------------------------------------------------------------------
-> genotype = 1.6/1.6

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         age |        14    64.64286    11.18108         43         79

-----------------------------------------------------------------------------------------
-> genotype = 1.6/0.7

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         age |        29    64.37931    13.25954         38         87

-----------------------------------------------------------------------------------------
-> genotype = 0.7/0.7

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         age |        16      50.375    10.63877         33         72
\end{verbatim}
Les intervalles de confiance reportés ci-dessus ne sont pas calculés à
partir de l'ANOVA réalisée ci-après, mais en considérant une approximation à
la loi normale. On peut en revanche obtenir un résumé descriptif simplifié
avec la commande \texttt{summarize}
\begin{verbatim}
. by genotype: summarize age
\end{verbatim}
ou plus détaillé en ajoutant l'option \texttt{detail}:
\begin{verbatim}
. by genotype: summarize age, detail
\end{verbatim}

Il est possible d'afficher la distribution des âges pour chacun des
génotypes à l'aide de boites à moustaches.
\begin{verbatim}
. graph box age, over(genotype)
\end{verbatim}

\includegraphics{./figs/stata_polymsm}

Si l'on souhaite représenter la distribution des âges à l'aide
d'histogrammes, on peut utiliser la commande \texttt{histogram} de la
manière suivante :
\begin{verbatim}
. histogram age, by(genotype, col(3))
\end{verbatim}
L'option \verb|by(genotype, col(3))| permet d'afficher les distributions
conditionnellement au génotype et d'aligner les histogrammes horizontalement
(c'est-à-dire sur 3 colonnes).

\includegraphics{./figs/stata_histby}

La commande \texttt{oneway} permet de réaliser une analyse de variance à un
effet fixe, comme suit :
\begin{verbatim}
. oneway age genotype

                        Analysis of Variance
    Source              SS         df      MS            F     Prob > F
------------------------------------------------------------------------
Between groups      2315.73355      2   1157.86678      7.86     0.0010
 Within groups      8245.79187     56   147.246283
------------------------------------------------------------------------
    Total           10561.5254     58   182.095266

Bartlett's test for equal variances:  chi2(2) =   1.0798  Prob>chi2 = 0.583
\end{verbatim}
Le tableau d'ANOVA produit par Stata est sensiblement identique à celui
fourni par \R, à ceci près que Stata reporte également les totaux pour les
sommes de carrés, carrés moyens et degrés de liberté associés. On notera
également que Stata fournit le résultat d'un test de Bartlett concernant
l'hypothèse d'homogénéité des variances. Le test de Levene pour tester
l'homogénéité des variances peut être obtenu à l'aide d'une approche un peu
différente : la commande \verb|- robvar -| qui fournit un ensemble de tests
robustes pour l'égalité des variances.
\begin{verbatim}
. robvar age, by(genotype)

            |     Summary of Age at Diagnosis
   Genotype |        Mean   Std. Dev.       Freq.
------------+------------------------------------
    1.6/1.6 |   64.642857   11.181077          14
    1.6/0.7 |    64.37931   13.259535          29
    0.7/0.7 |      50.375   10.638766          16
------------+------------------------------------
      Total |   60.644068   13.494268          59

W0  =  0.83032671   df(2, 56)     Pr > F = 0.44120161

W50 =  0.60460508   df(2, 56)     Pr > F = 0.54981692

W10 =  0.79381598   df(2, 56)     Pr > F = 0.45713722
\end{verbatim}
Le test de Levene est reporté sous la statistique de test \texttt{W0}.

Enfin, notons que le résumé numérique produit à l'étape précédente (moyennes
conditionnelles) peut être reproduit partiellement (sans les intervalles de
confiance) en ajoutant l'option \texttt{tabulate} :
\begin{verbatim}
. oneway age genotype, tabulate
\end{verbatim}
Si l'on souhaite former les intervalles de confiance associés aux moyennes
de groupe à partir des résultats de l'ANOVA, c'est-à-dire en considérant une
estimation de la résiduelle basée sur la variance commune, on peut procéder
comme dans le cas \R, en utilisant le carré moyen de l'erreur
(\texttt{Within groups}) et le quantile de référence de la loi $t$ (0.975)
que l'on peut obtenir comme ceci sous Stata (ici pour le premier génotype,
1.6/1.6) :
\begin{verbatim}
. display invttail(14-3, 0.025)
2.2009852
\end{verbatim}

Les différences de moyennes peuvent être obtenues en formant des contrastes
spécifiques à partir d'un modèle de régression qui donne des résultats
équivalents au modèle d'ANOVA, sous réserve de coder la variable qualitative
\texttt{genotype} sous forme d'une matrice d'indicatrices à l'aide de
l'opérateur \verb|i.*|. On ne présente pas les résultats de l'appel à la
commande \texttt{regress} puisque ce qui nous intéresse est simplement
d'exploiter les résultats qu'elle sauvegarde (cf. \verb|e()|).
\begin{verbatim}
. regress age i.genotype
\end{verbatim}

Pour la différence de moyennes entre génotype 0.7/0.7 et 1.6/0.7, par
exemple, on utiliserait la commande suivante :
\begin{verbatim}
. lincom 3.genotype - 2.genotype

 ( 1)  - 2.genotype + 3.genotype = 0

------------------------------------------------------------------------------
         age |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         (1) |  -14.00431   3.778935    -3.71   0.000    -21.57443   -6.434194
------------------------------------------------------------------------------
\end{verbatim}
On procédera de même pour les deux autres différences de moyennes, 0.7/0.7 -
1.6/1.6 et 1.6/0.7 - 1.6/1.6.

En fait, la même approche (par régression) nous permettrait d'obtenir les
intervalles de confiance à 95~\% pour chaque moyenne de groupe :
\begin{verbatim}
. lincom _cons + 1.genotype

 ( 1)  1b.genotype + _cons = 0

------------------------------------------------------------------------------
         age |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         (1) |   64.64286   3.243084    19.93   0.000     58.14618    71.13953
------------------------------------------------------------------------------
\end{verbatim}

\soln{\ref{exo:4.2}}
Les données peuvent être saisies manuellement dans Stata (ou à l'aide de
l'éditeur de données, ou \texttt{Data Editor}), comme présentées dans le
tableau de l'énoncé, c'est-à-dire trois variables organisées en colonnes :

\includegraphics{./figs/stata_dataentry}

Pour utiliser la commande \texttt{oneway}, il est toutefois nécessaire de
réarranger ces trois variables de façon à avoir une variable réponse et une
variable décrivant le facteur de classification (à trois classes, A, B et
C). Pour cela, on utilise la commande \texttt{stack} qui permet de
concaténer (verticalement) des variables arrangées en colonnes.
\begin{verbatim}
. stack A B C, into(pb) clear
. rename _stack tx
. label define txlab 1 "A" 2 "B" 3 "C"
. label values tx txlab
. list in 1/5

     +-----------+
     | tx     pb |
     |-----------|
  1. |  A   19.8 |
  2. |  A   20.5 |
  3. |  A   23.7 |
  4. |  A   27.1 |
  5. |  A   29.6 |
     +-----------+
\end{verbatim}

Concernant le résumé descriptif des données, on pourra procéder comme suit :
\begin{verbatim}
. tabstat pb, stats(mean sd n) by(tx)

Summary for variables: pb
     by categories of: tx 

    tx |      mean        sd         N
-------+------------------------------
     A |      25.1  4.438468         6
     B |  20.76667  2.798333         6
     C |  18.28333  1.965113         6
-------+------------------------------
 Total |  21.38333  4.199335        18
--------------------------------------
. graph box pb, over(tx)
\end{verbatim}

\includegraphics{./figs/stata_pbtx}

L'ANOVA à un facteur est réalisée comme dans l'exercice précédent avec la
commande \texttt{oneway}. Notons que l'on a ajouté l'option
\texttt{bonferroni} pour tester automatiquement les paires de moyennes entre
elles. Le résultat du test F est significatif et le test de Bartlett
concernant l'homogénéité des variances suggère que cette hypothèse est
raisonnable au vu des données. Les tests multiples protégés permettent de
préciser 
\begin{verbatim}
. oneway pb tx, bonferroni

                        Analysis of Variance
    Source              SS         df      MS            F     Prob > F
------------------------------------------------------------------------
Between groups      142.823331      2   71.4116655      6.82     0.0078
 Within groups      156.961681     15   10.4641121
------------------------------------------------------------------------
    Total           299.785012     17   17.6344125

Bartlett's test for equal variances:  chi2(2) =   3.0035  Prob>chi2 = 0.223

                            Comparison of pb by tx
                                (Bonferroni)
Row Mean-|
Col Mean |          A          B
---------+----------------------
       B |   -4.33333
         |      0.105
         |
       C |   -6.81667   -2.48333
         |      0.007      0.610
\end{verbatim}

Si l'hypothèse de normalité doit être remise en cause, on peut alors
utiliser la commande \texttt{kwallis} pour réaliser une ANOVA sur les rangs
(approche non-paramétrique).
\begin{verbatim}
. kwallis pb, by(tx)

Kruskal-Wallis equality-of-populations rank test

  +---------------------+
  | tx | Obs | Rank Sum |
  |----+-----+----------|
  |  A |   6 |    82.00 |
  |  B |   6 |    59.00 |
  |  C |   6 |    30.00 |
  +---------------------+

chi-squared =     7.942 with 2 d.f.
probability =     0.0189

chi-squared with ties =     7.942 with 2 d.f.
probability =     0.0189
\end{verbatim}
Le résultat ci-dessus est cohérent avec le résultat de l'ANOVA
paramétrique. Pour la comparaison des paires de variables, on utilisera un
test de Wilcoxon (échantillons indépendants), en corrigeant la $p$-valeur
par le nombre de tests réalisés (ici, 3). On peut soit utiliser la commande
\texttt{ranksum}, par exemple \verb+ranksum pb if tx == 1 | tx == 2, by(tx)+, 
pour comparer les deux premiers traitements, ou alors la commande
\texttt{kwallis2} qui fournit les mêmes résultats que \texttt{kwallis} mais
également les tests post-hoc associés. Il est toutefois nécessaire
d'installer ce package (\texttt{findit kwallis2}, puis suivre la procédure
d'installation indiquée).
\begin{verbatim}
. kwallis2 pb, by(tx)

One-way analysis of variance by ranks (Kruskal-Wallis Test)

tx       Obs   RankSum  RankMean 
--------------------------------
  1        6     82.00     13.67
  2        6     59.00      9.83
  3        6     30.00      5.00

Chi-squared (uncorrected for ties) =     7.942 with    2 d.f. (p = 0.01886)
Chi-squared (corrected for ties)   =     7.942 with    2 d.f. (p = 0.01886)

Multiple comparisons between groups
-----------------------------------
(Adjusted p-value for significance is 0.008333)

Ho: pb(tx==1) = pb(tx==2)
    RankMeans difference =      3.83  Critical value =      7.38
    Prob = 0.106805 (NS)

Ho: pb(tx==1) = pb(tx==3)
    RankMeans difference =      8.67  Critical value =      7.38
    Prob = 0.002463 (S)

Ho: pb(tx==2) = pb(tx==3)
    RankMeans difference =      4.83  Critical value =      7.38
    Prob = 0.058424 (NS)
\end{verbatim}

\soln{\ref{exo:4.3}}
Il n'existe pas de solution très commode pour importer un fichier SPSS sous
Stata directement, sauf lorsque l'on travaille sous Windows (voir la
commande \texttt{usespss}). Le fichier contenant les données,
\texttt{weights.sav}, a été exporté depuis R au format Stata à l'aide des
commandes suivantes :
\begin{verbatim}
library(foreign)
weights <- read.spss("weights.sav", to.data.frame=TRUE)
write.dta(weights, file="weights.dta")
\end{verbatim}
Depuis Stata, on peut donc l'importer très simplement à l'aide de la
commande \texttt{use} :
\begin{verbatim}
. use "weights.dta", clear
. list in 1/5

     +----------------------------------------------------------------+
     |   ID   WEIGHT   LENGTH   HEADC   GENDER   EDUCATIO      PARITY |
     |----------------------------------------------------------------|
  1. | L001     3.95     55.5    37.5   Female   tertiary   3 or more |
  2. | L003     4.63       57    38.5   Female   tertiary   Singleton |
  3. | L004     4.75       56    38.5     Male     year12   2 sibling |
  4. | L005     3.92       56      39     Male   tertiary   One sibli |
  5. | L006     4.56       55    39.5     Male     year10   2 sibling |
     +----------------------------------------------------------------+
\end{verbatim}

Le tableau d'effectifs et de fréquences relatives pour la variable
\texttt{PARITY} s'obtient à partir de la commande \texttt{tabulate} :
\begin{verbatim}
. tabulate PARITY

            PARITY |      Freq.     Percent        Cum.
-------------------+-----------------------------------
         Singleton |        180       32.73       32.73
       One sibling |        192       34.91       67.64
        2 siblings |        116       21.09       88.73
3 or more siblings |         62       11.27      100.00
-------------------+-----------------------------------
             Total |        550      100.00
\end{verbatim}
Les moyennes et écarts-type du poids selon la taille de la fratrie
s'obtiennent ainsi :
\begin{verbatim}
. tabstat WEIGHT, stats(mean sd) by(PARITY) format(%9.2f)

Summary for variables: WEIGHT
     by categories of: PARITY (PARITY)

          PARITY |      mean        sd
-----------------+--------------------
       Singleton |      4.26      0.62
     One sibling |      4.39      0.59
      2 siblings |      4.46      0.61
3 or more siblin |      4.43      0.54
-----------------+--------------------
           Total |      4.37      0.60
--------------------------------------
\end{verbatim}
L'option \verb|format(%9.2f)| permet de limiter l'affichage des nombres à
deux décimales.

L'ANOVA à un facteur se réalise comme dans les exercices précédents, à
l'aide de la commande \texttt{oneway} et en fournissant la variable réponse
et la variable qualitative décrivant les groupes à comparer.
\begin{verbatim}
. oneway WEIGHT PARITY

                        Analysis of Variance
    Source              SS         df      MS            F     Prob > F
------------------------------------------------------------------------
Between groups       3.4769599      3   1.15898663      3.24     0.0219
 Within groups      195.364879    546   .357811134
------------------------------------------------------------------------
    Total           198.841839    549   .362189142

Bartlett's test for equal variances:  chi2(3) =   1.9085  Prob>chi2 = 0.592
\end{verbatim}
Comme on l'a vu avec R, on peut calculer la part de variance expliquée à
partir des sommes de carré reportées ci-dessus. On peut également utiliser
la commande \texttt{anova} qui renvoit, outre un tableau d'analyse de
variance, le coefficient de détermination associé au modèle.
\begin{verbatim}
. anova WEIGHT PARITY

                           Number of obs =     550     R-squared     =  0.0175
                           Root MSE      = .598173     Adj R-squared =  0.0121

                  Source |  Partial SS    df       MS           F     Prob > F
              -----------+----------------------------------------------------
                   Model |   3.4769599     3  1.15898663       3.24     0.0219
                         |
                  PARITY |   3.4769599     3  1.15898663       3.24     0.0219
                         |
                Residual |  195.364879   546  .357811134   
              -----------+----------------------------------------------------
                   Total |  198.841839   549  .362189142
\end{verbatim}

Pour afficher les données sous forme d'histogrammes pour chaque groupe, il
faut utiliser la commande \texttt{histogram} avec l'option \texttt{by} pour
définir le facteur de classification. L'option \texttt{freq} permet quant à
elle d'afficher des effectifs plutôt que des proportions (ou densités).
\begin{verbatim}
. histogram WEIGHT, by(PARITY) freq
\end{verbatim}

\includegraphics{./figs/stata_histweight}

Pour afficher afficher un diagramme de dispersion, comme sous R, on peut
utiliser une commande externe, telle que \texttt{stripplot} (\texttt{ssc
  install stripplot}), par exemple
\begin{verbatim}
. stripplot WEIGHT, over(PARITY) stack height(.4) center vertical width(.3)
\end{verbatim}
ou plus simplement 
\begin{verbatim}
. twoway scatter WEIGHT PARITY, jitter(3) xlabel(1 "Singleton" 2 "One sibling" 3 "2 siblings" 4 "3 more")
\end{verbatim}

\includegraphics{./figs/stata_stripheight}

La commande \texttt{oneway} affiche par défaut le résultat d'un test de
Bartlett pour comparer les variances des groupes entre elles. Si l'on
souhaite utiliser un test de Levene, il faut utiliser la commande
\texttt{robvar} qui fournit le résultat du test de Levene (\texttt{W0}) :
\begin{verbatim}
. robvar WEIGHT, by(PARITY)

            |          Summary of WEIGHT
     PARITY |        Mean   Std. Dev.       Freq.
------------+------------------------------------
  Singleton |   4.2589445   .61950106         180
  One sibli |   4.3886979   .59258231         192
  2 sibling |   4.4600862   .60519991         116
  3 or more |   4.4341935   .53526321          62
------------+------------------------------------
      Total |   4.3664182   .60182152         550

W0  =  0.63850632   df(3, 546)     Pr > F = 0.59046456

W50 =  0.64415596   df(3, 546)     Pr > F = 0.58688851

W10 =  0.64584135   df(3, 546)     Pr > F = 0.58582455
\end{verbatim}

Il existe plusieurs façon de recoder des variables sous Stata, mais dans le
cas présent le plus simple pour aggréger les deux dernières classes
(\texttt{2 siblings} et \texttt{3 or more}) consiste à générer une deuxième
variable, \texttt{PARITY2}, de la manière suivante :
\begin{verbatim}
. recode PARITY (1=1) (2=2) (3/4=3), gen(PARITY2)
\end{verbatim}
Pour vérifier que la conversion s'est déroulée correctement, on affichera un
simple tableau de contingence croisant les effectifs des deux variables :
\begin{verbatim}
. tabulate PARITY PARITY2

                   |    RECODE of PARITY (PARITY)
            PARITY |         1          2          3 |     Total
-------------------+---------------------------------+----------
         Singleton |       180          0          0 |       180 
       One sibling |         0        192          0 |       192 
        2 siblings |         0          0        116 |       116 
3 or more siblings |         0          0         62 |        62 
-------------------+---------------------------------+----------
             Total |       180        192        178 |       550
\end{verbatim}

Les résultats de l'ANOVA à un facteur considérant la variable nouvellement
créée sont reportés ci-dessous :
\begin{verbatim}
. oneway WEIGHT PARITY2

                        Analysis of Variance
    Source              SS         df      MS            F     Prob > F
------------------------------------------------------------------------
Between groups      3.44987154      2   1.72493577      4.83     0.0083
 Within groups      195.391968    547   .357206522
------------------------------------------------------------------------
    Total           198.841839    549   .362189142

Bartlett's test for equal variances:  chi2(2) =   0.7963  Prob>chi2 = 0.672
\end{verbatim}
Pour le test de la tendance linéaire, on présente deux manières (méthode des
contrastes et régression linéaire). Pour générer un contraste testant la
tendance linéaire, il faut explicitement demander à Stata d'effectuer une
régression en considérant la variable \texttt{PARITY2} comme une variable
qualitative (passage par des variables indicatrices codant pour les deux
dernières modalités du facteur). On utilisera dans ce cas la commande de
post-estimation \texttt{contrast}. Comme le résultat de la régression sur la
variable qualitative ne nous intéresse pas vraiment, on demandera à Stata de
ne pas afficher les résultats à l'aide de l'instruction \verb|quietly :|.
\begin{verbatim}
. quietly: regress WEIGHT i.PARITY2
. contrast p.PARITY2, noeffects

Contrasts of marginal linear predictions

Margins      : asbalanced

------------------------------------------------
             |         df           F        P>F
-------------+----------------------------------
     PARITY2 |
   (linear)  |          1        9.25     0.0025
(quadratic)  |          1        0.40     0.5288
      Joint  |          2        4.83     0.0083
             |
    Residual |        547
------------------------------------------------
\end{verbatim}
Le contraste d'intérêt apparaît sur la ligne intitulée \verb|(linear)|.

Pour l'approche par régression linéaire simple, la commande est plus simple :
\begin{verbatim}
. regress WEIGHT PARITY2

      Source |       SS       df       MS              Number of obs =     550
-------------+------------------------------           F(  1,   548) =    9.27
       Model |  3.30800821     1  3.30800821           Prob > F      =  0.0024
    Residual |  195.533831   548   .35681356           R-squared     =  0.0166
-------------+------------------------------           Adj R-squared =  0.0148
       Total |  198.841839   549  .362189142           Root MSE      =  .59734

------------------------------------------------------------------------------
      WEIGHT |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     PARITY2 |   .0961272   .0315707     3.04   0.002     .0341129    .1581415
       _cons |   4.174513   .0679786    61.41   0.000     4.040983    4.308044
------------------------------------------------------------------------------
\end{verbatim}
Le test de tendance correspond au test associé à la pente de la droite de
régression, ici le coefficient \texttt{PARITY2}.

\chapter*{Devoir \no 7}
\addcontentsline{toc}{chapter}{Devoir \no 7}

\chapter{Régression linéaire et logistique}

Dans cette partie sont proposés les corrigés des exercices 5.1, 5.2, 5.3, 6.1,
6.3 et 6.5 dont les énoncés figurent dans les chapitres 5
(p.~\pageref{chap:reg}) et 6 (p.~\pageref{chap:logistic}).

\section*{Corrigés}

\soln{\ref{exo:5.1}}
Les données tabulées sont disponibles dans un fichier texte que l'on peut
importer à l'aide de la commande \texttt{insheet}. Toutefois, on peut
également importer les données avec \texttt{infile} qui a l'avantage de
fonctionner même dans le cas où les données ne sont pas issue d'un tableau
de type Excel.
\begin{verbatim}
. infile int (Sub Age Sex) Height Weight BMP FEV RV FRC TLC PEmax using "cystic.dat" in 2/26, clear
\end{verbatim}
Dans l'instruction ci-dessus, on a explicitement demandé à Stata de coder
les trois premières variables sous forme d'entiers et non de nombres réels,
ce qui dans le présent ne change pas grand-chose mais permet d'économiser de
l'espace mémoire pour les gros jeux de données.
Comme on l'a fait avec R, on recodera d'emblée la variable \texttt{Sex} en
variable qualitative pour éviter toute confusion.
\begin{verbatim}
. label define labsex 0 "M" 1 "F"
. label values Sex labsex
. tabulate Sex

        Sex |      Freq.     Percent        Cum.
------------+-----------------------------------
          M |         14       56.00       56.00
          F |         11       44.00      100.00
------------+-----------------------------------
      Total |         25      100.00
\end{verbatim}

L'estimation du coefficient de corrélation de Bravais-Pearson se fait avec
la commande \texttt{correlate}, mais celle-ci ne fournit pas d'options pour
l'intervalle de confiance. On peut utiliser des commandes externes comme
\texttt{ci2} qui fonctionne sur le même principe que la commande interne
\texttt{ci} (pour les moyennes et proportions) dans le cas paramétrique ou
non-paramétrique (Spearman) ou bien \texttt{corrci}, uniquement pour le cas
paramétrique. \label{cmd:corrci}
\begin{verbatim}
. correlate PEmax Weight
(obs=25)

             |    PEmax   Weight
-------------+------------------
       PEmax |   1.0000
      Weight |   0.6363   1.0000
. corrci PEmax Weight

(obs=25)

                correlation and 95% limits
PEmax  Weight      0.636    0.322    0.824
\end{verbatim}

Pour tester ce coefficient de corrélation contre l'alternative $H_0:
\rho=0.3$, il faut
% FIXME:
% change to compute test against H_0: \rho=0.3
\begin{verbatim}
. pwcorr PEmax Weight, sig

             |    PEmax   Weight
-------------+------------------
       PEmax |   1.0000 
             |
             |
      Weight |   0.6363   1.0000 
             |   0.0006
             |
\end{verbatim}

Pour afficher l'ensemble des diagrammes de dispersion, on utilise la
commande \texttt{graph matrix} en sépcifiant la liste de variables que l'on
souhaite voir figurer dans le graphique. Ici, on omettra la variable
\texttt{Sex} : 
\begin{verbatim}
. graph matrix Age Height-PEmax
\end{verbatim}

\includegraphics{./figs/stata_splom}

Les corrélations pour chaque paire de variables s'obtiennent avec la
commande \texttt{pwcorr}, et on utilisera la même notation pour indiquer les
variables qui nous intéressent.
\begin{verbatim}
. pwcorr Age Height-PEmax

             |      Age   Height   Weight      BMP      FEV       RV      FRC      TLC      PEmax
-------------+-----------------------------------------------------------------------------------
         Age |   1.0000 
      Height |   0.9261   1.0000 
      Weight |   0.9065   0.9221   1.0000 
         BMP |   0.3778   0.4408   0.6703   1.0000 
         FEV |   0.2945   0.3167   0.4492   0.5455   1.0000 
          RV |  -0.5519  -0.5695  -0.6234  -0.5824  -0.6659   1.0000 
         FRC |  -0.6394  -0.6243  -0.6182  -0.4344  -0.6651   0.9106   1.0000 
         TLC |  -0.4734  -0.4595  -0.4214  -0.3633  -0.4425   0.5899   0.7056    1.0000 
       PEmax |   0.6135   0.5992   0.6363   0.2295   0.4534  -0.3156  -0.4172   -0.1805   1.0000
\end{verbatim}
Pour les corrélations de Spearman, on remplacera \texttt{pwcorr} par
\texttt{spearman}.
\begin{verbatim}
. spearman Age Height-PEmax, stats(rho)
(obs=25)

             |      Age   Height   Weight      BMP      FEV       RV      FRC      TLC    PEmax
-------------+---------------------------------------------------------------------------------
         Age |   1.0000 
      Height |   0.9335   1.0000 
      Weight |   0.9013   0.9619   1.0000 
         BMP |   0.5091   0.5734   0.7264   1.0000 
         FEV |   0.2975   0.4256   0.4637   0.5623   1.0000 
          RV |  -0.5815  -0.6221  -0.7005  -0.6917  -0.6830   1.0000 
         FRC |  -0.7185  -0.6642  -0.6706  -0.5547  -0.6044   0.8547   1.0000 
         TLC |  -0.4926  -0.4734  -0.4846  -0.4935  -0.4398   0.5895   0.6721   1.0000 
       PEmax |   0.5198   0.5920   0.4881   0.2224   0.3140  -0.3089  -0.3835  -0.1482   1.0000
\end{verbatim}

% FIXME:
% filtrage correlations elevees à faire.

La corrélation partielle entre les variables \texttt{PEmax} et
\texttt{Weight} en tenant compte de \texttt{Age} est obtenue à l'aide de la
commande \texttt{pcorr} ; la variable d'intérêt doit être située en première
position dans la liste des variables et par défaut Stata affiche l'ensemble
des coefficients de corrélation partielle pour les autres variables.
\begin{verbatim}
. pcorr PEmax Weight Age
(obs=25)

Partial and semipartial correlations of PEmax with

               Partial   Semipartial      Partial   Semipartial   Significance
   Variable |    Corr.         Corr.      Corr.^2       Corr.^2          Value
------------+-----------------------------------------------------------------
     Weight |   0.2405        0.1899       0.0578        0.0361         0.2577
        Age |   0.1126        0.0869       0.0127        0.0076         0.6002
\end{verbatim}
Pour réaliser un tercilage de la variable \texttt{Age}, on peut procéder comme à
l'exercice~\ref{exo:2.4}, c'est-à-dire en créant une variable dérivée, avec
\texttt{egen}, et en utilisant la fonction \texttt{cut}. Mais on peut
directement utiliser la commande \texttt{xtile} qui est plus simple d'emploi :
\begin{verbatim}
. xtile Age3 = Age, nq(3)
\end{verbatim}
Enfin, pour le diagramme de dispersion, voici une première solution :
\begin{verbatim}
. scatter PEmax Weight if Age3 != 2, mlab(Age3)
\end{verbatim}
On pourrait également (2\ieme\ solution) enchaîner deux appels à la commande
\texttt{scatter}, en restreignant à chaque fois l'échantillon aux seules
classe de \texttt{Age3} qui nous intéressent (1 et 3, en l'occurence).
\begin{verbatim}
. scatter PEmax Weight if Age3 == 1, msymbol(circle) || scatter PEmax Weight if Age3 == 3, msymbol(square) 
  legend(label(1 "1st tercile") label(2 "3rd tercile"))
\end{verbatim}

\includegraphics{./figs/stata_tercile}

\soln{\ref{exo:5.2}}
Le chargement des données ne pose pas de difficultés particulières car
celles-ci ont été exportées depuis Excel et sont au format CSV. On veillera
cependant à préciser le type de délimiteur de champ, ici un point-virgule/
\begin{verbatim}
. insheet using "/Users/chl/Documents/Tutors/cesam-r/tex/quetelet.csv", delim(";") clear
(5 vars, 32 obs)
. describe

Contains data
  obs:            32                          
 vars:             5                          
 size:           320                          
-----------------------------------------------------------------------------------------
              storage  display     value
variable name   type   format      label      variable label
-----------------------------------------------------------------------------------------
id              byte   %8.0g                  ID
pas             int    %8.0g                  PAS
qtt             str5   %9s                    QTT
age             byte   %8.0g                  AGE
tab             byte   %8.0g                  TAB
-----------------------------------------------------------------------------------------
Sorted by:
\end{verbatim}
Après l'importation, on remarque que la variable \texttt{qtt} n'est pas
reconnue en tant que nombre mais a été codée sous forme de chaîne de
caractère. Ceci s'explique par le fait que la partie décimale est séparée de
la partie entière par une virgule et non un point. Il faut donc convertir cette
variable en nombre, ce que l'on peut réaliser avec la commande
\texttt{destring} et l'option \texttt{dpcomma}.
\begin{verbatim}
. destring qtt, dpcomma replace
qtt has all characters numeric; replaced as double
\end{verbatim}
Ensuite, on recode la variable \texttt{tab} en variable qualitative avec des
étiquettes plus informatives. 
\begin{verbatim}
. label define ltab 0 "NF" 1 "F"
. label values tab ltab
. list in 1/5

     +------------------------------+
     | id   pas     qtt   age   tab |
     |------------------------------|
  1. |  1   135   2.876    45    NF |
  2. |  2   122   3.251    41    NF |
  3. |  3   130     3.1    49    NF |
  4. |  4   148   3.768    52    NF |
  5. |  5   146   2.979    54     F |
     +------------------------------+
\end{verbatim}
Enfin, le résumé numérique des variable s'obtient avec la commande \texttt{summarize}.
\begin{verbatim}
. summarize pas-tab

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         pas |        32    144.5313    14.39755        120        180
         qtt |        32    3.441094    .4970781      2.368      4.637
         age |        32       53.25    6.956083         41         65
         tab |        32      .53125    .5070073          0          1
\end{verbatim}
Comme la variable \texttt{tab} est internellement codée en 0/1, la moyenne
correspond à la fréquence relative des fumeurs, soit 53~\%.

Le coefficeint de corrélation liénaire entre les variables \texttt{pas} et
\texttt{qtt} peut être estimé, ainsi que son intervalle de confiance, avec
la commande \texttt{corrci} (p.~\pageref{cmd:corrci}). L'option
\texttt{level(90)} permet de spécifier le niveau de confiance.
\begin{verbatim}
. corrci pas qtt, level(90)

(obs=32)

          correlation and 90% limits
pas qtt      0.742    0.571    0.851
\end{verbatim}

La commande \texttt{regress} permet d'effectuer une régression linéaire
pour une variable réponse (placée en premier dans la liste des variables) et
une ou plusieurs variables explicatives. On l'utilise comme suit :
\begin{verbatim}
. regress pas qtt

      Source |       SS       df       MS              Number of obs =      32
-------------+------------------------------           F(  1,    30) =   36.75
       Model |  3537.94574     1  3537.94574           Prob > F      =  0.0000
    Residual |  2888.02301    30  96.2674337           R-squared     =  0.5506
-------------+------------------------------           Adj R-squared =  0.5356
       Total |  6425.96875    31  207.289315           Root MSE      =  9.8116

------------------------------------------------------------------------------
         pas |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         qtt |   21.49167   3.545147     6.06   0.000     14.25151    28.73182
       _cons |    70.5764   12.32187     5.73   0.000     45.41179    95.74101
------------------------------------------------------------------------------
\end{verbatim}
Par défaut, on obtient un tableau d'analyse de variance pour le modèle de
régression et un tableau des coefficients du modèle, ici l'ordonnée à
l'origine (70.58) et la pente de la droite de régression (21.49). Le test
de Student associé à la pente permet d'évaluer sa significativité au vu des
données. Si l'on souhaite manipuler les coefficients de régression, il est
possible de les extraire du tableau de résultats comme indiqué ci-dessous :
\begin{verbatim}
. matrix b = e(b)
. svmat b
. di "pente = " b1 ", ordonnée origine = " b2
pente = 21.491669, ordonnée origine = 70.576401
\end{verbatim}

En ce qui concerne l'affichage du diagramme de dispersion avec les deux
droites de régression superposées, on utilise une combinaison de
\texttt{lfit} (pour tracer la droite de régression) et \texttt{scatter}
(pour afficher les observations). L'inconvénient est qu'il est nécessaire de
construire manuellement la légende. On notera également que l'on impose que
les droites de régression soient présentées sur toute l'étendue de la
variable \texttt{qtt}, d'où l'option \texttt{range(2, 5)}. \label{para:twoway}
\begin{verbatim}
. twoway lfit pas qtt if tab == 0, range(2 5) lpattern(dot) || 
  scatter pas qtt if tab == 0, msymbol(square) || 
  lfit pas qtt if tab == 1, range(2 5) ||
  scatter pas qtt if tab == 1, msymbol(circle) 
  legend(label(1 "") label(2 "NF") label(3 "") label(4 "F"))
\end{verbatim}

\includegraphics{./figs/stata_lfit}

La régression de \texttt{qtt} sur \texttt{pas} en restreignant l'analyse aux
seules observations du groupe fumeur (\verb|tab == 1| ou \verb|tab == "F"|) 
ne pose pas de problème car on vient de le voir dans l'application
précdente, il suffit d'ajouter une option \texttt{if tab == 1} :
\begin{verbatim}
. regress pas qtt if tab == 1

      Source |       SS       df       MS              Number of obs =      17
-------------+------------------------------           F(  1,    15) =   19.40
       Model |  2088.16977     1  2088.16977           Prob > F      =  0.0005
    Residual |  1614.30082    15  107.620055           R-squared     =  0.5640
-------------+------------------------------           Adj R-squared =  0.5349
       Total |  3702.47059    16  231.404412           Root MSE      =  10.374

------------------------------------------------------------------------------
         pas |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         qtt |   20.11804   4.567193     4.40   0.001      10.3833    29.85278
       _cons |   79.25533   15.76837     5.03   0.000     45.64585    112.8648
------------------------------------------------------------------------------
\end{verbatim}

\soln{\ref{exo:5.3}}
Puisque les données sont disponibles au format CSV \og classique\fg\
(utilisant la virgule comme séparateur de champ), on peut les importer très
simplement à l'aide de la commande \texttt{insheet}. 
\begin{verbatim}
. insheet using "/Users/chl/Documents/Tutors/cesam-r/tex/Framingham.csv"
(10 vars, 4699 obs)
. describe, simple
sex       sbp       dbp       scl       chdfate   followup  age       bmi   month     id
. list in 1/5

     +------------------------------------------------------------------------+
     | sex   sbp   dbp   scl   chdfate   followup   age    bmi   month     id |
     |------------------------------------------------------------------------|
  1. |   1   120    80   267         1         18    55     25       8   2642 |
  2. |   1   130    78   192         1         35    53   28.4      12   4627 |
  3. |   1   144    90   207         1        109    61   25.1       8   2568 |
  4. |   1    92    66   231         1        147    48   26.2      11   4192 |
  5. |   1   162    98   271         1        169    39   28.4      11   3977 |
     +------------------------------------------------------------------------+
\end{verbatim}
Pour faciliter l'interprétation et la lecture des tableaux de résultats, on
associera des étiquettes plus informatives à la variable qualitative \texttt{sex}.
\begin{verbatim}
. label define labsex 1 "M" 2 "F"
. label values sex labsex
. tabulate sex

        sex |      Freq.     Percent        Cum.
------------+-----------------------------------
          M |      2,049       43.61       43.61
          F |      2,650       56.39      100.00
------------+-----------------------------------
      Total |      4,699      100.00
\end{verbatim}
Pour afficher un résumé du nombre de données manquantes pour chacune des
variables de ce tableau de données, on peut utiliser la commande suivante :
\begin{verbatim}
. misstable summarize
                                                               Obs<.
                                                +------------------------------
               |                                | Unique
      Variable |     Obs=.     Obs>.     Obs<.  | values        Min         Max
  -------------+--------------------------------+------------------------------
           scl |        33               4,666  |    248        115         568
           bmi |         9               4,690  |    248       16.2        57.6
  -----------------------------------------------------------------------------
\end{verbatim}
On voit donc que les variables \texttt{scl} et \texttt{bmi} incluent 33 et 9
valeurs manquantes, respectivement. D'où le tableau corrigé des effectifs
par sexe :
\begin{verbatim}
. tabulate sex if bmi < .

        sex |      Freq.     Percent        Cum.
------------+-----------------------------------
          M |      2,047       43.65       43.65
          F |      2,643       56.35      100.00
------------+-----------------------------------
      Total |      4,690      100.00
\end{verbatim}

Sous Stata, il n'est pas possible d'utiliser des marqueurs transparents dans
un diagramme de dispersion. Dans le cas où le nombre d'observations est
élevé, Il est donc préférable de modifier le type de symbole par défaut
(disque) et d'utiliser de petits cercles (\texttt{oh} ou \texttt{Oh}).
\begin{verbatim}
. scatter sbp bmi, by(sex) msymbol(Oh)
\end{verbatim}

\includegraphics{./figs/stata_overplotting}

Le coefficient de corrélation linéaire entre les variables \texttt{sbp} et
\texttt{bmi} selon le sexe peut être obtenu à l'aide de \texttt{correlate}
après stratification sur le facteur \texttt{sex}. On notera que l'option
\texttt{by} figure en premier et qu'il est nécessaire de trier les données
dans un premier temps, d'où l'ajout de la commande \texttt{sort}
immédiatement après la stratification.
\begin{verbatim}
. by sex, sort: correlate sbp bmi

----------------------------------------------------------------------------------------
-> sex = M
(obs=2047)

             |      sbp      bmi
-------------+------------------
         sbp |   1.0000
         bmi |   0.2364   1.0000


----------------------------------------------------------------------------------------
-> sex = F
(obs=2643)

             |      sbp      bmi
-------------+------------------
         sbp |   1.0000
         bmi |   0.3736   1.0000
\end{verbatim}

% FIXME:
% test correlation by sex

Pour le modèle de régression, on va d'abord transformer les variables
\texttt{bmi} (variable explicative) et \texttt{sbp} (variable réponse) à
l'aide d'une transformation logarithmique.
\begin{verbatim}
. gen logbmi = log(bmi)
(9 missing values generated)
. gen logsbp = log(sbp)
\end{verbatim}
On peut ensuite vérifier visuellement à l'aide d'un histogramme (ou d'un
QQ-plot) que cette transformation a bien permis de ramener les distributions
de ces deux variables proches de la normale. Pour afficher simultanément les
quatre histogrammes, on peut sauver chacune des figures au format Stata
(\texttt{gph}) et ensuite les combiner à l'aide de la commande \texttt{graph combine}.
\begin{verbatim}
. histogram bmi, saving(gphbmi)
(bin=36, start=16.200001, width=1.1499999)
(file gphbmi.gph saved)
. histogram logbmi, saving(gphlogbmi)
(bin=36, start=2.7850113, width=.03523642)
(file gphlogbmi.gph saved)
. histogram sbp, saving(gphsbp)
(bin=36, start=80, width=5.2777778)
(file gphsbp.gph saved)
. histogram logsbp, saving(gphlogsbp)
(bin=36, start=4.3820267, width=.03378876)
(file gphlogsbp.gph saved)
. graph combine gphbmi.gph gphlogbmi.gph gphsbp.gph gphlogsbp.gph
\end{verbatim}

\includegraphics{./figs/stata_combine}

Le modèle de régression stratifié par sexe ne pose pas de problème majeur,
et contrairement à R il n'est pas nécessaire de calculer les intervalles de
confiance pour les pentes avec une commande séparée car ceux-ci sont
directement fournis dans le tableau de résultats renvoyés par Stata.
\begin{verbatim}
. regress logsbp logbmi if sex == 1, noheader
------------------------------------------------------------------------------
      logsbp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      logbmi |    .272646   .0232152    11.74   0.000     .2271182    .3181739
       _cons |   3.988043   .0754584    52.85   0.000      3.84006    4.136026
------------------------------------------------------------------------------

. regress logsbp logbmi if sex == 2, noheader
------------------------------------------------------------------------------
      logsbp |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      logbmi |   .3985947   .0185464    21.49   0.000     .3622278    .4349616
       _cons |   3.593017   .0597887    60.10   0.000     3.475779    3.710254
------------------------------------------------------------------------------
\end{verbatim}

\soln{\ref{exo:6.1}}
Dans un premier temps, il est nécessaire de construire le tableau
d'effectifs donné dans l'énoncé. On s'inspirera de la méthode manuelle vue à
l'exercice~\ref{exo:3.5stata} (p.~\pageref{exo:3.5stata}). Par contre, on va
saisir directement les labels des variables et non des codes
numériques. Pour cela, il est nécessaire d'indiquer à Stata quel est le
format de ces labels à l'aide de l'instruction \texttt{str} que l'on suffixe
par le nombre de caractères que l'on souhaite utiliser pour le codage des
modalités des variables.
\begin{verbatim}
. clear all
. input str1 traitement str3 infection N

     traitem~t  infection          N
  1. "A" "Non" 157
  2. "B" "Non" 119
  3. "A" "Oui" 52
  4. "B" "Oui" 103
  5. end
. list

     +---------------------------+
     | traite~t   infect~n     N |
     |---------------------------|
  1. |        A        Oui   157 |
  2. |        B        Oui   119 |
  3. |        A        Non    52 |
  4. |        B        Non   103 |
     +---------------------------+
\end{verbatim}


Voici donc le tableau de l'énoncé, avec le test du $\chi^2$ associé :
\begin{verbatim}
. tabulate traitement infection [fweight=N], chi

           |       infection
traitement |       Non        Oui |     Total
-----------+----------------------+----------
         A |       157         52 |       209 
         B |       119        103 |       222 
-----------+----------------------+----------
     Total |       276        155 |       431 

          Pearson chi2(1) =  21.6401   Pr = 0.000
\end{verbatim}
Si l'on souhaite une valeur plus précise pour le degré de significativité du
test, on peut utiliser les informations renvoyées (de manière invisible) par
la commande précédente :
\begin{verbatim}
. return list

scalars:
                  r(N) =  431
                  r(r) =  2
                  r(c) =  2
               r(chi2) =  21.64010539408598
                  r(p) =  3.28902266933e-06
\end{verbatim}
D'où pour le "petit $p$", 
\begin{verbatim}
. display %10.9f r(p)
0.000003289
\end{verbatim}
La commande ci-dessus demande à Stata d'afficher le résultat sous forme d'un
nombre avec 9 décimales.

Concernant les effectifs théoriques, on utilise exactement la même commande
que précédemment mais en ajoutant l'option \texttt{expected}.
\begin{verbatim}
. tabulate traitement infection [fweight=N], chi expected nofreq

           |       infection
traitement |       Non        Oui |     Total
-----------+----------------------+----------
         A |     133.8       75.2 |     209.0 
         B |     142.2       79.8 |     222.0 
-----------+----------------------+----------
     Total |     276.0      155.0 |     431.0 

          Pearson chi2(1) =  21.6401   Pr = 0.000
\end{verbatim}
Notons que l'on a supprimé l'affichage des effectifs observés grâce à
l'option \texttt{nofreq}.

% FIXME:
% tabodds infection traitement [fw=N], or
Pour calculer la valeur de l'odds-ratio, on utilisera la commande
\texttt{cc}. Toutefois, les commandes "epitab" nécessitent que les variables
soient codées sous forme binaire (0 = non-exposé/non-malade, 1 =
exposé/malade). La saisie des données effectuée à l'étape précédente n'étant
pas compatible avec ce format, il est nécessaire de recoder les données
comme dans l'exercice~\ref{exo:3.5stata}. Par exemple, en utilisant la
commande \texttt{input} :
\begin{verbatim}
. input traitement infection N

     traitem~t  infection          N
  1. 1 0 157
  2. 0 0 119
  3. 1 1 52
  4. 0 1 103
  5. end
. label define tx 0 "Placebo" 1 "Traitement"
. label values traitement tx
. label define ouinon 0 "Non" 1 "Oui"
. label values infection ouinon
\end{verbatim}
Ensuite, il est possible de procéder à l'estimation de l'odds-ratio
\begin{verbatim}
. cc infection traitement [fweight=N], woolf exact

                 | traitement             |             Proportion
                 |   Exposed   Unexposed  |      Total     Exposed
-----------------+------------------------+------------------------
           Cases |        52         103  |        155       0.3355
        Controls |       157         119  |        276       0.5688
-----------------+------------------------+------------------------
           Total |       209         222  |        431       0.4849
                 |                        |
                 |      Point estimate    |    [95% Conf. Interval]
                 |------------------------+------------------------
      Odds ratio |         .3826603       |    .2540087    .5764721 (Woolf)
 Prev. frac. ex. |         .6173397       |    .4235279    .7459913 (Woolf)
 Prev. frac. pop |         .3511679       |
                 +-------------------------------------------------
                                  1-sided Fisher's exact P = 0.0000
                                  2-sided Fisher's exact P = 0.0000
\end{verbatim}
On remarquera que cette fois-ci on a précisé l'option \texttt{exact} pour
obtenir un test de Fisher au lieu de l'approximation par la loi du $\chi^2$
pour le test d'hypothèse sur le tableau.

Si l'on tient compte des données par centre, il est nécessaire de
reconstruire les tableaux d'effectifs, en ne considérant que les marges
colonnes des tableaux d'effectifs donnés dans l'énoncé. La saisie des
données avec \texttt{input} ne pose pas de difficultés particulières.
\begin{verbatim}
. input infection centre N

     infection     centre          N
  1. 0 1 98
  2. 1 1 27
  3. 0 2 152
  4. 1 2 106
  5. 0 3 26
  6. 1 3 22
  7. end
. tabulate infection centre [fweight=N], chi

           |              centre
 infection |         1          2          3 |     Total
-----------+---------------------------------+----------
         0 |        98        152         26 |       276 
         1 |        27        106         22 |       155 
-----------+---------------------------------+----------
     Total |       125        258         48 |       431 

          Pearson chi2(2) =  16.1673   Pr = 0.000
\end{verbatim}
Une fois de plus on adopte le format rapide de saisie des données aggrégées
: 2 variables (épisodes infectieux oui/non, \no\ de centre) et les effectifs
associés au croisement de chacun des niveaux de ces variables. L'option
\texttt{fweight} permet ensuite d'appliquer un test du $\chi^2$ via la
commande \texttt{tabulate} en pondérant le tableau à deux entrées par les
effectifs \texttt{N}. 

Pour réaliser un test de Mantel-Haenszel, il est nécessaire de revenir à
l'ensemble des données (3 tableaux $2\times 2$) et d'utiliser la commande
\texttt{cc} en précisant le facteur de stratification grâce à l'option
\texttt{by}. Voici une manière de procéder, en considérant trois variables
\texttt{tx} (traitement A (1) ou B (0)), \texttt{inf} (infection non (0)/oui (1)) et
\texttt{cen} (centre, 1 à 3). Attention à bien coder les classes des deux
variables de classification en 0 et 1.
\begin{verbatim}
. input tx inf cen N

            tx        inf        cen          N
  1. 1 0 1 51
  2. 1 1 1 8
  3. 0 0 1 47
  4. 0 1 1 19
--%<----
 13. end
\end{verbatim}
On peut vérifier la structure des données à l'aide de la commande
\texttt{table}, en suivant exactement le même principe pour les options
(variables de classification, facteur de pondération et variable de
stratification). On en profitera pour ajouter des étiquettes plus
informatives aux modalités des variables de classification.
\begin{verbatim}
. label define txlab 0 "A" 1 "B"
. label define inflab 0 "Non" 1 "Oui"
. label values tx txlab
. label value inf inflab
. table tx inf [fw=N], by(cen)

----------------------
cen and   |    inf    
tx        |  Non   Oui
----------+-----------
1         |
        A |    8    51
        B |   19    47
----------+-----------
2         |
        A |   35    91
        B |   71    61
----------+-----------
3         |
        A |    9    15
        B |   13    11
----------------------
\end{verbatim}
Concernant le test de Mantel-Haenszel, on obtient les résultats suivants :
\begin{verbatim}
. cc tx inf [freq=N], by(cen)

             cen |       OR       [95% Conf. Interval]   M-H Weight
-----------------+-------------------------------------------------
               1 |   .3880289      .1346123    1.04317        7.752 (exact)
               2 |   .3304442      .1899288   .5726148     25.04264 (exact)
               3 |   .5076923      .1370954   1.857073       4.0625 (exact)
-----------------+-------------------------------------------------
           Crude |   .3826603      .2483246   .5877322              (exact)
    M-H combined |   .3620925      .2379264   .5510569              
-------------------------------------------------------------------
Test of homogeneity (M-H)      chi2(2) =     0.47  Pr>chi2 = 0.7898

                   Test that combined OR = 1:
                                Mantel-Haenszel chi2(1) =     23.01
                                                Pr>chi2 =    0.0000
\end{verbatim}
%
%
%
\soln{\ref{exo:6.3}}
Pour le chargement des données qui se présentent sous la forme de données
tabulées dans un fichier texte sans ligne d'en-tête, on utilisera la
commande \texttt{infile} en spécificiant le nom des variables. On notera que
Stata indique le nombre de lignes ("observations") lues, ce qui permet de
d'assurer de l'intégrité des données si l'on connaît le nombre
d'observations à l'avance.
\begin{verbatim}
. infile ck pres abs using sck.dat
(13 observations read)
\end{verbatim}

Le nombre total de sujets correspond à la somme des valeurs dans les
variables \texttt{pres} et \texttt{abs}. Le plus simple est donc de faire la
somme de l'ensemble de ces valeurs pour avoir l'effectif total :
\begin{verbatim}
. generate tot = pres+abs
. egen ntot = sum(tot)
. display ntot
360
\end{verbatim}

%% Concernant la représentation des fréquences relatives des variables
%% \texttt{pres} et \texttt(abs}, il est nécessaire de \og construire\fg\ deux
%% nouvelles variables. Voici une solution possible :
%% \begin{verbatim}
%% . quietly: summarize pres
%% . scalar npres = r(sum)
%% . gen ppres = pres / npres
%% \end{verbatim}

Il existe plusieurs commandes pour construire un modèle de régression
logistique sous Stata. Dans le cas des données dites "groupées", on peut
utiliser la commande \texttt{blogit} qui nécessite de connaître les
effectifs pour chacune des deux classes de la variable binaire à prédire, ou
plus exactement les effectifs de la classe "positive" et les effectifs
totaux. Bien qu'on présente son usage ici, on verra qu'il est préférable
d'utiliser un autre type de commande par la suite.
\footnote{\url{http://www.stata.com/support/faqs/statistics/logistic-regression-with-grouped-data/}}
\begin{verbatim}
. blogit pres tot ck

Logistic regression for grouped data              Number of obs   =        360
                                                  LR chi2(1)      =     283.15
                                                  Prob > chi2     =     0.0000
Log likelihood = -93.886407                       Pseudo R2       =     0.6013

------------------------------------------------------------------------------
    _outcome |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          ck |   .0351044   .0040812     8.60   0.000     .0271053    .0431035
       _cons |  -2.326272   .2993611    -7.77   0.000    -2.913009   -1.739535
------------------------------------------------------------------------------
\end{verbatim}
La lecture de ce type de sortie ne présente pas de problème particulier : on
tiendra compte du fait que le coefficient de régression associé à la
variable d'étude \texttt{ck} est exprimé sur une échelle log odds ; pour
obtenir la valeur de l'odds-ratio associé, il est donc nécessaire de prendre
l'exponentielle de la valeur retournée (.0351044), par exemple
\begin{verbatim}
. display exp(_b[ck])
1.0357278
\end{verbatim}

Pour afficher sur un même graphique les proportions empiriques (malades) et
les valeurs prédites par le modèle (sous forme d'une ligne brisée), il est
nécessaire de calculer ces deux quantités.
\begin{verbatim}
. gen prop = pres/tot
. predict pred, p
. label variable prop "observed"
. label variable pred "predicted"
. graph twoway (line pred ck) (scatter prop ck), ytitle("Probabilité")
\end{verbatim}

\includegraphics{./figs/stata_sck}

La commande \texttt{predict} comprend le nom dela variable (\texttt{pred}
dans laquelle on souhaite stocker les résultats, et le type de prédictions
que l'on souhaite réaliser (\texttt{p}, pour probabilités). Un diagramme de
dispersion composé de deux séries d'instructions délimitées par des
parenthèses est ensuite généré à l'aide de la commande \texttt{twoway}.

La commande \texttt{blogit} permet de travailler avec des données
groupées. Il existe une autre manière de construire des modèles de
régression logistique, pour données individuelles ou groupées. En
particulier, les comamndes \texttt{logit} et \texttt{logistic} fournissent
des résultats additionnels. Dans la suite, on va transformer le jeu de
données initial (données groupées) en données individuelles, puis ré-estimer
les paramètres du même modèle de régression logistique. Les commandes
suivantes permettent de générer autant de lignes qu'il y a d'unités
statistiques (nombre d'observations tel que calculé et stocké dans
\texttt{ntot}) ainsi qu'une variable binaire, \texttt{infct}, codant pour
l'observation de l'événement (malade/non-malade).
\begin{verbatim}
. expand tot
(347 observations created)
. bysort ck: gen infct = _n <= pres
. logit infct ck, nolog

Logistic regression                               Number of obs   =        360
                                                  LR chi2(1)      =     283.15
                                                  Prob > chi2     =     0.0000
Log likelihood = -93.886407                       Pseudo R2       =     0.6013

------------------------------------------------------------------------------
       infct |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          ck |   .0351044   .0040812     8.60   0.000     .0271053    .0431035
       _cons |  -2.326272   .2993611    -7.77   0.000    -2.913009   -1.739535
------------------------------------------------------------------------------
\end{verbatim}

Parmi les commandes de post-estimation disponible, la commande
\texttt{classification} fournit automatiquement un tableau de synthèse des
unités correctement ou incorrectement classées selon le critère choisi (par
défaut, $P(\texttt{malade})>0.5$).
\begin{verbatim}
. estat classification

Logistic model for infct

              -------- True --------
Classified |         D            ~D  |      Total
-----------+--------------------------+-----------
     +     |       215            16  |        231
     -     |        15           114  |        129
-----------+--------------------------+-----------
   Total   |       230           130  |        360

Classified + if predicted Pr(D) >= .5
True D defined as infct != 0
--------------------------------------------------
Sensitivity                     Pr( +| D)   93.48%
Specificity                     Pr( -|~D)   87.69%
Positive predictive value       Pr( D| +)   93.07%
Negative predictive value       Pr(~D| -)   88.37%
--------------------------------------------------
False + rate for true ~D        Pr( +|~D)   12.31%
False - rate for true D         Pr( -| D)    6.52%
False + rate for classified +   Pr(~D| +)    6.93%
False - rate for classified -   Pr( D| -)   11.63%
--------------------------------------------------
Correctly classified                        91.39%
--------------------------------------------------
\end{verbatim}

\begin{verbatim}
. estat classification, cutoff(.15)

Logistic model for infct

              -------- True --------
Classified |         D            ~D  |      Total
-----------+--------------------------+-----------
     +     |       228            42  |        270
     -     |         2            88  |         90
-----------+--------------------------+-----------
   Total   |       230           130  |        360

Classified + if predicted Pr(D) >= .15
True D defined as infct != 0
--------------------------------------------------
Sensitivity                     Pr( +| D)   99.13%
Specificity                     Pr( -|~D)   67.69%
Positive predictive value       Pr( D| +)   84.44%
Negative predictive value       Pr(~D| -)   97.78%
--------------------------------------------------
False + rate for true ~D        Pr( +|~D)   32.31%
False - rate for true D         Pr( -| D)    0.87%
False + rate for classified +   Pr(~D| +)   15.56%
False - rate for classified -   Pr( D| -)    2.22%
--------------------------------------------------
Correctly classified                        87.78%
--------------------------------------------------
\end{verbatim}

ROC curve
\begin{verbatim}
. lroc

Logistic model for infct

number of observations =      360
area under ROC curve   =   0.9593
\end{verbatim}

\includegraphics{./figs/stata_sck2}

Sensibilité/spécificité
\begin{verbatim}
. lsens
\end{verbatim}

\includegraphics{./figs/stata_sck3}

% 
%
%
\soln{\ref{exo:6.5}}
Les données ont été sauvegardées dans un format compact (3 colonnes
indiquant la présence ou non d'un cancer, le niveau de consommation
d'alcool, et les effectifs associés).
\begin{verbatim}
. insheet using "cc_oesophage.csv", clear
. label define yesno 0 "No" 1 "Yes" 
. label values cancer yesno 
. label define dose 0 "< 80g" 1 ">= 80g"
. label values alcohol dose
. list

     +-----------------------------+
     | cancer   alcohol   patients |
     |-----------------------------|
  1. |     No     < 80g        666 |
  2. |    Yes     < 80g        104 |
  3. |     No    >= 80g        109 |
  4. |    Yes    >= 80g         96 |
     +-----------------------------+
\end{verbatim}
Les commandes précédentes permettent de charger le fichier de données et
d'associer aux modalités des variables (\texttt{cancer} et \texttt{alcohol})
des noms plus informatifs. Pour obtenir le nombre total de patients, on peut
utiliser la commande suivante :
\begin{verbatim}
. egen ntot = sum(patients)
. display ntot
975
\end{verbatim}

La proportion d'individus à risque, c'est-à-dire ayant une consommation
journalière d'alcool $\ge 80$ g s'obtient à partir d'un simple tableau
d'effectifs croisant les variables \texttt{cancer} et \texttt{alcohol} (il
faut indiquer comment remplir les cellules en ajoutant une option
\texttt{weight}), et en demandant les profils lignes, c'est-à-dire les les
fréquences relatives par ligne.
\begin{verbatim}
. tabulate cancer alcohol [fweight=patients], row

+----------------+
| Key            |
|----------------|
|   frequency    |
| row percentage |
+----------------+

           |        alcohol
    cancer |     < 80g     >= 80g |     Total
-----------+----------------------+----------
        No |       666        109 |       775 
           |     85.94      14.06 |    100.00 
-----------+----------------------+----------
       Yes |       104         96 |       200 
           |     52.00      48.00 |    100.00 
-----------+----------------------+----------
     Total |       770        205 |       975 
           |     78.97      21.03 |    100.00
\end{verbatim}

Le calcul de l'odds-ratio peut se faire à l'aide de l'une des commandes
Stata dites "immédiates", \texttt{cc}, en gardant à l'esprit qu'il faut bien
tenir compte de la colonne \texttt{patients}, comme précédemment :
\begin{verbatim}
. cc cancer alcohol [fweight=patients], woolf

                 | alcohol                |             Proportion
                 |   Exposed   Unexposed  |      Total     Exposed
-----------------+------------------------+------------------------
           Cases |        96         104  |        200       0.4800
        Controls |       109         666  |        775       0.1406
-----------------+------------------------+------------------------
           Total |       205         770  |        975       0.2103
                 |                        |
                 |      Point estimate    |    [95% Conf. Interval]
                 |------------------------+------------------------
      Odds ratio |         5.640085       |    4.000589    7.951467 (Woolf)
 Attr. frac. ex. |         .8226977       |    .7500368    .8742371 (Woolf)
 Attr. frac. pop |         .3948949       |
                 +-------------------------------------------------
                               chi2(1) =   110.26  Pr>chi2 = 0.0000
\end{verbatim}

Si l'on dispose du tableau d'effectifs, c'est-à-dire la répartition des 975
sujets dans les quatre cellules du tableau croisant l'exposition à l'alcool
et le statut cas-témoin, il est également possible d'utiliser le calculateur
pour odds-ratio dans les études cas-témoins accessible par le menu
\textsf{Statistics} $\rhd$ \textsf{Epidemiology and related} $\rhd$
\textsf{Tables for epidemiologists}.

\includegraphics{./figs/stata_ORcalculator}

Pour tester l'hypothèse que la proportion de personnes avec une consommation
journalière d'alcool $\ge 80$ g est identique chez les cas ($p_1$) et les
témoins ($p_1$), on peut procéder comme suit :
\begin{verbatim}
. prtesti 96 0.4800 109 0.1406

Two-sample test of proportions                     x: Number of obs =       96
                                                   y: Number of obs =      109
------------------------------------------------------------------------------
    Variable |       Mean   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |        .48   .0509902                      .3800611    .5799389
           y |      .1406   .0332949                      .0753433    .2058567
-------------+----------------------------------------------------------------
        diff |      .3394   .0608978                      .2200424    .4587576
             |  under Ho:   .0641131     5.29   0.000
------------------------------------------------------------------------------
        diff = prop(x) - prop(y)                                  z =   5.2938
    Ho: diff = 0

    Ha: diff < 0                 Ha: diff != 0                 Ha: diff > 0
 Pr(Z < z) = 1.0000         Pr(|Z| < |z|) = 0.0000          Pr(Z > z) = 0.0000
\end{verbatim}

Une autre façon de tester cette hypothèse consiste à remarquer que
l'hypothèse précédente, $H_0:\, \pi_0=\pi_1$, n'est vraie que si
l'odds-ratio vaut 1. D'où l'idée d'exploiter directement le test du $\chi^2$
pour l'odds-ratio donné par la commande \texttt{cc} (ici, $\chi^2=110.26$,
$p<0.001$). 

Le modèle de régression logistique, comme les autres modèles de régression
sous Stata, se formule ainsi : 

\begin{verbatim}
. logistic alcohol cancer [freq=patients]

Logistic regression                               Number of obs   =        975
                                                  LR chi2(1)      =      96.43
                                                  Prob > chi2     =     0.0000
Log likelihood =  -453.2224                       Pseudo R2       =     0.0962

------------------------------------------------------------------------------
     alcohol | Odds Ratio   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      cancer |   5.640085   .9883492     9.87   0.000     4.000589    7.951467
       _cons |   .1636637   .0169104   -17.52   0.000     .1336604    .2004018
------------------------------------------------------------------------------
\end{verbatim}

Par défaut, Stata présente les résultats (coefficients du modèle) sous forme
d'odds-ratio, avec leurs intervalles de confiance associés. Si l'on souhaite
obtenir directement les coefficients de régression (sur l'échelle du log
odds), il faut utiliser la commande \texttt{logit} après avoir utilisé
\texttt{logistic}. On pourrait également utiliser directement une commande de
type \verb|logistic alcohol cancer [freq=patients]|.
\begin{verbatim}
. logit

Logistic regression                               Number of obs   =        975
                                                  LR chi2(1)      =      96.43
                                                  Prob > chi2     =     0.0000
Log likelihood =  -453.2224                       Pseudo R2       =     0.0962

------------------------------------------------------------------------------
     alcohol |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      cancer |   1.729899   .1752366     9.87   0.000     1.386442    2.073356
       _cons |  -1.809942   .1033238   -17.52   0.000    -2.012453   -1.607431
------------------------------------------------------------------------------
\end{verbatim}

\chapter*{Devoir \no 8}
\addcontentsline{toc}{chapter}{Devoir \no 8}

\chapter{Analyse de données de survie}

Dans cette partie sont proposés les corrigés des exercices 7.1, 7.2 et 7.3
dont les énoncés figurent dans le chapitre 7 (p.~\pageref{chap:survival}).

\section*{Corrigés}

\soln{\ref{exo:7.1}}
Le fichier de données est un fichier texte avec des tabulations comme
séparateur de champ. On peut l'importer sous Stata en utilisant la commande
\texttt{insheet}. Pour afficher le nom des variables après importation, il
suffit d'utiliser \texttt{describe} avec l'option \texttt{simple}.
\begin{verbatim}
. insheet using "pbc.txt", tab
(28 vars, 312 obs)
. describe, simple
number    rx        asictes   spiders   bilirub   albumin   alkphos   trigli
prothrom  age       sample    logalbu   _st       _t
status    sex       hepatom   edema     cholest   copper    sgot      platel
histol    years     logbili   logprot   _d        _t0
\end{verbatim}
Après recodage des étiquettes des variables \texttt{rx} et \texttt{sex}, 
\begin{verbatim}
. label define trt 1 "Placebo" 2 "DPCA"
. label define sexe 0 "M" 1 "F"
. label values rx trt
. label values sex sexe
\end{verbatim}
on peut vérifier la proportion de patients décédés (\texttt{status}, 0 =
vivant et 1 = décédé) et leur répartition selon le groupe de traitement à
l'aide de tris simple et croisé. Pour le tri croisé, on ajoutera l'option
\texttt{row} pour obtenir les fréquences relatives par status.
\begin{verbatim}
. tabulate status

     status |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |        187       59.94       59.94
          1 |        125       40.06      100.00
------------+-----------------------------------
      Total |        312      100.00

. tabulate status rx, row

+----------------+
| Key            |
|----------------|
|   frequency    |
| row percentage |
+----------------+

           |          rx
    status |   Placebo       DPCA |     Total
-----------+----------------------+----------
         0 |        93         94 |       187 
           |     49.73      50.27 |    100.00 
-----------+----------------------+----------
         1 |        65         60 |       125 
           |     52.00      48.00 |    100.00 
-----------+----------------------+----------
     Total |       158        154 |       312 
           |     50.64      49.36 |    100.00
\end{verbatim}

Pour afficher la distribution des temps de suivi, on utilisera un simple
diagramme de dispersion comme on l'a vu dans le cas de R. Pour faire
appraître distinctement les observations selon le status (0 ou 1), on
pourrait très bien superposer deux séries de points sur le même graphique
(cf. exercice~5.2, p.~\pageref{para:twoway}). Voici une autre manière de
procéder :
\begin{verbatim}
. separate number, by(status)

              storage  display     value
variable name   type   format      label      variable label
-----------------------------------------------------------------------------------------
number0         int    %8.0g                  number, status == 0
number1         int    %8.0g                  number, status == 1

. twoway scatter number0 number1 years, msymbol(S O)
\end{verbatim}
La première commande permet en fait de séparer les numéros de patients en
fonction du status afin d'afficher les deux séries d'observation en fonction
de la durée de suivi en années.

\includegraphics{./figs/stata_followup}

La médiane de la durée de suivi par groupe de traitement peut s'obtenir avec
la commande \texttt{tabstat} en opérant par groupe grâce à l'option
\texttt{by}. 
\begin{verbatim}
. tabstat years, by(rx) stats(median) nototal

Summary for variables: years
     by categories of: rx 

     rx |       p50
--------+----------
Placebo |    5.1882
   DPCA |   4.95825
-------------------
\end{verbatim}

Le nombre de décès enregistrés au-delà de 10.5 années de suivi s'obtient
avec un simple tri à plat par la commande \texttt{tabulate} :
\begin{verbatim}
. tabulate status if years > 10.49

     status |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |         23       85.19       85.19
          1 |          4       14.81      100.00
------------+-----------------------------------
      Total |         27      100.00
\end{verbatim}
de même que le sexe des patients décédé après cette période :
\begin{verbatim}
. tabulate sex if years > 10.49 & status == 1

        sex |      Freq.     Percent        Cum.
------------+-----------------------------------
          M |          2       50.00       50.00
          F |          2       50.00      100.00
------------+-----------------------------------
      Total |          4      100.00
\end{verbatim}

Concernant l'analyse des patients transplantés, on peut également procéder
comme on l'a fait avec R, c'est-à-dire restreindre le tableau de données à
ces seuls patients. Comme Stata ne permet de travailler qu'avec un seul
tableau de données à la fois, il est toutefois nécessaire de sauvegarder
temporairement les données actuelles avant de créer un nouveau tableau.
\begin{verbatim}
. preserve
. egen idx = anymatch(number), values(5 105 111 120 125 158 183 241 246 247 254 263 264
  265 274 288 291 295 297 345 361 362 375 380 383)
. keep if idx
(293 observations deleted)
. gen days = years*365
. tabstat age sex days, stats(mean median sum)

   stats |       age       sex      days
---------+------------------------------
    mean |  41.17568  .8421053  1507.177
     p50 |   40.9008         1  1434.012
     sum |  782.3379        16  28636.37
----------------------------------------
\end{verbatim}
La première commande permet de construire une liste des individus que l'on
souhaite utiliser pour filtrer le tableau de données d'origine (sur la base
des numéros de sujet contenus dans la variable \texttt{number}). Ensuite, on
applique le calcul des statistiques descriptives à l'aide d'une commande
\texttt{tabstat}. Une fois les calculs terminés, on peut restorer les
données d'origine de la manière suivante :
\begin{verbatim}
. restore
\end{verbatim}

Comme R, Stata utilise ses propres conventions pour le codage des données de
survie. Les commandes essentielles sont ainsi : \texttt{stset} pour définir
la façon dont les événements sont enregistrés et le temps d'observation,
\texttt{sts} pour calculer un tableau de survie à partir de l'estimateur de
Kaplan-Meier. Voici comment appliquer ces commandes pour construire le
tableau et la courbe de survie, sans considération du facteur traitement.
\begin{verbatim}
. stset years, failure(status)

     failure event:  status != 0 & status < .
obs. time interval:  (0, years]
 exit on or before:  failure

------------------------------------------------------------------------------
      312  total obs.
        0  exclusions
------------------------------------------------------------------------------
      312  obs. remaining, representing
      125  failures in single record/single failure data
 1713.854  total analysis time at risk, at risk from t =         0
                             earliest observed entry t =         0
                                  last observed exit t =   12.4736

. sts list
\end{verbatim}
La deuxième commande affiche le tableau demandé. Pour la courbe de survie,
on utilisera :
\begin{verbatim}
. sts graph, ci censored(single)
\end{verbatim}
L'option \texttt{ci} permet d'afficher l'intervalle de confiance à 95~\%
pour l'estimateur de KM.

\includegraphics{./figs/stata_stsgraph}

%
%
%
\soln{\ref{exo:7.2}}
Le format texte du fichier de données \texttt{prostate.dat} est identique à
celui du fichier \texttt{pbc.txt} de l'exercice précédent, à ceci près que
les champs sont séparés par un simple espace. On utilisera donc la commande
\texttt{insheet} :
\begin{verbatim}
. insheet using "prostate.dat", delimiter(" ")
(7 vars, 38 obs)
. list in 1/5

     +--------------------------------------------------------+
     | treatm~t   time   status   age   haem   size   gleason |
     |--------------------------------------------------------|
  1. |        1     65        0    67   13.4     34         8 |
  2. |        2     61        0    60   14.6      4        10 |
  3. |        2     60        0    77   15.6      3         8 |
  4. |        1     58        0    64   16.2      6         9 |
  5. |        2     51        0    65   14.1     21         9 |
     +--------------------------------------------------------+
\end{verbatim}
Le nombre de patients vivants à la date de point est obtenu à partir de
\texttt{tabulate} :
\begin{verbatim}
. tabulate status

     Status |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |         32       84.21       84.21
          1 |          6       15.79      100.00
------------+-----------------------------------
      Total |         38      100.00
\end{verbatim}
soit 32 personnes encore en vie à la fin de la durée de suivi.

Comme dans l'exercice~7.1, il est nécessaire d'indiquer à Stata quelle
variable sert à identifier les événements (\texttt{status}) et le temps
(\texttt{time}). On utilisera la commande \texttt{stset} de la manière
suivante :
\begin{verbatim}
. stset time, failure(status)

     failure event:  status != 0 & status < .
obs. time interval:  (0, time]
 exit on or before:  failure

------------------------------------------------------------------------------
       38  total obs.
        0  exclusions
------------------------------------------------------------------------------
       38  obs. remaining, representing
        6  failures in single record/single failure data
     1890  total analysis time at risk, at risk from t =         0
                             earliest observed entry t =         0
                                  last observed exit t =        70
\end{verbatim}

La médiane de survie selon le traitement est disponible à partir de la
commande \texttt{stci}, en précisant le percentile d'intérêt (ici,
\texttt{p(50)}). 
\begin{verbatim}
. stci, by(treatment) p(50)

         failure _d:  status
   analysis time _t:  time

             |    no. of 
treatment    |  subjects         50%     Std. Err.     [95% Conf. Interval]
-------------+-------------------------------------------------------------
           1 |        18          69      .3043484           42          .
           2 |        20           .             .            .          .
-------------+-------------------------------------------------------------
       total |        38           .             .           69          .
\end{verbatim}

Pour afficher les courbes de survie pour chacun des bras de traitement, on
utilisera la commande \texttt{sts graph} en spécifiant le facteur de
classification à l'aide de l'option \texttt{by}. L'affichage des données
censurées se fait à l'aide de l'option \texttt{censored}.
\begin{verbatim}
. sts graph, by(treatment) censored(s)
\end{verbatim}

\includegraphics{./figs/stata_ststwoway}

Le test du log-rank s'effectue avec la commande \texttt{sts test}. Notons
que l'on n'a besoin de spécifier que le facteur traitement.
\begin{verbatim}
. sts test treatment

         failure _d:  status
   analysis time _t:  time


Log-rank test for equality of survivor functions

          |   Events         Events
treatment |  observed       expected
----------+-------------------------
1         |         5           2.47
2         |         1           3.53
----------+-------------------------
Total     |         6           6.00

                chi2(1) =       4.42
                Pr>chi2 =     0.0355
\end{verbatim}
%
%
%
\soln{\ref{exo:7.3}}
L'importation des données ne pose pas de problème particulier puisque
celles-ci sont déjà stockées au format Stata.
\begin{verbatim}
. use hip, clear
. describe

Contains data from hip.dta
  obs:           106                          hip fracture study
 vars:             7                          30 Jan 2009 11:58
 size:         1,060                          
-----------------------------------------------------------------------------------------
              storage  display     value
variable name   type   format      label      variable label
-----------------------------------------------------------------------------------------
id              byte   %4.0g                  patient ID
time0           byte   %5.0g                  begin of span
time1           byte   %5.0g                  end of span
fracture        byte   %8.0g                  fracture event
protect         byte   %8.0g                  wears device
age             byte   %4.0g                  age at enrollment
calcium         float  %8.0g                  blood calcium level
-----------------------------------------------------------------------------------------
Sorted by:
\end{verbatim}
On peut vérifier les données de quelques individus.
\begin{verbatim}
. sort id time0
. list if id == 1

     +---------------------------------------------------------+
     | id   time0   time1   fracture   protect   age   calcium |
     |---------------------------------------------------------|
  1. |  1       0       1          1         0    76      9.35 |
     +---------------------------------------------------------+
. list if id == 15

     +---------------------------------------------------------+
     | id   time0   time1   fracture   protect   age   calcium |
     |---------------------------------------------------------|
 21. | 15       0       5          0         0    64     11.67 |
 22. | 15       5      12          1         .     .      11.4 |
     +---------------------------------------------------------+
. list if id == 48

     +---------------------------------------------------------+
     | id   time0   time1   fracture   protect   age   calcium |
     |---------------------------------------------------------|
104. | 48       0       5          0         1    67     11.21 |
105. | 48       5      15          0         .     .     11.43 |
106. | 48      15      39          0         .     .     11.29 |
     +---------------------------------------------------------+
\end{verbatim}
ce qui permet de voir que pour les variables \texttt{protect} et
\texttt{age} les données ne sont pas répétées après la première occasion, ce
qui évidemment géénère un nombre important de valeurs manquantes pour ces variables.
\begin{verbatim}
. summarize

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
          id |       106    28.21698    13.09599          1         48
       time0 |       106    4.792453    5.631065          0         15
       time1 |       106     11.5283    8.481024          1         39
    fracture |       106    .2924528    .4570502          0          1
     protect |        48    .5833333    .4982238          0          1
-------------+--------------------------------------------------------
         age |        48      70.875    5.659205         62         82
     calcium |       106    10.10849    1.407355       7.25      12.32
\end{verbatim}

Pour le codage des données brutes en données de survie, on utilisera la
commande \texttt{stset} de la manière suivante :
\begin{verbatim}
. stset time1, id(id) time0(time0) failure(fracture)

                id:  id
     failure event:  fracture != 0 & fracture < .
obs. time interval:  (time0, time1]
 exit on or before:  failure

------------------------------------------------------------------------------
      106  total obs.
        0  exclusions
------------------------------------------------------------------------------
      106  obs. remaining, representing
       48  subjects
       31  failures in single failure-per-subject data
      714  total analysis time at risk, at risk from t =         0
                             earliest observed entry t =         0
                                  last observed exit t =        39
\end{verbatim}

\begin{verbatim}
. stdescribe

         failure _d:  fracture
   analysis time _t:  time1
                 id:  id

                                   |-------------- per subject --------------|
Category                   total        mean         min     median        max
------------------------------------------------------------------------------
no. of subjects               48   
no. of records               106    2.208333           1          2          3

(first) entry time                         0           0          0          0
(final) exit time                       15.5           1       12.5         39

subjects with gap              3   
time on gap if gap            30          10          10         10         10
time at risk                 714      14.875           1       11.5         39

failures                      31    .6458333           0          1          1
------------------------------------------------------------------------------
\end{verbatim}

\begin{verbatim}
. stvary

         failure _d:  fracture
   analysis time _t:  time1
                 id:  id

               subjects for whom the variable is
                                                 never    always   sometimes
    variable |  constant    varying             missing   missing   missing
-------------+--------------------------------------------------------------
     protect |        48          0                   8         0        40
         age |        48          0                   8         0        40
     calcium |         8         40                  48         0         0

. streset, past future
-> stset time1, id(id) failure(fracture) time0(time0)
-> streset, full past future

                id:  id
     failure event:  fracture != 0 & fracture < .
obs. time interval:  (time0, time1]
 exit on or before:  time .
    t for analysis:  (time-origin)
            origin:  min
              ever:  _insmpl

------------------------------------------------------------------------------
      106  total obs.
        0  exclusions
------------------------------------------------------------------------------
      106  obs. remaining, representing
       48  subjects
       31  failures in single failure-per-subject data
      714  total analysis time at risk, at risk from t =         0
                             earliest observed entry t =         0
                                  last observed exit t =        39

. stfill age protect, forward

         failure _d:  fracture
   analysis time _t:  (time1-origin)
             origin:  min
  exit on or before:  time .
                 id:  id

replace missing values with previously observed values:
             age:  58 real changes made
         protect:  58 real changes made

. streset
\end{verbatim}


\end{document}

