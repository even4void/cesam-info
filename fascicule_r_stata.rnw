\documentclass[11pt]{report}
\usepackage[francais]{babel}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{fancyhdr}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{url}
\usepackage{calc}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{parskip}
\usepackage{hyperref,wasysym}
\usepackage{multirow}
\usepackage{dcolumn}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{/}{D{/}{/}{-1}}
\usepackage{xspace}
\usepackage{wasysym}
\usepackage{enumitem}  % iterable item list
\usepackage{fancyvrb}

\usepackage[noae,nogin]{Sweave}
\SweaveOpts{prefix.string=figs,strip.white=all,keep.source=TRUE}

% bibliography stuff
\usepackage[style=verbose-trad2,natbib=true,backend=bibtex]{biblatex}
\bibliography{refs}

% Xelatex setup
\usepackage{fontspec,xltxtra,xunicode}
%\usepackage[math-style=ISO,bold-style=ISO]{unicode-math}
\usepackage[vargreek-shape=unicode]{unicode-math} %controls which shape of
                                        %epsilon and phi are considered default
\unimathsetup{math-style=french} % uses so called french style italic lc
                                 % latin, upright everything else
\setmathfont{Latin Modern Math} % this is obligatory or no math font will be
                                % set...
\defaultfontfeatures{Scale=MatchLowercase}
\setmainfont{Myriad Pro}
%\setsansfont{Myriad Pro}
%\setmathfont[Numbers=OldStyle]{Asana Math}
%\setmonofont[Scale=0.86]{Inconsolata}
\setmonofont[Scale=0.71]{Menlo}

% page layout
\usepackage[left=2cm,bottom=2cm,right=2cm]{geometry}

\usepackage[french=guillemets*]{csquotes} 
\MakeOuterQuote{"} 

% we now need the following two import statements with TeXLive 2012 (updated
% Sep. 2012) -- don't know why.
\usepackage{xcolor}
\usepackage{textcomp}
% custom color for R code chunks
\definecolor{Sinput}{rgb}{0.3,0.3,0.3}
\definecolor{Soutput}{rgb}{0.5,0.5,0.5}
\definecolor{Scode}{rgb}{0,0.42,0.33}

% add line break when using paragraph sectioning
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
  {-3.25ex\@plus -1ex \@minus -.2ex}%
  {1.5ex \@plus .2ex}%
  {\normalfont\normalsize\bfseries}}

\renewcommand\part{%
  \if@openright
  \cleardoublepage
  \else
  \clearpage
  \fi
  \thispagestyle{empty}%   % Original »plain« replaced by »emptyx
  \if@twocolumn
  \onecolumn
  \@tempswatrue
  \else
  \@tempswafalse
  \fi
  \null\vfil
  \secdef\@part\@spart}
\makeatother

\setlength{\headheight}{0.3in}
\setlength{\headwidth}{\textwidth}

\addto\captionsfrancais{%
  \renewcommand\chaptername{Semaine}}
\titleformat{\chapter}
  {\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter.}{20pt}{\huge}

\setlist[enumerate]{label=\fbox{\textbf{\arabic{chapter}.\arabic*}}}

% Manage solutions
\usepackage{answers}   %[nosolutionfiles]
\Newassociation{sol}{Solution}{solutions}
\renewcommand{\Solutionlabel}[1]{\fbox{#1}}
\newcommand{\soln}[1]{\vskip1ex\par\noindent\fbox{#1}\quad}

\theoremstyle{definition}
\newtheorem{exo}{Exercice}[chapter]
\newcommand{\R}{\textsf{R}\xspace}
\newcommand{\Stata}{\textsf{Stata}\xspace}
\newcommand{\SAS}{\textsf{SAS}\xspace}
\SetLabelAlign{parright}{\parbox[t]{\labelwidth}{\raggedleft#1}}

\setlist[description]{style=multiline,topsep=10pt,leftmargin=35pt,font=\normalfont,%
    align=parright}


% index
\usepackage{multicol}
\usepackage{makeidx}
\addto\captionsfrancais{%
\renewcommand*\indexname{Liste des commandes \R}}
\newcommand{\foo}[1]{\texttt{#1}}
\newcommand{\cmd}[1]{\index{#1@\foo{#1}}}
\newcommand{\blankpage}{
  \newpage
  \thispagestyle{empty}
  \mbox{}
  \newpage
  }

\mdfdefinestyle{titlep}{innerleftmargin=20,innerrightmargin=20,innertopmargin=20,innerbottommargin=20,roundcorner=10pt}
  
\begin{document}
\thispagestyle{empty}
\centerline{\small Centre d'Enseignement de la Statistique Appliquée, à la Médecine et à la Biologie Médicale}
\vspace*{2cm}
\begin{center}
\centerline{\includegraphics[scale=.55]{cesam}}
\vspace*{2cm}
\begin{minipage}{.75\textwidth}
\begin{mdframed}[style=titlep]
\centerline{\Huge Programme de travail}
\vskip1em
\centerline{\Huge du cours d'informatique du CESAM}
\end{mdframed}
\end{minipage}
\end{center}
\vskip3em
\centerline{\Huge\bf Introduction au logiciel R}
\vskip5em
\begin{center}
  \begin{tabular}{ll}
    \textbf{Responsables :} & \\
    Christophe LALANNE & \url{christophe.lalanne@inserm.fr} \\
    Pr Mounir MESBAH   & \url{mounir.mesbah@upmc.fr}
  \end{tabular}
\end{center}
\vskip3em
\centerline{\Large \url{http://www.cesam.upmc.fr}}
\vskip3em
\centerline{\LARGE Année Universitaire 2012–2013}
\vfill
\begin{center}
\begin{minipage}{.6\textwidth}
\centering
Adresser toute correspondance à :\\
Université Pierre et Marie Curie – Paris 6
Secrétariat du CESAM – Les Cordeliers
Service Formation Continue, esc. B, 4ème étage,
15 rue de l’école de médecine,
75006 PARIS\\
ou par Courriel à : \url{cesam@upmc.fr}
\end{minipage}
\end{center}  

\blankpage

% Sweave custom outputs
% could use R CMD pgfsweave --pgfsweave-only $< but it's too much color
\DefineVerbatimEnvironment{Sinput}{Verbatim}
{formatcom = {\color{Sinput}}} 
\DefineVerbatimEnvironment{Soutput}{Verbatim}
{formatcom = {\color{Soutput}},fontsize=\small}
\DefineVerbatimEnvironment{Scode}{Verbatim}
{formatcom = {\color{Sinput}}} 
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}
%\renewcommand{\texttt}[1]{\textcolor{Sinput}{#1}}
\setkeys{Gin}{width=.4\textwidth}
\SweaveOpts{prefix.string=figs/fig}

<<echo=FALSE>>=
library(lattice)
library(gridExtra)
ltheme <- canonical.theme(color=FALSE)     
ltheme$strip.background$col <- "transparent"
lattice.options(default.theme=ltheme)
options(width=100, show.signif.stars=FALSE)
set.seed(101)
@ 

\chapter*{Calendrier}
\thispagestyle{empty}
\vskip3em


\begin{center}
\resizebox{\linewidth}{!}{\begin{tabular}{|l|p{10cm}|l|l|l|}
\hline
  \multicolumn{5}{|c|}{Introduction au logiciel R} \\
\hline
  Sem. 05/02 & Éléments du langage, gestion de données & Cours 1 & Corrigés
  pp.~\pageref{start:sol1}–\pageref{stop:sol1} & \\
  Sem. 12/02 & Statistiques descriptives et estimation & Cours 2 & Corrigés
  pp.~\pageref{start:sol2}–\pageref{stop:sol2} & Devoir \no 1\\
  Sem. 19/02 & Comparaisons de deux variables & Cours 3 & Corrigés
  pp.~\pageref{start:sol3}–\pageref{stop:sol3} & Devoir \no 2\\
  Sem. 26/02 & Analyse de variance et plans d'expérience & Cours 4 &
  Corrigés pp.~\pageref{start:sol4}–\pageref{stop:sol4} & Devoir \no 3\\
  Sem. 12/03 & Corrélation et régression linéaire & Cours 5 & Corrigés
  pp.~\pageref{start:sol5}–\pageref{stop:sol5} & Devoir \no 4\\
  Sem. 19/03 & Mesures d'association en épidémiologie et régression
  logistique & Cours 6 & Corrigés
  pp.~\pageref{start:sol6}–\pageref{stop:sol6} & Devoir \no 5\\
  Sem. 26/03 & Données de survie & Cours 7 & Corrigés
  pp.~\pageref{start:sol7}–\pageref{stop:sol7} & Devoir \no 6\\
\hline
\end{tabular}}
\end{center}

\chapter*{Organisation}
\setcounter{page}{1}

Une bonne partie des actions effectuées à partir d'un logiciel statistique
revient à manipuler, voire transformer des données numériques représentant
des données statistiques au sens propre. Il est donc primordial de bien
comprendre comment sont représentées de telles données dans un logiciel tel
que \R ou \Stata. La description des variables d'intérêt et le résumé de leur
distribution sous forme numérique et graphique constitue une étape préalable
et fondamentale à toute modélisation statistique, d'où l'importance de ces
premières étapes dans le déroulement d'un projet d'analyse statistique. Dans
un second temps, il est essentiel de bien maîtriser les commandes permettant
de calculer les principales mesures d'association en recherche médicale et
de construire les modèles explicatifs et prédictifs classiques : analyse de
variance, régression linéaire et logistique et régression de Cox. À quelques
exceptions près, on préfèrera recourir aux commandes \R et \Stata disponibles
lors de l'installation du logiciel (commandes de base), plutôt qu'à des
packages ou librairies spécialisées de commandes. Des éléments de
comparaison entre les commandes de base et d'autres commandes plus
spécifiques, ainsi que des pointeurs vers des ressources bibliographiques ou
en ligne, sont fournis dans les chapitres de cours.

Dans la \textbf{première séance}, on introduira les commandes de base pour
la gestion de données sous \R. Il s'agit principalement de la création et de
la manipulation de variables quantitatives et qualitatives (recodage de
valeurs individuelles, comptage des observations manquantes), de
l'importation de bases de données stockées sous forme de fichiers texte,
ainsi que d'opérations arithmétiques élémentaires (minimum, maximum, moyenne
arithmétique, différence, fréquence, etc.). On verra également comment
sauvegarder des bases de données pré-traitées au format texte ou
\R. L'objectif est de comprendre comment les données sont représentées sous
\R, et comment travailler à partir de celles-ci.

La \textbf{deuxième séance} porte sur les commandes utiles pour la
description d'un tableau de données constitué de variables quantitatives ou
qualitatives. L'approche descriptive est strictement univariée, ce qui
constitue le préalable à toute démarche statistique. Les commandes
graphiques de base (histogramme, courbe de densité, diagramme en barres)
seront présentées en complément des résumés descriptifs numériques usuels de
tendance centrale (moyenne, médiane) et de dispersion (variance,
quartiles). On abordera également l'estimation ponctuelle et par intervalle
à l'aide d'une moyenne arithmétique et d'une proportion empirique.
L'objectif est de se familiariser avec l'emploi de commandes \R simples
opérant sur une variable, éventuellement en précisant certaines options pour
le calcul, et à la sélection d'unités statistiques parmi l'ensemble des
observations disponibles.

La \textbf{troisième séance} est consacrée à la comparaison de deux
échantillons, pour des mesures quantitatives ou qualitatives. On aborde les
tests d'hypothèse suivants : test de Student pour échantillons indépendants
ou appariés, test non-paramétrique de Wilcoxon, test du $\chi^2$ et test
exact de Fisher, test de NcNemar, à partir des principales mesures
d'association pour deux variables (différence de moyennes, odds-ratio et
risque relatif). À partir de cette séance, on insistera moins sur la
description univariée de chaque variable, mais il est conseillé de toujours
procéder aux étapes de description des données vue lors de la 2\ieme\
séance. L'objectif est de maîtriser les principaux tests statistiques dans
le cas où l'on s'intéresse à la relation entre une variable quantitative et
une variable qualitative, ou pour deux variables qualitatives.

La \textbf{quatrième séance} est une introduction à l'analyse de variance
dans laquelle on cherche à expliquer la variabilité observée au niveau d'une
variable réponse numérique par la prise en compte d'un facteur de groupe ou
de classification et à l'estimation par intervalle de différences de
moyennes. On mettra l'accent sur la construction d'un tableau d'ANOVA
résumant les différentes sources de variabilité et sur les méthodes
graphiques permettant de résumer la distribution des données individuelles
ou aggrégées. On discutera également le test de tendance linéaire lorsque le
facteur de classification peut être considéré comme naturellement
ordonné. L'objectif est de comprendre comment construire un modèle
explicatif dans le cas où l'on a un, voire deux, facteurs explicatifs, et
comment présenter numériquement et graphiquement les résultats d'un tel
modèle à l'aide de \R.

La \textbf{cinquième séance} porte sur l'analyse de la relation linéaire
entre deux variables quantitatives continues. Dans l'approche de corrélation
linéaire, qui suppose une relation symétrique entre les deux variables, on
s'intéressera à quantifier la force et la direction de l'association de
manière paramétrique (corrélation de Bravais-Pearson) ou non-paramétrique
(corrélation de Spearman basée sur les rangs) et à la représentation
graphique de cette relation. La régression linéaire simple sera utilisée
dans le cas où l'une des deux variables numériques joue le rôle d'une
variable réponse et l'autre celui d'une variable explicative. On présentera
les commandes utiles pour l'estimation des coefficients de la droite de
régression, la construction du tableau d'ANOVA associé à la régression, et
la prédiction. L'objectif de la séance reste identique à celui de la 4\ieme\
séance, à savoir présenter les commandes \R nécessaires à la construction
d'un modèle statistique simple entre deux variables, dans une optique
explicative ou prédictive.

Lors de la \textbf{sixième séance} seront abordées les principales mesures
d'association rencontrées dans les études épidémiologiques : odds-ratio,
risque relatif, prévalence, etc. Les commandes \R permettant l'estimation
(ponctuelle et par intervalle) et les tests d'hypothèse associés seront
illustrées sur des données de cohorte ou d'études cas-témoins. La mise en
\oe uvre d'un modèle de régression logistique simple permet de compléter
l'éventail des méthodes statistiques permettant d'expliquer la variabilité
observée au niveau d'une variable réponse binaire. L'objectif est de
comprendre les commandes \R à utiliser dans le cas où les variables sont
binaires, soit pour résumer un tableau de contingence sous forme
d'indicateurs d'association soit pour modéliser la relation entre une
réponse binaire (malade/non-malade) et une variable explicative qualitative
à partir de données dites groupées.

La \textbf{septième séance} est une introduction à l'analyse de données
censurées et au principaux tests (log-rank, Wilcoxon) et modèles de survie
(modèle de Cox). La spécificité des données censurées impose un soin
particulier dans le codage des données sous \R, et l'objectif est de
présenter les commandes \R essentielles à la bonne représentation des
données de survie sous forme numérique, à leur résumé numérique (médiane de
survie) et graphique (courbe de Kaplan-Meier) et à la mise en \oe uvre des
tests courants. 

%% Les \textbf{séances 8 à 11} reprennent essentiellement les mêmes idées, en
%% utilisant \Stata au lieu de \R : importation et recodage de données,
%% description numérique et graphique des variables quantitatives et
%% qualitatives, mesures d'association entre une variable quantitative ou
%% qualitative et une variable qualitative, analyse de la variance, association
%% entre deux variables quantitatives (corrélation linéaire et de rangs) et
%% régression linéaire, régression logistique et analyse de données de survie
%% par le modèle de Cox et le test du log-rank. L'illustration des commandes
%% \Stata permettant de répondre à ces questions est basée sur une partie des
%% exercices des 7 premières séances sert de base, ce qui permet d'éviter
%% d'avoir à se familiariser avec de nouveaux exemples ou cas d'utilisation.

Durant les séances, le travail réalisé sur machine pour répondre aux
questions posées dans les exercices doit être sauvegardé dans un fichier
script R avec un nom permettant de facilement retrouver le travail lors des
séances ultérieures (par exemple, \texttt{semaine01.R}). Il est conseillé
d'enregistrer ce fichier dans un répertoire spécifique, par exemple
\texttt{script}, sur une clé USB. De même, on supposera que toutes les
données utilisées dans les séances sont disponibles dans un répertoire
appelé \texttt{data}, également sur une clé USB.

Après chaque séance (à l'exception de la première), il est proposé un
\textbf{devoir} composé d'une quinzaine de questions et permettant à
l'étudiant de s'auto-évaluer. Les questions se limitent strictement aux
éléments vus en cours et durant les exercices dirigés. Les réponses aux
questions doivent être fournies sur le site web du cours :
\url{http://cesam-informatique.net}. 

\cleardoublepage

%% \part*{Module R}

%% \pagestyle{fancy}
%% \fancyhf{}
%% \fancyhead[L]{\bf\nouppercase{\leftmark}}
%% \fancyhead[R]{\raisebox{-.5ex}{\includegraphics[height=0.2in,width=0.38in]{cesam}}}
%% \fancyfoot[C]{\thepage}

\chapter{Éléments du langage}\label{chap:langage}
\Opensolutionfile{solutions}[solutions1]

\section*{Énoncés}
%
%
%
\begin{exo}\label{exo:1.1}
Un chercheur a recueilli les mesures biologiques suivantes (unités
arbitraires) :
\begin{verbatim}
3.68  2.21  2.45  8.64  4.32  3.43  5.11  3.87
\end{verbatim}
\begin{description}
\item[(a)] Stocker la séquence de mesures dans une variable appelée
  \texttt{x}.  
\item[(b)] Indiquer le nombre d'observations (à l'aide de \R), les valeurs
  minimale et maximale, ainsi que l'étendue.  
\item[(c)] En fait, le chercheur réalise que la valeur 8.64 correspond à une
  erreur de saisie et doit être changée en 3.64. De même, il a un doute sur
  la 7\ieme mesure et décide de la considérer comme une valeur manquante :
  effectuer les transformations correspondantes. 
\end{description}
\begin{sol}
On commence par saisir les données :
<<ex1-1a>>=
x <- c(3.68,2.21,2.45,8.64,4.32,3.43,5.11,3.87)
x
@
Le nombre d'éléments de \texttt{x}, c'est-à-dire le nombre d'observations ou
unités statistiques, s'obtient ainsi :
<<ex1-1b>>=
length(x)
@ 
En ce qui concerne l'étendue de variation des mesures collectées, on peut
utiliser une commande générique, telle que \verb|summary(x)|, ou des
commandes plus spécifiques, comme indiqué ci-dessous :
<<ex1-1c>>=
min(x)
max(x)
range(x)
range(x)[2] - range(x)[1]  # ou diff(range(x))
@ 
\cmd{min}\cmd{max}\cmd{range}
On notera que \verb|range(x)| renvoit deux valeurs, correspondant dans
l'ordre de lecture au minimum et au maximum, de la variable \texttt{x}, ce
qui nous permet de calculer l'étendue comme la différence entre ces deux
valeurs. 

Concernant les transformations suggérées, on peut remplacer la 4\ieme
mesure, \verb|x[4]|, comme ceci : \verb|x[4] <- 3.64|. En fait cela
nécessite de connaître le numéro ou la position de l'observation, ce qui
se révèle peu pratique dans le cas où le nombre d'observations est
grand. Une solution alternative consiste donc à "isoler" l'observation en
question à l'aide de sa valeur (on parlera d'un test logique sur les valeurs
de \texttt{x}) :
<<ex1-1d>>=
x[x == 8.64] <- 3.64
x
@

Enfin, pour recoder la 7\ieme observation en valeur manquante, on procèdera
comme suit :
<<ex1-1e>>=
x[7] <- NA
x
@ 
\cmd{NA}
\end{sol}
\end{exo}
%
%
%
\begin{exo}\label{exo:1.2}
La charge virale plasmatique permet de décrire la quantité de virus (p.~ex.,
VIH) dans un échantillon de sang. Ce marqueur virologique qui permet de
suivre la progression de l’infection et de mesurer l’efficacité des
traitements est rapporté en nombre de copies par millilitre, et la plupart
des instruments de mesure ont un seuil de détectabilité de 50
copies/ml. Voici une série de mesures, $X$, exprimées en logarithmes (base 10)
collectées sur 20 patients :
\begin{verbatim}
3.64 2.27 1.43 1.77 4.62 3.04 1.01 2.14 3.02 5.62 5.51 5.51 1.01 1.05 4.19
2.63 4.34 4.85 4.02 5.92
\end{verbatim}
Pour rappel, une charge virale de 100 000 copies/ml équivaut à 5 log.
\begin{description}
\item[(a)] Indiquer combien de patients ont une charge virale considérée
  comme non-détectable. 
\item[(b)] Quelle est le niveau de charge virale médian, en copies/ml, pour
  les données considérées comme valides ?
\end{description}
\begin{sol}
Avant toute chose, il est nécessaire d'exprimer la limite de détection (50
copies/ml) en logarithmes ; celle-ci vaut en fait
<<ex1-2a>>=
log10(50)
@ 
\cmd{log10}
Ensuite, il suffit de filtrer les observations qui ne remplissent pas la
condition $X>1.70$ (on utilisera le résultat numérique exact, pas la valeur
approchée) :
<<ex1-2b>>=
X <- c(3.64,2.27,1.43,1.77,4.62,3.04,1.01,2.14,3.02,5.62,5.51,5.51,1.01,1.05,4.19,
       2.63,4.34,4.85,4.02,5.92)
length(X[X <= log10(50)])
@ 
\cmd{length}

Finalement, la charge virale médiane pour les 16 patients ayant une mesure
considérée comme valide peut se calculer comme suit :
<<ex1-2c>>=
Xc <- X[X > log10(50)]
round(median(10^Xc), 0)
@ 
\cmd{median}\cmd{round}
Notons que le résultat ci-dessus est présenté sans décimales.
\end{sol}
\end{exo}
%
%
%
\begin{exo}\label{exo:1.3}
Le fichier \texttt{dosage.txt} contient une série de 15 dosages biologiques,
stockés au format numérique avec 3 décimales, comme suit
\begin{verbatim}
6.379 6.683 5.120 ...
\end{verbatim}
\begin{description}
\item[(a)] Utiliser \verb|scan| pour lire ces données (bien lire l'aide en
  ligne concernant l'usage de cette commande, en particulier l'option
  \texttt{what=}). 
\item[(b)] Corriger la série de mesures afin de pouvoir calculer la moyenne
  arithmétique.   
\item[(c)] Enregistrer les données corrigées dans fichier texte appelé
  \texttt{data.txt}. 
\end{description}
\begin{sol}
Lorsque les données stockées dans un fichier texte ne sont pas trop
volumineuses, il est recommendé d'y jeter un oeil à l'aide d'un simple
éditeur de texte avant de les importer dans \R. Dans le cas présent, il
s'avère qu'il y a eu un problème de codage du séparateur décimal : pour une
des mesures, une virgule a été utilisée à la place d'un point pour séparer
la partie entière de la partie décimale des nombres.

Si l'on se contente de l'instruction suivante
<<ex1-3a, eval=FALSE>>=
x <- scan("data/dosage.txt")
@ 
\cmd{scan}
\R retournera un message d'erreur du type
\begin{verbatim}
Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  : 
  scan() attendait 'a real' et a reçu '2,914'
\end{verbatim}
puisque par défaut \R tente de lire des nombres (au format
anglo-saxon). Pour corriger cela, il faut donc demander à \R 
<<ex1-3b>>=
x <- scan("data/dosage.txt", what="character")
str(x)
head(x)
@ 
\cmd{scan}\cmd{str}\cmd{head}

Toutefois, il n'est pas possible de calculer directement la moyenne sur les
données importées de cette manière puisqu'il s'agit de caractères
(\texttt{character}). On corrigera donc l'observation mal enregistrée, avant
de convertir les mesures en nombres exploitables par \R :
<<ex1-3c>>=
x[x == "2,914"] <- "2.914"
x <- as.numeric(x)
head(x)
round(mean(x), 3)
@ 
\cmd{as.numeric}\cmd{head}\cmd{round}\cmd{mean}

Enfin, pour sauvegarder les données corrigées dans un nouveau fichier, on
peut utiliser la commande générale \texttt{write.table}, qui possède
beaucoup d'options, mais qui en générale fonctionne de la manière suivante :
<<ex1-3d>>=
write.table(x, file="data/data.txt", row.names=FALSE)
@ 
Un aperçu du fichier texte ainsi sauvegardé est fourni ci-dessous :
\begin{verbatim}
"x"
6.379
6.683
5.12
6.707
6.149
5.06
\end{verbatim}
Si l'on ne souhaite pas faire apparaître le nom de la variable sur la
première ligne, on rajoutera l'option \verb|col.names=FALSE|.
\end{sol}
\end{exo}
%
%
%
\begin{exo}\label{exo:1.4}
Lors d'une enquête épidémiologique, les données suivantes ont été collectées
(à partir d'un questionnaire retourné par voie postale) : l'âge (en années),
le sexe (M, masculin, F, féminin, ou T, transgenre), le niveau de QI (score
numérique, positif), le statut socio-économique (variable qualitative à
trois modalités, A, B et C), et un score de qualité de vie (exprimé sur une
échelle allant de 0 à 100 points). Un aperçu des données pour les 10
premiers individus est fourni ci-dessous :
\vskip1em

\begin{tabular}{cccccc}
\toprule
id & age & sexe & qi & sse & qdv \\
\midrule
1 &  26  &  M & 126 &  B & 72 \\
2 &  31  &  F & 123 &  A & 73 \\
3 &  28  &  M & 114 &  B & 72 \\
4 &  28  &  M & 125 &  B & 72 \\
5 &  29  &  F & 134 &  A & 76 \\
6 &  33  &  F & 141 &  B & 74 \\
7 &  32  &  F & 123 &  B & 72 \\
8 &  21  &  M & 114 &  A & 71 \\
9 &  36  &  M & 122 &  C & 71 \\
10 & 30  &  M & 127 &  A & 66 \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
  \item[(a)] Créer un \verb|data.frame|, nommé \texttt{dfrm}, pour stocker les données de
ces 10 individus.
  \item[(b)] L'ensemble de la base de données a été enregistré dans
un fichier Excel, puis exportée au format \textsf{CSV} (c'est-à-dire un
fichier texte dans lequel les données de chaque individu sont écrites sur
une même ligne, avec des virgules séparant les valeurs de chaque variable,
encore appelé "champs") sous le nom \texttt{enquete.csv} : charger le
fichier sous \R, et vérifier la concordance des données avec celles créées
au préalable.
\item[(c)] Indiquer la proportion d'hommes et de femmes dans cet échantillon.
\item[(d)] La variable qdv contient-elle des valeurs manquantes ? Si oui,
  combien et pour quels numéros d'observations (lignes du tableau de
  données) ?
\end{description}
\begin{sol}
Commençons par saisir les mesures correspondant à chacune des 6 variables
une par une :
<<ex1-4a>>=
id <- 1:10
age <- c(26,31,28,28,29,33,32,21,36,30)
sexe <- c("M","F","M","M","F","F","F","M","M","M")
qi <- c(126,123,114,125,134,141,123,114,122,127)
sse <- c("B","A","B","B","A","B","B","A","C","A")
qdv <- c(72,73,72,72,76,74,72,71,71,66)
dfrm <- data.frame(id, age, sexe, qi, sse, qdv)
dfrm
@
\cmd{data.frame}
Les variables intermédiaires (\texttt{id}, \texttt{age}, etc.) sont toujours
présentes dans l'espace de travail de R (ce que l'on peut vérifier en
tapant la commande \verb|ls()|), et on peut vouloir les supprimer pour
éviter toute confusion lors de leur manipulation puisqu'elles sont à présent
dispnibles directement depuis le \texttt{data.frame} nommé \texttt{dfrm}. On
utilisera la commande \texttt{rm} comme ceci :
<<ex1-4b>>=
rm(id, age, sexe, qi, sse, qdv)
ls()
@ 
\cmd{rm}\cmd{ls}
Pour exporter le fichier au format CSV, on utiliserait la commande
\texttt{write.csv} (séparateur de champ = virgule) ou \texttt{write.csv2}
(séparateur de champ = point-virgule).
<<ex1-4c, eval=FALSE>>=
write.csv(dfrm, file="data/dfrm.csv")
@ 
\cmd{write.csv}
Chargeons à présent les données réelles du fichier \texttt{enquete.csv}. 
<<ex1-4d>>=
enquete <- read.csv("data/enquete.csv")
str(enquete)
@ 
\cmd{read.csv}\cmd{str}
La commande \texttt{str} fournit un aperçu du type de chaque variable
(variable numérique ou qualitative) et des premières observations. On voit
ici que les variables \texttt{sexe} et \texttt{sse} sont bien considérées
comme des variables qualitatives (\texttt{factor} sous R). Pour afficher les
10 premières observations, on peut utiliser la commande \texttt{head}, par exemple
<<ex1-4e>>=
head(enquete, n=10)
@ 
\cmd{head}
ou alors directement indexer le tableau de données avec les 10 premiers
éléments : 
<<ex1-4f>>=
enquete[1:10,]
@
Ici, on indique entre crochets les observations qui nous intéressent (1 à
10, le \og à\fg\ se traduisant par \texttt{:} sous R) puis les colonnes
d'intérêt (ici, on ne met rien pour avoir l'ensemble des variables). On peut
ainsi vérifier la bonne concordance des valeurs.

La proportion d'hommes et de femmes peut être obtenue à l'aide de la
commande \texttt{table}, en tenant compte du nombre d'observations que l'on
peut peut calculer comme le nombre de lignes du tableau de données :
<<ex1-4g>>=
table(enquete$sexe) / nrow(enquete)
@ 
\cmd{table}
Notons que l'on aurait très bien pu compter chacune des modalités prises par
la variable \texttt{sexe}, soit \texttt{F} ou \texttt{H}, en se rappelant
que les classes ou modalités d'une variable qualitatives doivent être
entourées d'apostrophes anglo-saxonnes (\og quote\fg).
Par exemple, pour les femmes on pourrait procéder ainsi :
<<ex1-4h>>=
sum(enquete$sexe == "F") / nrow(enquete) 
@
\cmd{sum}\cmd{nrow}
Cette approche ne fonctionnera que s'il n'y a aucune donnée manquante car la
commande \texttt{nrow} renvoit la taille du tableau de données (nombre de
lignes) indépendemment de son contenu. Si l'on veut contrôler la présence de
valeurs manquantes, il est donc préférable d'utiliser la commande
\texttt{complete.cases}, soit
<<ex1-4i>>=
nrow(enquete)
sum(complete.cases(enquete$sexe))
sum(complete.cases(enquete$qdv))
@ 
\cmd{nrow}\cmd{sum}\cmd{complete.cases}
ce qui montre que pour la variable \texttt{qdv} il y a en réalité trois
données manquantes. La même information peut être obtenue à partir de la
commande plus générale \texttt{summary} :
<<ex1-4j>>=
summary(enquete)
@
\cmd{summary}
Pour identifier les observations manquantes pour la variable \texttt{qdv},
le plus simple est d'utiliser \texttt{which} en combinaison avec la commande
\texttt{is.na} qui renvoit vrai ou faux selon que la donnée est considérée
comme manquante ou non, soit :
<<ex1-4k>>=
which(is.na(enquete$qdv))
@ 
\cmd{which}\cmd{is.na}
\end{sol}
\end{exo}
\begin{exo}\label{exo:1.5}
Le fichier \texttt{anorexia.dat} contient les données d'une étude clinique
chez des patientes anorexiques ayant reçu l'une des trois thérapies
suivantes : thérapie comportementale, thérapie familiale, thérapie
contrôle.\autocite{hand93} 
\begin{description}
\item[(a)] Combien y'a-t-il de patientes au total ? Combien y'a-t-il de
  patientes par groupe de traitement ?
\item[(b)] Les mesures de poids sont en livres. Les convertir en
  kilogrammes.    
\item[(c)] Créer une nouvelle variable contenant les scores de différences
  (\texttt{After} - \texttt{Before}).
\item[(d)] Indiquer la moyenne et l'étendue (min/max) des scores de
  différences par groupe de traitement.
\end{description}
\begin{sol}
Un aperçu des données contenues dans le fichier \texttt{anorexia.dat} est
fourni ci-dessous (5 premières lignes du fichier) :
\begin{verbatim}
Group Before After
g1 80.5  82.2
g1 84.9  85.6
g1 81.5  81.4
g1 82.6  81.9
\end{verbatim}
On voit donc que la première ligne est une ligne d'en-tête indiquant le nom
des variables, et chacune des lignes suivantes représente une unité
statistique pour laquelle on a le groupe d'appartenance, la mesure de poids
avant le début de la prise en charge et la mesure de poids à la fin de la thérapie.
Pour importer ce type de données, on utilisera la commande
\texttt{read.table} en précisant l'option \verb|header=TRUE| pour bien
prendre en compte la ligne d'en-tête.
<<ex1-5a>>=
anorex <- read.table("data/anorexia.dat", header=TRUE)
names(anorex)
head(anorex)
@ 
Le nombre total de patientes correspond au nombre de lignes du tableau de
données :
<<ex1-5b>>=
nrow(anorex)
@ 
Il y a donc au total 72 patientes. Pour trouver la répartition des effectifs
par groupe, le plus simple est de faire un tri à plat de la variable
qualitative \texttt{Group} :
<<ex1-5c>>=
table(anorex$Group)
@ 
Pour convertir les poids exprimés en livres en kilogrammes, il suffit de
diviser chaque mesure par 2.2 (approximativement). On réalise cette
opération pour chacune des deux variables \texttt{Before} et \texttt{After}.
<<ex1-5d>>=
anorex$Before <- anorex$Before/2.2
anorex$After <- anorex$After/2.2
@ 
On notera que dans ce cas-là, les valeurs d'origine de ces deux variables
seront simplement remplacées. Si l'on souhaite y avoir accès de nouveau, il
faudra recharger le fichier. Une autre solution aurait consisté à créer deux
nouvelles variables, comme ceci :
<<ex1-5e, eval=FALSE>>=
anorex$Before.kg <- anorex$Before/2.2
anorex$After.kg <- anorex$After/2.2
@
Pour calculer les scores de différences \texttt{After} - \texttt{Before}, il
suffit de soustraire les valeurs des deux variables, en se rappelant que ce
genre d'opérations opère élément par élément (c'est-à-dire pour chaque unité
statistique). Ici, on ajoutera la variable nouvellement créée au tableau de
données \texttt{anorex}.
<<ex1-5f>>=
anorex$poids.diff <- anorex$After - anorex$Before
head(anorex)
@ 
Enfin, pour calculer la moyenne et l'étendue des scores de différence par
groupe de traitement, on peut procéder de deux manières. Soit on isole
chaque groupe et on calcule les statistiques demandées, soit on \og
factorise\fg\ l'opération de calcul en opérant par modalité de la variable
qualitative. La première solution s'obtiendrait ainsi, pour le 1\ier\ groupe
par exemple :
<<ex1-5g>>=
mean(anorex$poids.diff[anorex$Group == "g1"])
range(anorex$poids.diff[anorex$Group == "g1"])
@
L'autre solution plus économique consiste à utiliser la commande
\texttt{tapply} de la manière suivante :
<<ex1-5h>>=
tapply(anorex$poids.diff, anorex$Group, mean)
tapply(anorex$poids.diff, anorex$Group, range)
@ 
Comme on le voit, pour utiliser la commande \texttt{tapply} on spécifie la
variable réponse (\texttt{poids.diff}), le facteur de classification
(\texttt{Group}) et la commande à appliquer (\texttt{mean}). On notera que
plutôt que de préfixer chaque nom de variable par le nom du tableau de
données, on peut simplifier la syntaxe en utilisant \texttt{with} :
<<ex1-5i>>=
with(anorex, tapply(poids.diff, Group, mean))
@ 
\end{sol}
\end{exo}
%
% STAB TD 3
%
\begin{exo}\label{exo:1.6}
Soient les mesures de bioluminescence recueillies dans une étude sur
l'antagonisme Na/An. Les données sont présentées dans le tableau suivant et
elles ont été enregistrées dans le fichier \texttt{bioluminescence.dat} où
les mesures recueillies pour chaque traitement sont arrangées en colonnes
(comme dans le tableau).
\vskip1em

\begin{tabular}{cccc}
\toprule
Sans Na & Avec Na & Sans Na & Avec Na \\
Sans An & Sans An & Avec An & Avec An \\
\midrule
6.5 & 26.5 & 12.0 & 12.6 \\
6.2 & 22.7 & 8.2 & 13.4 \\
6.8 & 17.0 & 12.3 & 12.0 \\
5.7 & 18.0 & 11.3 & 6.0 \\
7.2 & 14.9 & 7.0 & – \\
5.9 & 24.0 & 10.5 & – \\
8.2 & 19.9 & – & – \\
8.0 & 24.0 & – & – \\
10.5 & – & – & – \\
7.4 & – & – & – \\
8.9 & – & – & – \\
11.5 & – & – & – \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Importer les données sous R et reformater le tableau de données
  de manière à ce que la variable réponse soit dans une seule colonne, et
  les facteurs d'étude (\texttt{Na} et \texttt{An}) dans deux colonnes
  séparées (\texttt{data.frame}).
\item[(b)] Vérifier le nombre d'observations disponible pour chacun des
  quatre traitements, un traitement correspondant au croisement des niveaux
  de chacun des facteurs d'étude.
\item[(c)] Enregistrer le tableau de données ainsi constitué au format
  \texttt{RData}. Il sera exploité lors de la séance~4 (p.~\pageref{chap:anova}).
\end{description}
\begin{sol}
Pour importer les données, on prendra garde au fait que le fichier texte
\texttt{bioluminescence.dat} ne comprend pas de ligne d'en-tête.  
<<ex1-6a>>=
biolum <- read.table("data/bioluminescence.dat", header=FALSE)
biolum
@   

On crée ensuite les deux facteurs d'étude, \texttt{An} et \texttt{Na}, en
indiquant le nombre de répétitions des niveaux à partir du nombre
d'observations non manquantes dans chaque colonne du tableau d'origine. Pour
le premier traitement (\texttt{An-/Na-}) on sait qu'il y a 12 observations,
sachant qu'au total \texttt{An-} totalise 20 observations (1\iere\ et
2\ieme\ colonnes).
<<ex1-6b>>=
An <- rep(c("-","+"), c(20,10))
Na <- rep(c("-","+","-","+"), c(12,8,6,4))
biolum <- with(biolum, data.frame(y=c(V1,V2[1:8],V3[1:6],V4[1:4]), An, Na))
head(biolum)
summary(biolum)
@ 

Pour obtenir le nombre d'observations par croisement des niveaux des deux
facteurs (soit 4 combinaisons ou traitements), on utilisera la commande
\texttt{length} pour compter le nombre d'observations avec la commande
\texttt{tapply} qui permet d'isoler les combinaisons de niveaux de facteurs.
<<ex1-6c>>=
with(biolum, tapply(y, list(An, Na), length))
@ 

Pour sauvegarder le nouveau tableau de données, on utilisera la commande
\texttt{save} en donnant le nom du fichier dans lequel stocker les données,
par exemple \texttt{bioluminescence.RData}.
<<ex1-6d>>=
save(biolum, file="data/bioluminescence.RData")
@ 
\end{sol}
\end{exo}

\Closesolutionfile{solutions}

\chapter{Statistiques descriptives et estimation}\label{chap:descriptive}
\Opensolutionfile{solutions}[solutions2]


\section*{Énoncés}
%
%
%
\begin{exo}\label{exo:2.1}
Une variable quantitative $X$ prend les valeurs suivantes sur un échantillon
de 26 sujets :
\begin{verbatim}
24.9,25.0,25.0,25.1,25.2,25.2,25.3,25.3,25.3,25.4,25.4,25.4,25.4,
25.5,25.5,25.5,25.5,25.6,25.6,25.6,25.7,25.7,25.8,25.8,25.9,26.0
\end{verbatim}
\begin{description}
\item[(a)] Calculer la moyenne, la médiane ainsi que le mode de $X$. 
\item[(b)]  Quelle est la valeur de la variance estimée à partir de ces données ? 
\item[(c)] En supposant que les données sont regroupées en 4 classes dont les
  bornes sont : 24.9–25.1, 25.2–25.4, 25.5–25.7, 25.8–26.0, afficher la
  distribution des effectifs par classe sous forme d'un tableau d'effectifs. 
\item[(d)] Représenter la distribution de $X$ sous forme d'histogramme, sans
  considération d'intervalles de classe \emph{a priori}.
\end{description}
\begin{sol}
Une manière de représenter ce type de données consiste à utiliser saisir les
observations comme ceci :
<<ex2-1a>>=
x <- c(24.9,25.0,25.0,25.1,25.2,25.2,25.3,25.3,25.3,25.4,25.4,25.4,25.4,
       25.5,25.5,25.5,25.5,25.6,25.6,25.6,25.7,25.7,25.8,25.8,25.9,26.0)
@ 

La médiane et la moyenne sont obtenues à l'aide des commandes
suivantes :
<<ex2-1b>>=
median(x)
mean(x)
@
\cmd{median}\cmd{mean}
Concernant la médiane, on peut vérifier qu'il s'agit bien de la valeur
correspondant au deuxième quartile ou au 50\ieme\ percentile, c'est-à-dire
la valeur de $X$ telle que 50~\% des observations lui sont inférieures :
<<>>=
quantile(x)
@ 
\cmd{quantile}

Pour le mode, il est nécessaire d'afficher la distribution des effectifs
selon les valeurs de $X$, puis vérifier à quelle valeur de $X$ est associé
l'effectif le plus grand:
<<ex2-1c>>=
table(x)
@
\cmd{table}
<<ex2-1d, echo=FALSE>>=
m <- names(table(x)[table(x)==max(table(x))])
@
Ici, on voit qu'il y a deux modes : \Sexpr{m[1]} et \Sexpr{m[2]}.

L'estimé de la variance est donné par :
<<ex2-1e>>=
var(x)
@ 
\cmd{var}

En supposant que les données sont regroupées en 4 classes dont les bornes
sont : 24.9–25.1, 25.2–25.4, 25.5–25.7, 25.8–26.0, on peut recalculer la
distribution des effectifs par classe.
<<ex2-1f>>=
xc <- cut(x, breaks=c(24.9,25.2,25.5,25.8,26.0), include.lowest=TRUE, right=FALSE)
table(xc)
@
\cmd{cut}\cmd{table}
On remarquera que R utilise la notation anglo-saxonne pour représenter les
bornes des intervalles de classe : le symbole \texttt{)} à droite d'un
nombre signifie que ce nombre est exclu de l'intervalle, alors que
\texttt{]} signifie que l'intervalle contient ce nombre.

On peut visualiser la distribution des effectifs à l'aide d'un histogramme
comme suit :
<<ex2-1g, fig=TRUE>>=
histogram(~ x, type="count")
@
\cmd{histogram}

Par défaut, R détermine automatiquement les intervalles de classe. Si l'on
souhaite spécifier soi-même les bornes des intervalles de classe, on
utilisera l'option \texttt{breaks=}. Par exemple, pour afficher la
distribution des effectifs selon les 4 classes définies plus haut, on écrirait :
\begin{verbatim}
> histogram(~ x, type="count", breaks=c(24.9,25.2,25.5,25.8,26.0))
\end{verbatim}
\end{sol}
\end{exo}
%
% Everitt 2011 p. 38
%
\begin{exo}\label{exo:2.2}
On dispose des temps de survie de 43 patients souffrant de leucémie
granulocytaire chronique, mesurés en jours depuis le diagnostique :
\autocite[p.~38]{everitt01} 
\begin{verbatim}
7,47,58,74,177,232,273,285,317,429,440,445,455,468,495,497,532,571,
579,581,650,702,715,779,881,900,930,968,1077,1109,1314,1334,1367,
1534,1712,1784,1877,1886,2045,2056,2260,2429,2509
\end{verbatim}
\begin{description}
\item[(a)] Calculer le temps de survie médian. 
\item[(b)] Combien de patients ont une survie inférieure (strictement) à 900
  jours au moment de l'étude ? 
\item[(c)] Quelle est la durée de survie associée au 90\ieme\ percentile ?
\end{description}
\begin{sol}
Les données sont saisies comme à l'exercice précédent.  
<<ex2-2a>>=
s <- c(7,47,58,74,177,232,273,285,317,429,440,445,455,468,495,497,532,571,
       579,581,650,702,715,779,881,900,930,968,1077,1109,1314,1334,1367,
       1534,1712,1784,1877,1886,2045,2056,2260,2429,2509)
@ 
Le temps de survie médian est obtenu facilement :
<<ex2-2b>>=
median(s)
@ 
\cmd{median}

Pour déterminer le nombre de patients avec une survie $\le 900$ jours, il
suffit d'effectuer un test :
<<ex2-2c>>=
table(s <= 900)
@
\cmd{table}
La colonne libellée \texttt{TRUE} indique le nombre d'observations vérifiant
la condition ci-dessus. 
% Une autre solution possible est \verb|length(which(s <= 900))| (expliquer).

La survie associée au 90\ieme\ percentile peut s'obtenir à l'aide de la commande
\texttt{quantile}, par exemple :
<<ex2-2d>>=
quantile(s, 0.9)
@ 
\cmd{quantile}
On peut vérifier que le résultat correspond bien à la valeur de survie telle
que 90~\% des observations ne dépasse pas cette valeur.
<<ex2-2e>>=
table(s <= quantile(s, 0.9))
@ 
\cmd{table}
Ici, $38/43$ est bien inférieur à 0.90 tandis que $39/43=0.91$.
\end{sol}
\end{exo}
%
% Everitt 2011 p. 38
%
\begin{exo}\label{exo:2.3}
Le fichier \texttt{elderly.dat} contient la taille mesurée en cm de 351
personnes âgées de sexe féminin, sélectionnées aléatoirement dans la
population lors d'une étude sur l'ostéoporose. Quelques observations sont
cependant manquantes.
\begin{description}
\item[(a)] Combien y'a t-il d'observations manquantes au total ?
\item[(b)] Donner un intervalle de confiance à 95~\% pour la taille moyenne
  dans cet échantillon, en utilisant une approximation normale.
\item[(c)] Représenter la distribution des tailles observées sous forme
  d'une courbe de densité.  
\end{description}
\begin{sol}
Pour lire le fichier qui ne se compose que d'une série de valeurs
numériques, on utilise la commande \texttt{scan}. Attention, comme il y a
des valeurs manquantes, codées dans le fichier texte par des ".", il est
nécessaire de s'assurer que ces observations sont bien identifiées comme
telles par \R. 
<<ex2-3a>>=
tailles <- scan("data/elderly.dat", na.strings=".")
@ 
\cmd{scan}
On pourrait afficher la distribution des observations à l'aide d'un
simple histogramme (\verb|histogram(~ tailles)|). Cela permet en particulier
de vérifier la forme générale de la distribution et la présence
d'eventuelles valeurs "extrêmes".

Le nombre d'observations non complètes (données manquantes) est déterminé en
comptant le nombre de valeurs manquantes (\texttt{NA}), ce qui est
équivalent à établir un tri à plat des valeurs considérées comme manquantes
par \R à l'aide de la commande \texttt{table}.
<<ex2-3b>>=
sum(is.na(tailles))
table(is.na(tailles))
@
\cmd{sum}\cmd{is.na}\cmd{table}
On a donc 5 observations manquantes au total.

La moyenne est obtenue à l'aide de la commande \texttt{mean}. Cependant,
comme il existe des valeurs manquantes il est nécessaire de préciser
l'option \verb|na.rm=TRUE| pour indiquer à \R de calculer la moyenne
arithmétique sur les cas complets. La même remarque vaut pour l'usage de
\texttt{sd}. On a également besoin d'avoir une estimation de
l'écart-type. Le quantile de référence (97.5~\%) pour la loi normale 97.5~\%
est généralement pris à 1.96 dans les tables statistiques mais on peut
obtenir sa valeur avec R grâce à \texttt{qnorm}.  Soit au final :
<<ex2-3c>>=
m <- mean(tailles, na.rm=TRUE)    # moyenne
s <- sd(tailles, na.rm=TRUE)      # écart-type
n <- sum(!is.na(tailles))         # nombre d'observations
m - qnorm(0.975) * s/sqrt(n)      # borne inf. IC 95 %
m + qnorm(0.975) * s/sqrt(n)      # borne sup. IC 95 %
@ 
\cmd{mean}\cmd{sd}\cmd{sum}\cmd{is.na}\cmd{qnorm}\cmd{sqrt}
Attention, utiliser la commande \texttt{length} pour compter le nombre
d'observations serait erroné dans ce contexte : en présence de valeurs
manquantes, il faut explicitement s'assurer que l'on travaille bien sur les
cas complets. La commande \verb|sum(!is.na(tailles))| est équivalente à la
commande, peut-être plus explicite, \verb|sum(complete.cases(tailles))|.
Les deux dernières instructions peuvent se simplifier en exploitant la
capacité de \R à répéter le même calcul sur des séries de données stockées
dans une variable. Ici, on se contente de reproduire la formule $m\pm
z_{0.975}\frac{s}{\sqrt{n}}$ :
<<ex2-3d, eval=FALSE>>=
m + c(-1,1) * qnorm(0.975) * s/sqrt(n)
@
\cmd{qnorm}\cmd{sqrt}
% FIXME:
% sans doute à supprimer car trop compliqué.

Pour représenter la distribution des tailles sous forme d'une courbe de
densité, qui ne pose pas le problème du choix de classes \emph{a priori}, la
commande à utiliser est \texttt{densityplot}.
<<ex2-3e, fig=TRUE>>=
densityplot(~ tailles)
@ 
\cmd{densityplot}

\noindent Le degré de lissage de la courbe de densité estimée à partir des
données peut être contrôlé à l'aide de l'option \texttt{bw}. L'exemple
suivant produirait une courbe présentant beaucoup moins de variations
locales, par exemple.
\begin{verbatim}
> densityplot(~ tailles, bw=4)
\end{verbatim}
\cmd{densityplot}
\end{sol}
\end{exo}
% 
% Hosmer & Lemeshow 1989
%
\begin{exo}\label{exo:2.4}
Le fichier \texttt{birthwt} est un des jeux de données fournis avec \R. Il
comprend les résultats d'une étude prospective visant à identifier les
facteurs de risque associés à la naissance de bébés dont le poids est
inférieur à la norme (2,5 kg). Les données proviennent de 189 femmes, dont
59 ont accouché d'un enfant en sous-poids. Parmi les variables d'intérêt
figurent l'âge de la mère, le poids de la mère lors des dernières
menstruations, l'ethnicité de la mère et le nombre de visites médicales
durant le premier trimestre de grossesse.\autocite{hosmer89}
Les variables disponibles sont décrites comme suit : \texttt{low} (= 1 si
poids $<2.5$ kg, 0 sinon), \texttt{age} (années), \texttt{lwt} (poids de la
mère en livres), \texttt{race} (ethnicité codée en trois classes, 1 = white,
2 = black, 3 = other), \texttt{smoke} (= 1 si consommation de tabac durant
la grossesse, 0 sinon), \texttt{ptl} (nombre d'accouchements pré-terme
antérieurs), \texttt{ht} (= 1 si antécédent d'hypertension, 0 sinon),
\texttt{ui} (= 1 si manifestation d'irritabilité utérine, 0 sinon),
\texttt{ftv} (nombre de consultations chez le gynécologue durant le premier
trimestre de grossesse), \texttt{bwt} (poids des bébés à la naissance, en
\emph{g}).
\begin{description}
\item[(a)] Recoder les variables \texttt{low}, \texttt{race},
  \texttt{smoke}, \texttt{ui} et \texttt{ht} en variables
  qualitatives, avec des étiquettes ("labels") plus informatives.
\item[(b)] Convertir le poids des mères en \emph{kg}. Indiquer la moyenne, la
  médiane et l'intervalle inter-quartile. Représenter la distribution des
  poids sous forme d'histogramme.
\item[(c)] Indiquer la proportion de mères consommant du tabac durant la
  grossesse, avec un intervalle de confiance à 95~\%. Représenter les
  proportions (en \%) fumeur/non-fumeur sous forme d'un diagramme en
  barres.
\item[(d)] Recoder l'âge des mères en trois classes équilibrées (tercilage)
  et indiquer la proportion d'enfants dont le poids est $<2500$ \emph{g}
  pour chacune des trois classes.
\item[(e)] Construire un tableau d'effectifs ($n$ et \%) pour la variable
  ethnicité (\texttt{race}).  
\item[(f)] Décrire la distribution des variables \texttt{race},
  \texttt{smoke}, \texttt{ui}, \texttt{ht} et \texttt{age} après
  stratification sur la variable \texttt{low}.  
\end{description}
\begin{sol}
Les données fournies avec \R sont dans un format accessible à partir de la
commande \texttt{data}. Il est parfois nécessaire d'indiquer à \R dans quel
package trouver les données. Pour charger les données \texttt{birthwt}, on
utilisera donc
<<ex2-4a>>=
data(birthwt, package="MASS")
str(birthwt)
head(birthwt, 5)
@ 
\cmd{data}\cmd{str}\cmd{head}
Comme on peut le vérifier dans les sorties précédentes, l'ensemble des
variables est au format numérique. Pour recoder certaines des variables en
variables qualitatives, on utilisera la commande \texttt{factor}. 
<<ex2-4b>>=
yesno <- c("No","Yes")
ethn <- c("White","Black","Other")
birthwt <- within(birthwt, {
  low <- factor(low, labels=yesno)
  race <- factor(race, labels=ethn)
  smoke <- factor(smoke, labels=yesno)
  ui <- factor(ui, labels=yesno)
  ht <- factor(ht, labels=yesno)
})
@ 
\cmd{within}\cmd{factor}
La commande \texttt{within} utilisée permet simplement d'éviter de devoir
indiquer à chaque fois le nom du tableau de données (\texttt{data.frame}
dans la terminologie \R) suivi du nom de la variable, comme par exemple dans
les instructions suivantes :
<<ex2-4c, eval=FALSE>>=
birthwt$low <- factor(birthwt$low, labels=yesno)
birthwt$race <- factor(birthwt$race, labels=ethn)
@ 
\cmd{factor}
On verra plus tard une autre construction (\texttt{with}) permettant
d'éviter de saisir le nom du tableau de données à plusieurs reprises dans
une même commande. 

La conversion des poids des mères en \emph{kg} ne pose pas vraiment de
problème puisqu'il suffit de diviser les poids exprimés en livres par 2.2 :
<<ex2-4d>>=
birthwt$lwt <- birthwt$lwt/2.2
@ 
Notons que la transformation effectuée est définitive : il n'y a plus moyen
de retrouver les valeurs d'origine de la variable \texttt{lwt} dans le
\texttt{data.frame} \texttt{birthwt} (à moins de réimporter les
données). Dans le cas présent, ce n'est pas très important, mais en règle
générale il est préférable de créer de nouvelles variables.

On peut ensuite utiliser une commande telle que \texttt{summary} pour
obtenir les principaux indicateurs de tendance centrale et de forme de la
distribution (à partir de l'étendue et des quartiles).
<<ex2-4e>>=
summary(birthwt$lwt)
@ %$ 
\cmd{summary}
La médiane et la moyenne sont de 59 et 55 \emph{kg},
respectivement. L'intervalle inter-quartile se calcule aisément comme la
différence entre le 3\ieme (63.64) et le 1\ier quartile (50.00), mais on
peut également utiliser la commande \texttt{IQR} :
<<ex2-4f>>=
IQR(birthwt$lwt)
@ 
\cmd{IQR}
De manière générale, les quartiles peuvent être retrouvés ainsi :
<<ex2-4g>>=
quantile(birthwt$lwt)
@ 
\cmd{quantile}
d'où une autre manière de calculer l'intervalle inter-quartile,
<<ex2-4h>>=
lwt.quartiles <- quantile(birthwt$lwt)
lwt.quartiles
lwt.quartiles[4] - lwt.quartiles[2]
@ %$
\cmd{quantile}

La distribution des poids est indiquée dans l'histogramme suivant, en
utilisant la commande suivante :
<<ex2-4i, fig=TRUE>>=
histogram(~ lwt, data=birthwt, type="count", xlab="Poids de la mère (kg)", ylab="Effectifs")
@ 
\cmd{histogram}

La proportion de mères ayant fumé durant leur grossesse est obtenue à partir
d'un simple tri à plat de la variable \texttt{smoke}.
<<ex2-4j>>=
table(birthwt$smoke)
round(prop.table(table(birthwt$smoke))*100, 1)
@ 
\cmd{table}\cmd{round}\cmd{prop.table}
Pour obtenir une estimation de l'intervalle de confiance à 95~\% associé à
cette proportion empirique, on peut utiliser la commande \texttt{prop.test}
qui effectue également un test d'hypothèse sur l'égalité des proportions
estimées dans différents groupes. Cependant, la commande suivante ne renvoit
pas le résultat correct car, comme on peut le vérifier dans la proportion
qui est estimée dans ce cas, \R a travaillé sur la proportion de non-fumeurs.
<<ex2-4k>>=
prop.test(table(birthwt$smoke), correct=FALSE)
@ %$
\cmd{prop.test}
On peut donc utiliser directement la forme suivante :
<<ex2-4l>>=
prop.test(74, 189, correct=FALSE)
@ 
On retrouve bien la proportion estimée à l'étape précédente (\texttt{sample
  estimates}) (39.1~\%), ainsi que l'intervalle de confiance qui, ici, vaut
$[0.325;0.463]$. 

Un diagramme en barres représentant la proportion de fumeurs/non-fumeurs
dans cet échantillon est construit à partir de la commande \texttt{barchart}
comme proposé ci-après.
<<ex2-4m, fig=TRUE>>=
barchart(prop.table(table(birthwt$smoke))*100, horizontal=FALSE, 
         ylab="Proportion (%)", ylim=c(-5,105))
@ %$ 
\cmd{barchart}\cmd{prop.table}

Pour recoder l'âge en variable qualitative à partir des terciles, on
utilisera la commande \texttt{cut} :
<<ex2-4n>>=
birthwt$age.dec <- cut(birthwt$age, breaks=quantile(birthwt$age, c(0, 0.33, 0.66, 1)), 
                       include.lowest=FALSE)
table(birthwt$age.dec)
@ 
\cmd{cmd}\cmd{table}\cmd{quantile}
Le nombre d'enfants ayant un poids $<2.5$ \emph{kg} dans chacune des
classes est obtenue à partir d'un simple croisement des deux variables
\texttt{age.dec} et \texttt{low} :
<<ex2-4o>>=
with(birthwt, table(age.dec, low))
@ 
\cmd{with}\cmd{table}
Pour obtenir la fréquence relative des enfants de faible poids, il suffit de
diviser les effectifs précédents par les totaux colonnes, ce que l'on peut
réaliser ainsi avec \R :
<<ex2-4p>>=
tab <- with(birthwt, table(age.dec, low))
prop.table(tab, 2)[,"Yes"]
@ 
\cmd{prop.table}
La dernière commande permet d'isoler la colonne \texttt{Yes} du tableau
croisé.

Concernant l'ethnicité, on utilisera également la commande \texttt{table}
pour produire un tableau des effectifs, associée à \texttt{prop.table} pour
calculer les fréquences relatives, que l'on exprimera en \%.
<<ex2-4q>>=
res <- rbind(table(birthwt$race), prop.table(table(birthwt$race))*100)
rownames(res) <- c("n","%")
round(res, 2)
@ 
\cmd{rbind}\cmd{table}\cmd{prop.table}\cmd{rownames}\cmd{round}

Pour résumer la distribution des variables selon le facteur de
stratification, on pourrait très bien utiliser la commande \texttt{summary}
sur chacune des variables (\texttt{race}, \texttt{smoke}, \texttt{ui},
\texttt{ht} et \texttt{age}). Toutefois, il existe une manière plus
"économique" d'effectuer ce type de tâche avec la commande
\texttt{summary.formula} disponible dans le package \texttt{Hmisc}.
<<ex2-4r>>=
library(Hmisc)
summary(low ~ race + smoke + ui + ht + age, data=birthwt, method="reverse")
@ 
\cmd{library}\cmd{summary.formula}
Le principe général de cette commande est le suivant : on exprime à l'aide
d'une formule, selon la terminologie \R, la relation entre les variables ;
ici, décrire selon les modalités de la variable \texttt{low} les
distributions des autres variables indiquées dans la commande
(\texttt{table} pour les variables qualitatives et \texttt{quantile} pour
les variables numériques). Les commandes \texttt{tapply} (pour les variables
numériques) et \texttt{table} (pour les variables qualitatives) restent
toutefois applicables.
\end{sol}
\end{exo}
% 
% Tri à plat, graphique en barres
%
%% \begin{exo}\label{exo:2.5}
  
%% \end{exo}

\Closesolutionfile{solutions}


\chapter*{Devoir \no 1}
\addcontentsline{toc}{chapter}{Devoir \no 1}
  
Les exercices sont indépendants. Une seule réponse est correcte pour chaque
question. Lorsque vous ne savez pas répondre, cochez la case correspondante.

\section*{Exercice 1}
\noindent À partir des données sur les poids à la naissance, décrits à
l'exercice~\ref{exo:2.4} et que vous pouvez charger à l'aide de la commande
\verb|data(birthwt, package="MASS")|, veuillez indiquer les commandes
permettant de répondre aux questions suivantes :
\begin{description}
\item[\bf 1.1] \marginpar{\phantom{text}1.1 $\square$} Quel est le poids moyen des bébés
  dont la mère est âgée de 20 ans ou moins ?
  \begin{description}
  \item[A.] \verb|mean(birthwt$lwt & birthwt$age < 20)|
  \item[B.] \verb|mean(birthwt$lwt[birthwt$age <= 20])|
  \item[C.] \verb|mean(birthwt$lwt[age <= 20])|
  \item[D.] \verb|mean(birthwt$lwt[birthwt$age < 20])|
  \item[E.] Je ne sais pas.
  \end{description}
\item[\bf 1.2] \marginpar{\phantom{text}1.2 $\square$} Combien de bébés ont une mère ayant
  eu des antécédents d'hypertension ?
  \begin{description}
  \item[A.] \verb|table(birthwt$ht)[1]|
  \item[B.] \verb|nrow(birthwt$ht == 1)|
  \item[C.] \verb|sum(birtwht$ht == 1)|
  \item[D.] \verb|birthwt$ht[birthwt$low == 1]|
  \item[E.] Je ne sais pas.
  \end{description}
\item[\bf 1.3] \marginpar{\phantom{text}1.3 $\square$} On souhaite afficher la
  distribution des valeurs prises par la variable \texttt{ftv}, sous forme
  de pourcentage. Quelle commande est la plus appropriée ?
  \begin{description}
  \item[A.] \verb|barchart(ftv, data=birthwt, type="percent")|
  \item[B.] \verb|barchart(ftv, data=birthwt, type="%")|
  \item[C.] \verb|histogram(prop.table(table(birthwt$ftv))*100)|
  \item[D.] \verb|barchart(prop.table(table(birthwt$ftv))*100)|
  \item[E.] Je ne sais pas.    
  \end{description}  
\end{description}

Les sorties suivantes indiquent les résultats de différentes
procédures inférentielles (estimation ou test). Quelles commandes ont été
utilisées dans chacun des cas ? 
\begin{description}
\item[\bf 1.4] \marginpar{\phantom{text} 1.4 $\square$}
\begin{Verbatim}[frame=single]
	1-sample proportions test without continuity correction

data:  table(birthwt$ht), null probability 0.5 
X-squared = 144.0476, df = 1, p-value < 2.2e-16
alternative hypothesis: true p is not equal to 0.5 
95 percent confidence interval:
 0.8923149 0.9633103 
sample estimates:
        p 
0.9365079 
\end{Verbatim}
  \begin{description}
  \item[A.] \verb|bin.test(birthwt$ht)|
  \item[B.] \verb|prop.test(birthwt$ht)|
  \item[C.] \verb|prop.test(table(birthwt$ht))|
  \item[D.] \verb|prop.test(table(birthwt$ht), correct=FALSE)|
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 1.5] \marginpar{\phantom{text} 1.5 $\square$}
\begin{Verbatim}[frame=single]
  	One Sample t-test

data:  birthwt$bwt 
t = 55.5137, df = 188, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0 
95 percent confidence interval:
 2839.952 3049.222 
sample estimates:
mean of x 
 2944.587 
\end{Verbatim}
  \begin{description}
  \item[A.] \verb|T.test(birthwt$bwt)|
  \item[B.] \verb|t.test(birthwt$bwt, alternative=0)|
  \item[C.] \verb|t.test(bwt, data=birthwt)|
  \item[D.] \verb|t.test(birthwt$bwt, correct=FALSE)|
  \item[E.] Je ne sais pas.
  \end{description}  
\end{description}

\begin{description}
\item[\bf 1.6] \marginpar{\phantom{text}1.6 $\square$} Soit l'histogramme de fréquences
  suivant :
<<exo1-1-6, echo=FALSE, fig=TRUE>>=
histogram(~ bwt, data=birthwt, type="percent", xlab="Poids des bébés (kg)")
@

Quelle commande permet de reproduire cette figure (indépendemment du rapport
largeur/hauteur ou de la couleur) ?
\begin{description}
\item[A.] \verb|histogram(~ bwt/nrow(birthwt), data=birthwt, xlab="Poids des bébés (kg)")|
\item[B.] \verb|histogram(~ bwt, data=birthwt, type="density", xlab="Poids des bébés (kg)")|
\item[C.] \verb|histogram(~ bwt, data=birthwt, type="percent", xlab="Poids des bébés (kg)")|
\item[D.] \verb|histogram(~ bwt/1000, data=birthwt, type="percent", xlab="Poids des bébés (kg)")|
\item[E.] Je ne sais pas.
\end{description}
\end{description}

\section*{Exercice 2}
Soit les deux séries de mesures, $X_1$ et $X_2$, que l'on
supposera indépendantes :
\vskip1em

\begin{tabular}{l|rrrrrrrrrrrrrrr}
\texttt{x1} & 17.5 & 7.7 &16.5 &18.4 & 3.2 & 5.8 &13.5 &13.2 &12.0 & 7.8 &11.2 &13.2 & 4.1 & 2.4 & 8.0\\
\hline
\texttt{x2} & 18.6 &10.5 &15.0 & 3.0 &18.9& 13.1&  8.6& 16.9& 14.7 & 1.5 &17.6& 15.4& 17.9& 19.1&  9.8
\end{tabular}
\vskip1em

\begin{description}
\item[\bf 2.1] \marginpar{\phantom{text}2.1 $\square$} Quelles sont les moyennes de $X_1$
  et $X_2$ ?  
  \begin{description}
  \item[A.] \verb|mean(x1,x2)|
  \item[B.] \verb|c(mean(x1),mean(x2))|
  \item[C.] \verb|mean(c(x1,x2))|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 2.2] \marginpar{\phantom{text}2.2 $\square$} Quelle est l'intervalle
  inter-quartile de $X_1-X_2$ ?
  \begin{description}
  \item[A.] \verb|iqr(x1-x2)|
  \item[B.] \verb|sum(x1-x2)/14|
  \item[C.] \verb|diff(median(c(x1,x2)))|
  \item[D.] \verb|diff(quantile(x1-x2, c(.25,.75)))| 
  \item[E.] Je ne sais pas.
  \end{description}
\item[\bf 2.3] \marginpar{\phantom{text}2.3 $\square$} Donner un intervalle de confiance à
  95~\% pour la différence $X_1-X_2$ (en utilisant une loi de Student).
  \begin{description}
  \item[A.] \verb|t.test(x1,x2)|
  \item[B.] \verb|(x1-x2) + c(-1,1) * qt(0.975, 28)*sd(x1-x2)/sqrt(length(c(x1,x2)))|
  \item[C.] \verb|(x1-x2) + c(-1,1) * qt(0.975, 28)*sd(x1-x2)/sqrt(length(c(x1,x2)))/2-2|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 2.4] \marginpar{\phantom{text}2.4 $\square$} On suppose que ces deux variables
  numériques ont été converties en variables qualitatives à 4 classes
  équilibrées de la manière suivante :
\begin{verbatim}
> x1b <- cut(x1, breaks=quantile(x1), include.lowest=TRUE)
> x2b <- cut(x2, breaks=quantile(x2), include.lowest=TRUE) 
\end{verbatim}  
  Indiquer la ou les classes modales dans chacun des deux cas. 
  \begin{description}
  \item[A.] \verb|which.max(table(x1b)); which.max(table(x2b))|
  \item[B.] \verb|max(table(x1b, x2b))|
  \item[C.] \verb|which.max(table(x1b, x2b))|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 2.5] \marginpar{\phantom{text}2.5 $\square$} Soit la série de commandes
  suivantes :  
\begin{verbatim}
> x3 <- factor((x1-x2)>0, labels=c("+","-"))
> table(x3)["+"]
\end{verbatim}
  Quel est le résultat renvoyé ?
\begin{description}
\item[A.] La somme des différences $X_1-X_2$ positives.
\item[B.] Le nombre de différences $X_1-X_2$ positives.
\item[C.] La proportion de valeurs de $X_1$ supérieures à celles de $X_2$.
\item[D.] Je ne sais pas.
\end{description}
\end{description}

\section*{Exercice 3}
Soit les données arrangées sous forme de tableau comme indiqué ci-dessous :
\vskip1em

\begin{tabular}{cccc}
\toprule  
id & age & sexe & taille \\  
\midrule
01 & 23 &  G  & 172 \\
02 & 22.5  & G & 178 \\
03 & 22.5 & F & 159 \\
04 & . & G & 182 \\
05 & 22 & F & 154 \\
06 & 21.5 & F & 161 \\
07 & 22 & G & 176 \\
08 & 21.5 & G & 191 \\
09 & 22.5 & F & 1.60 \\
10 & 23 & G & 173\\
\bottomrule
\end{tabular}
\vskip1em

Les données manquantes sont indiquées à l'aide du symbole \verb|.| (point),
le sexe des étudiants est codé \texttt{G} (garçons) ou \texttt{F} (filles),
et les tailles sont exprimées en \emph{cm}. Le tableau a été enregistré sous
\R sous le nom \texttt{dfrm}, et le résultat de la commande \verb|str(dfrm)|
est fourni ci-dessous :
<<exo1-3-1a, echo=FALSE>>=
dfrm <- data.frame(id=ifelse(1:10<10, paste("0", 1:10, sep=""), 1:10),
                   age=c(23,22.5,22.5,NA,22,21.5,22,21.5,22.5,23),
                   sexe=c("G","G","F","G","F","F","G","G","F","G"),
                   taille=c(172,178,159,182,154,161,176,191,1.6,173))
@ 
<<exo1-3-1b>>=
str(dfrm)
@ 
\begin{description}
\item[\bf 3.1] \marginpar{\phantom{text}3.1 $\square$} La commande \verb|mean(dfrm$age)|
  fournit l'âge moyen des étudiants.
  \begin{description}
  \item[A.] Vrai.
  \item[B.] Faux.
  \item[C.] Je ne sais pas.
  \end{description}
\item[\bf 3.2] \marginpar{\phantom{text}3.2 $\square$} On veut remplacer l'âge manquant
  par l'âge moyen. Laquelle parmi ces commandes est correcte ?
  \begin{description}
  \item[A.] \verb|dfrm$age[4] <- mean(dfrm$age)|
  \item[B.] \verb|dfrm$age[is.na(dfrm$age)] <- mean(dfrm$age, na.rm=TRUE)|
  \item[C.] \verb|dfrm$age[dfrm$id == "04"] <- mean(dfrm$age)|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 3.3] \marginpar{\phantom{text}3.3 $\square$} On souhaite afficher la proportion
  de garçons et de filles sous forme d'un diagramme en barres. La commande
  \verb|barchart(sexe, data=dfrm)| est-elle correcte ?
  \begin{description}
  \item[A.] Vrai.
  \item[B.] Faux.
  \item[C.] Je ne sais pas.
  \end{description}
\item[\bf 3.4] \marginpar{\phantom{text}3.4 $\square$} Pour afficher un histogramme des
  tailles des garçons, quelle commande faut-il utiliser ?
  \begin{description}
  \item[A.] \verb|histogram(taille, subset=sexe == "G")|
  \item[B.] \verb|barchart(~ taille, data=subset(dfrm, sexe=="G"))|
  \item[C.] \verb|histogram(taille, data=dfrm, subset=sexe == "G")|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 3.5] \marginpar{\phantom{text}3.5 $\square$} Que fait la commande suivante ? 
\begin{verbatim}
with(subset(dfrm, sexe=="F"), mean(age) + c(-1,1) * qt(0.95, 3) * sd(age)/sqrt(4))
\end{verbatim}
  \begin{description}
  \item[A.] Elle fournit un intervalle de confiance à 95~\% pour l'âge moyen
    chez les filles.
  \item[B.] Elle fournit un intervalle de confiance à 90~\% pour l'âge moyen
    chez les filles.
  \item[C.] Je ne sais pas.
  \end{description}
\end{description}
  
  


%--------------------------------------------------------------- Chapter 03 --
\chapter{Comparaisons de deux variables}\label{chap:comparaisons}
\Opensolutionfile{solutions}[solutions3]


\section*{Énoncés}
%
% Everitt 2001 p. 64
%
\begin{exo}\label{exo:3.1}
On dispose des poids à la naissance d'un échantillon de 50 enfants
présentant un syndrôme de détresse respiratoire idiopathique aïgue. Ce type
de maladie peut entraîner la mort et on a observé 27 décès chez ces
enfants. Les données sont résumées dans le tableau ci-dessous et sont
disponibles dans le fichier \texttt{sirds.dat}, où les 27 premières
observations correspondent au groupe des enfants décédés au moment de
l'étude. \autocite[p.~64]{everitt01}
\vskip1em

\begin{tabular}{ll}
\toprule  
Enfants décédés &
1.050\; 1.175\; 1.230\; 1.310\; 1.500\; 1.600\; 1.720\; 1.750\; 1.770\; 2.275\; 2.500\; 1.030\; 1.100\; 1.185 \\
& 1.225\; 1.262\; 1.295\; 1.300\; 1.550\; 1.820\; 1.890\; 1.940\; 2.200\; 2.270\; 2.440\; 2.560\; 2.730 \\
Enfants vivants &
1.130\; 1.575\; 1.680\; 1.760\; 1.930\; 2.015\; 2.090\; 2.600\; 2.700\; 2.950\; 3.160\; 3.400\; 3.640\; 2.830 \\
& 1.410\; 1.715\; 1.720\; 2.040\; 2.200\; 2.400\; 2.550\; 2.570\; 3.005 \\
\bottomrule
\end{tabular}
\vskip1em

Un chercheur s'intéresse à l'existence éventuelle d'une différence entre le
poids moyen des enfants ayant survécu et celui des enfants décédés des
suites de la maladie. 
\begin{description}
\item[(a)] Réaliser un test $t$ de Student. Peut-on rejeter l'hypothèse nulle
  d'absence de différences entre les deux groupes d'enfants ? 
\item[(b)] Vérifier graphiquement que les conditions d'applications du test
(normalité et homogénéité des variances) sont vérifiées. 
\item[(c)] Quel est l'intervalle de confiance à 95~\% pour la différence de
  moyenne observée ?
\end{description}
\begin{sol}
Pour le chargement des données, on lit séparément les données numériques
contenues dans le fichier que l'on stocke dans une variable appelée
\texttt{poids}, et on crée une seconde variable codant pour le statut des
enfants au moment de l'étude.
<<ex3-1a>>=
poids <- scan("data/sirds.dat")
status <- factor(rep(c("décédé", "vivant"), c(27,23)))
@
\cmd{scan}\cmd{factor}
On réalise ensuite un test $t$ pour échantillons indépendants, en supposant
l'homogénéité des variances :
<<ex3-1b>>=
t.test(poids ~ status, var.equal=TRUE)
@ \cmd{t.test}
Le résultat du test, bilatéral par défaut (\verb|alternative="two.sided"|),
indique que l'on peut effectivement rejeter l'hypothèse nulle.

Pour vérifier l'hypothèse de normalité des populations parentes, on peut
représenter graphiquement la distribution des poids à la naissance pour
chacun des groupes à l'aide d'un histogramme. Par exemple,
<<ex3-1c, fig=TRUE>>=
histogram(~ poids | status, type="count")
@ 
\cmd{histogram}

Une alternative consiste à utiliser des graphiques de type quantile-quantile.

L'homogénéité des variances peut être évaluée à l'aide de diagrammes de type
boîtes à moustache :
<<ex3-1d, fig=TRUE>>=
bwplot(poids ~ status, xlab="status")
@ 
\cmd{bwplot}
\end{sol}
\end{exo}
%
% data(sleep)
%
\begin{exo}\label{exo:3.2}
La qualité de sommeil de 10 patients a été mesurée avant (contrôle) et après
traitement par un des deux hypnotiques suivants : (1) D. hyoscyamine
hydrobromide et (2) L. hyoscyamine hydrobromide. Le critère de jugement
retenu par les chercheurs était le gain moyen de sommeil (en heures) par
rapport à la durée de sommeil de base
(contrôle). \autocite[p.~20]{student08} Les données sont reportées
ci-dessous et figurent également parmi les jeux de données de base de R
(\verb|data(sleep)|).  
\begin{verbatim}
D. hyoscyamine hydrobromide :
0.7 -1.6 -0.2 -1.2 -0.1  3.4  3.7  0.8  0.0  2.0
L. hyoscyamine hydrobromide :
1.9  0.8  1.1  0.1 -0.1  4.4  5.5  1.6  4.6  3.4
\end{verbatim}

Les chercheurs ont conclu que seule la deuxième molécule avait réellement un
effet soporifique. 
\begin{description}
\item[(a)] Estimer le temps moyen de sommeil pour chacune des deux
  molécules, ainsi que la différence entre ces deux moyennes.
\item[(b)] Afficher la distribution des scores de différence (LHH - DHH)
  sous forme d'un histogramme, en considérant des intervalles de classe
  d'une demi-heure, et indiquer la moyenne et l'écart-type de ces scores de
  différence.
\item[(c)] Vérifier l'exactitude des conclusions à l'aide d'un test de Student.
\end{description}
\begin{sol}
On notera qu'il s'agit des mêmes patients qui sont testés dans les deux
conditions (le sujet est pris comme son propre témoin). L'aide en ligne pour
le jeu de données \texttt{sleep} indique bien que les deux premières
variables, \texttt{extra} (différentiel de temps de sommeil) et
\texttt{group} (type de médicament), sont les variables d'intérêt.
<<ex3-2a>>=
data(sleep)
mean(sleep$extra[sleep$group == 1])  # D. hyoscyamine hydrobromide
mean(sleep$extra[sleep$group == 2])  # L. hyoscyamine hydrobromide
m <- with(sleep, tapply(extra, group, mean))
m[2] - m[1]
@
\cmd{data}\cmd{mean}\cmd{with}\cmd{tapply}
Les commandes ci-dessus permettent de vérifier les moyennes de groupe et la
différence de gain de sommeil entre les molécules L. hyoscyamine
hydrobromide et D. hyoscyamine hydrobromide, en gardant à l'esprit que la
commande 
\begin{verbatim}
with(sleep, tapply(extra, group, mean)) 
\end{verbatim}
renvoit deux valeurs (les moyennes par type de molécule) et que la
différence deux valeurs stockées dans la variable auxiliaire \texttt{m}
renvoit bien la différence de moyennes entre les deux traitements.

Pour calculer les scores de différences, on procèdera comme dans
l'exercice~1.5 (p.~\pageref{exo:1.5}), soit
<<ex3-2b>>=
sdif <- sleep$extra[sleep$group == 2] - sleep$extra[sleep$group == 1]
c(mean=mean(sdif), sd=sd(sdif))
@ 
\cmd{mean}
Le calcul de la moyenne et de l'écart-type de ces scores de différences ne
pose pas de difficulté particulière, pas plus que l'affichage de leur
distribution sous forme d'histogramme.
<<ex3-2c, fig=TRUE>>=
histogram(~ sdif, breaks=seq(0, 5, by=0.5), xlab="LHH - DHH")
@ 
\cmd{histogram}

Le résultat du test $t$ est obtenu à partir de la commande \texttt{t.test}
en fournissant la variable réponse et le facteur de classification sous
forme d'une formule (variable réponse à gauche, variable explicative à
droite). Le résultat du test est reporté ci-dessous :
<<ex3-2d>>=
t.test(extra ~ group, data=sleep, paired=TRUE)
@ 
\cmd{t.test}
On notera qu'il est nécessaire d'indiquer à R où trouver les variables
d'intérêt, d'où l'usage de \verb|data=sleep|. L'option \verb|paired=TRUE|
est nécessaire pour rendre compte de l'appariement des observations. Notons
également que, par défaut, R présente la différence de moyennes entre le
premier et le second traitement, et non le second moins le premier comme
calculé précédemment.

Le résultat significatif du test et le sens de la différence de moyenne
observée (au niveau du gain horaire de sommeil) est bien en accord avec les
conclusions des chercheurs. On peut visualiser les résultats sous forme d'un
diagramme en barres :
<<3-2e, fig=TRUE>>=
barchart(with(sleep, tapply(extra, group, mean)))
@ 
\cmd{barchart}
\end{sol}
\end{exo}
%
% Hollander 1999 p. 29
%
\begin{exo}\label{exo:3.3}
Dans une étude clinique, des chercheurs se sont intéressés à l'effet d'une
certaine forme de thérapie (par administration de tranquilisants) sur
l'évolution de 9 patients souffrant d'un trouble mixte combinant anxiété et
dépression. Le niveau de dépression était mesuré à partir de l'échelle de
dépression de Hamilton à l'inclusion (première visite) et lors d'une seconde
visite (après traitement). Il s'agit d'une échelle composé de 21 items à
plusieurs modalités de réponse ordonnées à partir de laquelle on peut
calculer un score total ou moyen. Les données recueillies sont indiquées
ci-dessous :\autocite[p.~29]{hollander99}
\vskip1em

\begin{verbatim}
T0 :
1.83 0.50 1.62 2.48 1.68 1.88 1.55 3.06 1.30
T1 : 
0.878 0.647 0.598 2.050 1.060 1.290 1.060 3.140 1.290
\end{verbatim}
\vskip1em

On cherche à démontrer que le traitement a bien un effet se traduisant par
une diminution des scores moyens individuels. Pour cela, on se propose de
réaliser un test non-paramétrique.
\begin{description}
\item[(a)] Représenter la distribution des scores sous forme
  d'un diagramme de dispersion (en abscisses, données à la 1\iere\ visite ; en
  ordonnées, données à la 2\ieme\ visite).
\item[(b)] Effectuer un test de Wilcoxon.
\end{description}
\begin{sol}
Pour la saisie de données, on crée deux variables quantitatives représentant
les mesures individuelles collectées à la 1\iere\ et à la 2\ieme\ visite :
<<ex3-3a>>=
occ1 <- c(1.83,0.50,1.62,2.48,1.68,1.88,1.55,3.06,1.30)
occ2 <- c(0.878,0.647,0.598,2.05,1.06,1.29,1.06,3.14,1.29)
@   

L'affichage graphique des deux séries de mesure ne pose pas de difficulté :
<<ex3-3b, fig=TRUE>>=
xyplot(occ2 ~ occ1, type=c("p","g"), xlab="Première visite", ylab="Seconde visite")
@ 
\cmd{xyplot}

Pour faciliter la lisibilité, on pourrait rajouter une droite de pente 1. On
se contentera de superposer une grille pour faciliter la comparaison des
mesures avant-après (\verb|type="g"|).

Les données étant appariées (les mêmes patients sont mesurés à deux
occasions), il faut le préciser à R à l'aide de l'option \verb|paired=TRUE|.   
<<ex3-3c>>=
wilcox.test(occ1, occ2, paired=TRUE)
@   
\cmd{wilcox.test}
Le résultat du test suggère une amélioration de l'état (diminution du
score Hamilton de \Sexpr{round(mean(occ2-occ1),2)} points en moyenne).
\end{sol}
\end{exo}
%
% Selvin 1998 p. 323
%
\begin{exo}\label{exo:3.4}
Dans un essai clinique, on a cherché à évaluer un régime supposé réduire le
nombre de symptômes associé à une maladie bénigne du sein. Un groupe de 229
femmes ayant cette maladie ont été alétoirement réparties en deux
groupes. Le premier groupe a reçu les soins courants, tandis que les
patientes du second groupe suivaient un régime spécial (variable B =
traitement). Après un an, les individus ont été évalués et ont été classés
dans l'une des deux catégories : amélioration ou pas d'amélioration
(variable A = réponse). Les résultats sont résumés dans le tableau suivant,
pour une partie de l'échantillon :\autocite[p.~323]{selvin98}
\vskip1em

\begin{tabular}{l|cc|r}
& régime & pas de régime & total \\
\hline
amélioration & 26 & 21 & 47 \\
pas d'amélioration & 38 & 44 & 82 \\
\hline
total & 64 & 65 & 129
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Réaliser un test du chi-deux.  
\item[(b)] Quels sont les effectifs théoriques attendus sous une hypothèse
  d'indépendance ?
\item[(c)] Comparer les résultats obtenus en (a) avec ceux d'un test de
  Fisher.
\item[(d)] Donner un intervalle de confiance pour la différence de
  proportion d'amélioration entre les deux groupes de patientes.
\end{description}
\begin{sol}
On ne dispose que des données concernant les effectifs tels que reportés
dans le tableau, mais il n'est pas nécessaire d'avoir les données brutes
pour réaliser le test du $\chi^2$. Celui-ci est obtenu à partir de la
commande \texttt{chisq.test} et inclut par défaut une correction de
continuité (Yates).
<<ex3-4a>>=
regime <- matrix(c(26,38,21,44), nrow=2)
dimnames(regime) <- list(c("amélioration","pas d'amélioration"), c("régime","pas de régime"))
regime
chisq.test(regime)
@ 
\cmd{matrix}\cmd{dimnames}\cmd{list}\cmd{chisq.test}
Si l'on ne souhaite pas appliquer de correction de continuité, il faut
ajouter l'option \verb|correct=FALSE|.

Les effectifs théoriques ne sont pas affichés avec le résultat du test, mais
on peut les obtenir comme suit :
<<ex3-4b>>=
chisq.test(regime)$expected
@ %$

Concernant le test de Fisher, on procède de la même manière en utilisant la
comamnde \texttt{fisher.test} à partir du tableau de contingence.
<<ex3-4c>>=
fisher.test(regime)
@ 
% FIXME:
% finir commentaires
\end{sol}
\end{exo}
%
% Agresti 2002 p. 72
%
\begin{exo}\label{exo:3.5}
Dans un essai clinique, 1360 patients ayant déjà eu un infarctus dy myocarde
ont été assignés à l'un des deux groupes de traitement suivants : prise en
charge par aspirine à faible dose en une seule prise \emph{versus}
placebo. La table ci-après indique le nombre de décès par infarctus lors de
la période de suivi de trois ans :\autocite[p.~72]{agresti02} 
\vskip1em

\begin{tabular}{lccc}
\toprule
& \multicolumn{2}{c}{Infarctus} & \\
\cmidrule(r){2-3}
& Oui & Non & Total \\
\midrule
Placebo & 28 & 656 & 684 \\
Aspirine & 18 & 658 & 676 \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Calculer la proportion d'infarctus du myocarde dans les deux
  groupes de patients.
\item[(b)] Représenter graphiquement le tableau précédent sous forme d'un
  diagramme en barres ou d'un diagramme en points ("dotplot" de Cleveland).
\item[(c)] Indiquer la valeur de l'odds-ratio ainsi que du risque
  relatif. Pour l'odds-ratio, on considérera comme catégories de référence
  les modalités représentées par la première ligne et la première colonne
  du tableau. 
\item[(d)] À partir de l'intervalle de confiance à 95~\% pour l'odds, quelle
  conclusion peut-on tirer sur l'effet de l'aspirine dans la prévention d'un
  infarctus du myocarde ?
\end{description}
\begin{sol}
On travaillera à partir du tableau de contingence directement (aucune des
questions posées ne nécessite d'avoir accès aux données individuelles).
<<ex3-5a>>=
aspirine <- matrix(c(28,18,656,658), nrow=2) 
dimnames(aspirine) <- list(c("Placebo","Aspirine"), c("Oui","Non"))
aspirine
@ 
\cmd{matrix}\cmd{dimnames}\cmd{list}
On retiendra que par défaut R organise les données en colonnes (sauf si l'on
précise l'option \verb|byrow=TRUE|), d'où la saisie des effectifs selon la
présence ou non d'un infarctus (cf. exercice~\ref{exo:3.4}). Les effectifs
marginaux (totaux lignes et colonnes) s'obtiennent simplement en calculant
les sommes des cellules correspondantes. Par exemple, pour reproduire le
premier total ligne du tableau donné, on pourrait procéder ainsi :
<<ex3-5b>>=
sum(aspirine["Placebo",])
@ 
\cmd{sum}
ou, de manière équivalente, \verb|sum(aspirine[1,])|. Pour obtenir
simultanément les deux totaux lignes, sans répéter la commande précédente,
on peut utiliser la commande \texttt{apply}. Par exemple, pour les totaux
lignes 
<<ex3-5c>>=
apply(aspirine, 1, sum)
@
\cmd{apply}
Dans cette expression, l'option \texttt{1} signifie que l'on souhaite
travailler "par lignes". Pour obtenir les totaux colonnes, on utilisera
<<ex3-5d>>=
apply(aspirine, 2, sum)
@ 
\cmd{apply}
D'où les proportions demandées :
<<ex3-5e>>=
round(aspirine[,"Oui"]/sum(aspirine[,"Oui"]) * 100, 2)
@ 
\cmd{round}

La distribution des effectifs peut être visualisée sous forme de diagramme
en barres comme ceci :
<<ex3-5f, fig=TRUE>>=
barchart(aspirine, horizontal=FALSE, stack=FALSE, ylab="Effectifs")
@ 
\cmd{barchart}

En fait, cette représentation graphique n'est pas très informative car la
prévalence observée est très faible dans les deux groupes de traitement. Une
représentation des données plus appropriée consisterait à afficher les
événements positifs (survenue d'un infarctus pendant la période de suivi)
rapportés aux effectifs marginaux (nombre total de patients randomisés dans
chaque groupe). Une solution possible est indiquée ci-après :
<<ex3-5g, fig=TRUE>>=
prop.infarctus <- aspirine[,"Oui"]/sum(aspirine[,"Oui"])
barchart(prop.infarctus, ylab="Fréquence relative d'infarctus")
@ 
\cmd{barchart}\cmd{sum}

On peut calculer l'odds-ratio et le risque relatif comme on l'a fait plus
haut, c'est-à-dire en travaillant directement avec les valeurs des cellules
extraites du tableau \texttt{aspirine}. Par exemple, pour le risque relatif
<<ex3-5h>>=
(aspirine["Placebo","Oui"]/sum(aspirine["Placebo",])) / (aspirine["Aspirine","Oui"]/sum(aspirine["Aspirine",]))
@ 
Toutefois, il existe des librairies spécialisées pour effectuer ce genre de
calcul, et fournir une estimation de la précision de ces estimateurs sous
forme d'intervalles de confiance. Pour calculer l'odds-ratio, nous
utiliserons donc la commande disponible dans le package \texttt{vcd}, qu'il
est nécessaire de charger au préalable à l'aide de la commande
\texttt{library} : 
<<ex3-5i>>=
library(vcd)
asp.or <- oddsratio(aspirine, log=FALSE) 
print(list(or=asp.or , conf.int=confint(asp.or))) 
summary(oddsratio(aspirine))
@
\cmd{oddsratio}\cmd{vcd}\cmd{confint}
L'estimé du "vrai" OR est plutôt imprécis (l'IC à 95~\% est large et
contient la valeur 1), et le test sur le $\log(\text{OR})$ n'est pas
significatif ($p = 0.071$). On concluera que rien ne permet d'affirmer que
la probabilité de décès dûe à un infarctus diffère selon le facteur
d'exposition.
\end{sol}
\end{exo}
%
% Peat 2005 p. 235
%
\begin{exo}\label{exo:3.6}
Une étude a porté sur 86 enfants suivis dans un centre pour apprendre à
gérer leur maladie. Lors de leur arrivée, on a demandé aux enfants s'ils
savaient gérer leur maladie dans les conditions optimales (non détaillées),
c'est-à-dire s'ils savaient à quel moment il leur fallait recourir au
traitement prescrit. La même question a été posée à ces enfants à la fin de
leur suivi dans le centre. La variable mesurée est la réponse (affirmative
ou non) à cette question à l'entrée et à la sortie de l'étude. Les données,
disponibles au format SPSS dans le fichier \texttt{health-camp.sav}, sont
résumées dans le tableau ci-après :\autocite[p.~235]{peat05}  
\vskip1em

\begin{tabular}{lll...}
\toprule
& & & \multicolumn{2}{c}{Gestion maladie (sortie)} &  \\
\cmidrule(r){4-5}
& & & \multicolumn{1}{c}{Non} & \multicolumn{1}{c}{Oui} & Total \\
\midrule
Gestion maladie (entrée) & Non & Effectif & 27 & 29 & 56 \\
& & Fréquence & 31.4~\% & 33.7~\% & 65.1~\% \\
& Oui & Effectif & 6 & 24 & 30 \\
& & Fréquence & 7.0~\% & 27.9~\% & 34.9~\% \\
Total & & Effectif & 33 & 53 & 86 \\
& & Fréquence & 38.4~\% & 61.6~\% & 100.0~\% \\
\bottomrule
\end{tabular}
\vskip1em

On se demande si le fait d'avoir suivi le programme de formation proposé
dans le centre a augmenté le nombre d'enfants ayant une bonne connaissance
de leur maladie et de sa gestion quotidienne.
\begin{description}
\item[(a)] Reproduire le tableau d'effectifs et de fréquences relatives
  précédent à partir des données brutes.
\item[(b)] Indiquer le résultat d'un test de McNemar.
\item[(c)] Comparer le résultat du test réalisé en (b) mais sans continuité
  de correction avec le résultat obtenu à partir d'un test binomial.
\end{description}
\begin{sol}
Il n'est pas vraiment nécessaire d'importer le tableau de données brutes et
l'on pourrait se contenter de travailler avec les effectifs présentés dans
le tableau, comme dans l'exercice~\ref{exo:3.5}. Toutefois, pour charger le
fichier SPSS, il est nécessaire d'utiliser une commande \R spécifique et
disponible dans le package \texttt{foreign}.
<<ex3-6a>>=
library(foreign)
hc <- read.spss("data/health-camp.sav", to.data.frame=TRUE)
@ 
\cmd{foreign}\cmd{read.spss}
L'option \verb|to.data.frame=TRUE| est importante car c'est celle qui assure
qu'à l'issue de l'importation les données seront bien stockées dans un
tableau où les lignes figurent les observations et les colonnes les
variables. Si l'on regarde comment les données sont représentées dans le
tableau importé, on constate que l'information est un peu vague :
<<ex3-6b>>=
head(hc)
@ 
\cmd{head}
Ceci s'explique par la perte des labels associés aux variables dans SPSS. On
peut retrouver cette informaiton en interrogeant la base de données \R avec
la commande \texttt{str} :
<<ex3-6c>>=
str(hc)
@ 
\cmd{str}
À partir de la ligne \verb|attr(*, "variable.labels")|, on voit donc que les
colonnes \texttt{BEFORE} et \texttt{AFTER} correspondent à la question
portant sur la connaissance de la maladie, alors que les colonnes
\texttt{BEFORE2} et \texttt{AFTER2} correspondent à la question portant sur
le traitement.

Finalement, on peut reproduire le tableau initial (effectifs et fréquences
relatives) ainsi :
<<ex3-6d>>=
table(hc[,c("BEFORE","AFTER")])
round(prop.table(table(hc[,c("BEFORE","AFTER")])), 2)
@ 
\cmd{table}\cmd{prop.table}
Pour obtenir les distributions marginales, on peut utiliser la commande
\texttt{margin.table}. 
<<ex3-6e>>=
margin.table(table(hc[,c("BEFORE","AFTER")]), 1)
margin.table(table(hc[,c("BEFORE","AFTER")]), 2)
@ 
\cmd{margin.table}

Pour réaliser le test de McNemar, on peut utiliser directement le tableau de
contingence construit précédemment. Par souci de commodité, on stockera ce
dernier dans une variable \R appelé \texttt{hc.tab}.
<<ex3-6f>>=
hc.tab <- table(hc[,c("BEFORE","AFTER")])
mcnemar.test(hc.tab)
@

On peut comparer les résultats du test de McNemar sans appliquer la
correction de continuité avec ceux d'un test binomial (test exact).
<<ex3-6g>>=
binom.test(6, 6+29)
mcnemar.test(hc.tab, correct=FALSE)
@ 
\end{sol}
\end{exo}

\Closesolutionfile{solutions}

%--------------------------------------------------------------- Devoir 02 ---
\chapter*{Devoir \no 2}
\addcontentsline{toc}{chapter}{Devoir \no 2}

Les exercices sont indépendants. Une seule réponse est correcte pour chaque
question. Lorsque vous ne savez pas répondre, cochez la case correspondante.

\section*{Exercice 1}
Voici des données anthropométriques (poids, taille et périmètre crânien)
recueillies sur deux groupes de bébés (119 garçons, 137 filles) à leur
naissance. Un bref résumé des principaux indicateurs descriptifs pour ces
trois variables est fourni ci-après.
\vskip1em

\begin{tabular}{llrr}
\toprule
& & Garçons & Filles \\
\cmidrule(r){3-4}
& & (N=119) & (N=137) \\
\midrule
Poids (kg) & Moyenne & 3.44 & 3.53 \\
\texttt{BWEIGHT}           & Écart-type & 0.33 & 0.43 \\
           & Étendue & 2.70–4.43 & 2.71–4.72 \\
Taille (cm) & Moyenne & 50.33 & 50.28 \\
\texttt{BLENGTH}            & Écart-type & 0.78 & 0.85 \\
            & Étendue & 49.0–51.5 & 49.0–52.0 \\
PC (cm) & Moyenne & 34.94 & 34.25 \\
\texttt{BHEADCIR}        & Écart-type & 1.31 & 1.38 \\
        & Étendue & 31.5–38.0 & 29.5–38.0 \\
\bottomrule
\end{tabular}
\vskip1em

Les données ont été enregistrés dans un fichier texte nommé
\texttt{babies.dat} dont un aperçu est fourni ci-dessous :
\begin{verbatim}
ID BWEIGHT BLENGTH BHEADCIR GENDER
L002     3.09 50 33.5 1
L003     3.94 50 35 2
L006     3.2 49 36 1
L007     2.93 51 31.5 2
L017     3.16 49 34 1
L025     3.36 51 33 2
L034     4.24 50 33.5 2
\end{verbatim}

Veuillez indiquer les commandes permettant de répondre aux questions suivantes :
\begin{description}
\item[\bf 1.1] \marginpar{\phantom{text}1.1 $\square$} On souhaite importer les données
  sous R en utilisant la commande \texttt{read.table}. Quelles options
  faut-il renseigner ?
  \begin{description}
  \item[A.] \verb|babies <- read.table("babies.dat")|
  \item[B.] \verb|babies <- read.table("babies.dat", row.names=FALSE)|
  \item[C.] \verb|babies <- read.table(header=TRUE, "babies.dat")|
  \item[D.] \verb|babies <- read.table("babies.dat", header=TRUE)|
  \item[E.] Je ne sais pas
  \end{description}
\item[\bf 1.2] \marginpar{\phantom{text}1.2 $\square$} Comment recoder la variable
  \texttt{GENDER} en variable qualitative avec les modalités \texttt{G} (=1)
  et \texttt{F} (=2) ?
  \begin{description}
  \item[A.] \verb|babies$GENDER <- factor(babies$GENDER, levels=0:1, labels=c("G","F"))||
  \item[B.] \verb|babies$GENDER <- factor(babies$GENDER, levels=c("G","F"))|
  \item[C.] \verb|babies$GENDER <- factor(babies$GENDER, labels=c("G","F"))|
  \item[D.] \verb|babies$GENDER <- as.factor(babies$GENDER, labels=c("G","F"))|
  \item[E.] Je ne sais pas.
  \end{description}
\item[\bf 1.3] \marginpar{\phantom{text}1.3 $\square$} La distribution des effectifs selon
  le sexe peut être obtenue de la manière suivante
  \begin{description}
  \item[A.] \verb|table(GENDER, data=babies)|
  \item[B.] \verb|with(babies, table(~ GENDER))|
  \item[C.] \verb|margin.table(babies$GENDER)|
  \item[D.] \verb|table(babies$GENDER)|
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 1.4] \marginpar{\phantom{text}1.4 $\square$} Le poids moyen des bébés à la
  naissance, exprimé en grammes, chez les sujets des deux sexes peut être
  obtenu de la manière suivante
  \begin{description}
  \item[A.] \verb|tapply(BWEIGHT ~ GENDER, data=babies, FUN=mean)|
  \item[B.] \verb|with(babies, tapply(BWEIGHT, GENDER, mean))|
  \item[C.] \verb|with(babies, tapply(GENDER, BWEIGHT, mean))|
  \item[D.] \verb|by(babies$GENDER, babies$BWEIGHT, mean)|
  \item[E.] Je ne sais pas.
  \end{description}  
\end{description}

\begin{description}
\item[\bf 1.5] \marginpar{\phantom{text}1.5 $\square$} Quelle commande permet de
  reproduire la figure suivante (indépendemment du rapport largeur/hauteur
  et de la couleur) ?
\begin{center}
  \includegraphics{./figs/dev2_histo}
\end{center}
\begin{description}
\item[A.] \verb|histogram(BHEADCIR ~ GENDER, data=babies)|
\item[B.] \verb|histogram(~ BHEADCIR, data=babies, groups=GENDER)|
\item[C.] \verb+histogram(~ BHEADCIR | GENDER, data=babies)+
\item[D.] \verb+histogram(~ BHEADCIR | GENDER, data=babies, type="count")+
\item[E.] Je ne sais pas.
\end{description}

\item[\bf 1.6] \marginpar{\phantom{text}1.6 $\square$} Quelle commande permet de
  reproduire la figure suivante (indépendemment du rapport largeur/hauteur
  et de la couleur) ?
\begin{center}
  \includegraphics{./figs/dev2_bwplot}
\end{center}
\begin{description}
\item[A.] \verb|bwplot(GENDER ~ BWEIGHT, data=babies)|
\item[B.] \verb|bwplot(~ BWEIGHT, data=babies, groups=GENDER)|
\item[C.] \verb+bwplot(~ BWEIGHT | GENDER, data=babies)+
\item[D.] \verb|bwplot(BWEIGHT ~ GENDER, data=babies, horizontal=TRUE)|
\item[E.] Je ne sais pas.
\end{description}
\end{description}

\begin{description}
\item[\bf 1.7] \marginpar{\phantom{text}1.7 $\square$} Voici les résultats d'un test de
  Student comparant les tailles moyennes des bébés des deux sexes à la
  naissance.
\begin{Verbatim}[frame=single]
data:  BWEIGHT by GENDER 
t = -1.9108, df = 249.659, p-value = 0.9714
alternative hypothesis: true difference in means is greater than 0 
95 percent confidence interval:
 -0.1681625        Inf 
sample estimates:
  mean in group Male mean in group Female 
            3.441429             3.531642 
\end{Verbatim}
De quel type de test s'agit-il exactement ?
\begin{description}
\item[A.] C'est un test unilatéral supposant l'homogénéité des variances.
\item[B.] C'est un test bilatéral supposant l'homogénéité des variances.
\item[C.] C'est un test unilatéral ne supposant pas l'homogénéité des variances.
\item[D.] C'est un test bilatéral ne supposant pas l'homogénéité des variances.
\item[E.] Je ne sais pas.
\end{description}
\item[\bf 1.8] \marginpar{\phantom{text}1.8 $\square$} Le statisticien décide de réaliser
  un test non-paramétrique à l'aide de la commande
\begin{verbatim}
wilcox.test(BWEIGHT ~ GENDER, data=babies, paired=TRUE)
\end{verbatim}
Ce test est-t-il correctement spécifié dans le contexte de cette étude ?
  \begin{description}
  \item[A.] Oui.
  \item[B.] Non.
  \item[C.] Je ne sais pas.
  \end{description}
\end{description}

\section*{Exercice 2}
La distribution des fumeurs (F) et non-fumeurs (NF) dans un ensemble de 177
personnes vivant en couple est indiquée dans le tableau suivant :
\vskip1em

\begin{tabular}{llrrr}
\toprule
& & \multicolumn{2}{c}{Mari} & \\
\cmidrule(r){3-4}
& & F & NF & Total \\
\midrule
Femme & F & 42 & 22 & 64 \\
& NF & 32 & 81 & 113 \\
& Total & 74 & 103 & 177 \\
\bottomrule
\end{tabular}
\vskip1em

Veuillez indiquer les commandes permettant de répondre aux questions suivantes :
\begin{description}
\item[\bf 2.1] \marginpar{\phantom{text}2.1 $\square$} Pour saisir ces données tabulaires
  sous R, on peut utiliser la commande
  \begin{description}
  \item[A.] \verb|smoke <- matrix(c(42,22,32,81))|
  \item[B.] \verb|smoke <- matrix(c(42,32,22,81), nrow=2, byrow=TRUE)|
  \item[C.] \verb|smoke <- matrix(c(42,22,32,81), nrow=2, byrow=TRUE)|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 2.2] \marginpar{\phantom{text}2.2 $\square$} Quelle commande permet d'obtenir la
  distribution marginale chez les femmes (totaux lignes) ?
  \begin{description}
  \item[A.] \verb|table(smoke)|
  \item[B.] \verb|margin.table(smoke, 1)|
  \item[C.] \verb|margin.table(smoke, 2)|
  \item[D.] \verb|prop.table(smoke)|
  \item[E.] Je ne sais pas.
  \end{description}
\item[\bf 2.3] \marginpar{\phantom{text}2.3 $\square$} Le nombre total d'individus peut
  être obtenu à l'aide de la commande
  \begin{description}
  \item[A.] \verb|sum(table(smoke))|
  \item[B.] \verb|length(table(smoke))|
  \item[C.] \verb|dim(table(smoke))|
  \item[D.] Je ne sais pas.
  \end{description}
\end{description}

\begin{description}
\item[\bf 2.4] \marginpar{\phantom{text}2.4 $\square$} La commande 
\begin{verbatim}
(smoke[1,1]/smoke[1,2]) / (smoke[2,1]/smoke[2,2])
\end{verbatim}
fournit une estimation
\begin{description}
\item[A.] du risque relatif,
\item[B.] de l'odds-ratio,
\item[C.] de la statistique $\chi^2$ ?
\item[D.] Je ne sais pas.
\end{description}  
\end{description}

\begin{description}
\item[\bf 2.5] \marginpar{\phantom{text}2.5 $\square$} Voici le résultat d'un test
  statistique :
\begin{Verbatim}[frame=single]
data:  smoke 
p-value = 1.669e-06
alternative hypothesis: true odds ratio is not equal to 1 
95 percent confidence interval:
 2.381413 9.864192 
sample estimates:
odds ratio 
  4.784393
\end{Verbatim}
De quel test s'agit-il ?
\begin{description}
\item[A.] Il s'agit d'un test du $\chi^2$.
\item[B.] Il s'agit d'un test binomial pour la proportion de maris fumeurs
  versus non-fumeurs.
\item[C.] Il s'agit d'un test exact de Fisher.
\item[D.] Je ne sais pas.
\end{description}  
\end{description}

\section*{Exercice 3}
Soit les données suivantes :
\begin{verbatim}
    M+  M-
E+  72  89
E-  28 131
\end{verbatim}
où les colonnes \texttt{M+/M-} désignent le nombre d'individus malades et
non-malades, respectivement, et les lignes \texttt{E+/E-} les individus
exposés et non-exposés dans une étude de cohorte. Le tableau est appelé
\texttt{tab} sous R.

\begin{description}
\item[\bf 3.1] \marginpar{\phantom{text}3.1 $\square$} Quelle commande permet de
  reproduire la figure suivante (indépendemment du rapport largeur/hauteur
  et de la couleur) ?
\begin{center}
  \includegraphics{./figs/dev2_barchart}
\end{center}
\begin{description}
\item[A.] \verb|barchart(prop.table(tab, 2)*100, stack=FALSE, xlim=c(-5, 105))|
\item[B.] \verb|barchart(prop.table(tab, 1)*100, stack=FALSE, xlim=c(-5, 105))|
\item[C.] \verb|barchart(prop.table(tab, 1)*100, xlim=c(-5, 105))|
\item[D.] Je ne sais pas.
\end{description}
\item[\bf 3.2] \marginpar{\phantom{text}3.2 $\square$} La prévalence observée est donnée par : 
\begin{description}
\item[A.] \verb|margin.table(tab, 2)[1]/320|
\item[B.] \verb|prop.table(tab, 2)[1]|
\item[C.] \verb|sum(tab[1,])/320|
\item[D.] Je ne sais pas.
\end{description}
\item[\bf 3.3] \marginpar{\phantom{text}3.3 $\square$} La commande suivante fournit un
  intervalle de confiance à 95~\% pour 
l'odds-ratio : 
\begin{verbatim}
confint(oddsratio(tab))
\end{verbatim}
\begin{description}
\item[A.] Vrai.
\item[B.] Faux.
\item[C.] Je ne sais pas.
\end{description}
\end{description}


% --------------------------------------------------------------- Chapter 04 --
\chapter{Analyse de variance et plans d'expérience}\label{chap:anova}
\Opensolutionfile{solutions}[solutions4]


\section*{Énoncés}
%
% Dupont 2009 p. 326
%
\begin{exo}\label{exo:4.1}
Dans une étude sur le gène du récepteur à \oe strogènes, des généticiens se
sont intéressés à la relation entre le génotype et l'âge de diagnostic du
cancer du sein. Le génotype était déterminé à partir des deux allèles d'un
polymorphisme de restriction de séquence (1.6 et 0.7 kb), soit trois groupes
de sujets : patients homozygotes pour l'allèle 0.7 kb (0.7/0.7), patients
homozygotes pour l'allèle 1.6 kb (1.6/1.6), et patients hétérozygotes
(1.6/0.7). Les données ont été recueillies sur 59 patientes atteintes d'un
cancer du sein, et sont disponibles dans le fichier
\texttt{polymorphism.dta} (fichier \Stata). Les données moyennes sont
indiquées ci-dessous :\autocite[p.~327]{dupont09}
\vskip1em

\begin{tabular}{lrrrr}
\toprule
& \multicolumn{3}{c}{Génotype} & \\
\cmidrule(r){2-4}
& 1.6/1.6 & 1.6/0.7 & 0.7/0.7 & Total \\
\midrule
Nombre de patients & 14 & 29 & 16 & 59 \\
\emph{Âge lors du diagnostic} & & & & \\
\quad Moyenne & 64.64 & 64.38 & 50.38 & 60.64 \\
\quad Écart-type & 11.18 & 13.26 & 10.64 & 13.49 \\
\quad IC 95~\% & (58.1–71.1) & (59.9–68.9) & (44.3–56.5) & \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Tester l'hypothèse nulle selon laquelle l'âge de diagnostic ne varie
  pas selon le génotype à l'aide d'une ANOVA. Représenter sous forme
  graphique la distribution des âges pour chaque génotype.
\item[(b)] Les intervalles de confiance présentés dans le tableau ci-dessus ont
  été estimés en supposant l'homogénéité des variances, c'est-à-dire en
  utilisant l'estimé de la variance commune ; donner la valeur de ces
  intervalles de confiance sans supposer l'homoscédasticité. 
\item[(c)] Estimer les différences de moyenne correspondant à l'ensemble des
  combinaisons possibles des trois génotypes, avec une estimation de
  l'intervalle de confiance à 95~\% associé et un test paramétrique
  permettant d'évaluer le degré de significativité de la différence
  observée.
\item[(d)] Représenter graphiquement les moyennes de groupe avec des
  intervalles de confiance à 95~\%.
\end{description}
\begin{sol}
Pour charger les données, il est nécessaire d'importer la librairie
\texttt{foreign} qui permet de lire les fichiers enregistrés par \Stata.
<<ex4-1a>>=
library(foreign)
polymsm <- read.dta("data/polymorphism.dta")
head(polymsm)
@ 
\cmd{read.dta}\cmd{library}\cmd{head}  
Notons que \texttt{polymsm} est un \texttt{data.frame}, ce qui sera utile
pour utiliser les commandes graphiques ou réaliser l'ANOVA. La première
colonne contient une série d'identifiants uniques pour les individus ; elle
ne sera pas utile dans le cas présent. Pour calculer les moyennes et
écart-types de chaque groupe, on peut procéder comme suit : 
<<ex4-1b>>=
with(polymsm, tapply(age, genotype, mean))
with(polymsm, tapply(age, genotype, sd))
@ 
\cmd{tapply}\cmd{mean}\cmd{sd}
La distribution des âges selon le génotype est indiquée dans le diagramme en
boîtes à moustaches suivant.
<<ex4-1c, fig=TRUE>>=
bwplot(age ~ genotype, data=polymsm)
@ 
\cmd{bwplot}

Il est également possible d'utiliser des histogrammes, en utilisant
\texttt{histogram} au lieu de \texttt{bwplot}.
<<ex4-1d, fig=TRUE>>=
histogram(~ age | genotype, data=polymsm)
@ 
\cmd{histogram}

Le modèle d'ANOVA est réalisé à l'aide de la commande \texttt{aov} :
<<ex4-1e>>=
aov.res <- aov(age ~ genotype, data=polymsm)
summary(aov.res)
@ 
\cmd{aov}\cmd{summary.aov} 
La statistique de test $F$ est reportée dans la colonne \texttt{F value},
avec le degré de significativité associé dans la colonne suivante
(\texttt{Pr(>F)}). L'ANOVA indique qu'au moins une paire de moyennes est
significativement différente, en considérant un rique d'erreur de 5~\%.

On peut vérifier l'exactitude des intervalles de confiance reportés dans le
tableau présenté plus haut. Pour cela, il nous faut une estimation de
l'erreur résiduelle, qui est simplement la racine carrée du carré moyen
associé au terme d'erreur dans le tableau d'ANOVA ci-dessus.
<<ex4-1f>>=
mse <- unlist(summary(aov.res))["Mean Sq2"]
se <- sqrt(mse)
@ 
\cmd{unlist}\cmd{sqrt}
Soit, une estimation de la racine de la variance commune de
\Sexpr{round(se,2)}. À partir de là, on peut construire les intervalles de
confiance à 95~\% pour chaque moyenne de groupe comme suit :
<<ex4-1g>>=
ni <- table(polymsm$genotype)
n <- sum(ni)
m <- with(polymsm, tapply(age, genotype, mean))
lci <- m - qt(0.975, n-3) * se / sqrt(ni)
uci <- m + qt(0.975, n-3) * se / sqrt(ni)
rbind(lci, uci) 
@ %$ 
\cmd{table}\cmd{mean}\cmd{qt}\cmd{sqrt}\cmd{rbind}

Dans le cas plus général, pour l'estimation de l'intervalle de confiance
d'une moyenne (ou d'une proportion) on pourra se référer à la commande
\texttt{epi.conf} du package \texttt{epiR}, par exemple :
<<ex4-1h>>=
library(epiR)
epi.conf(polymsm$age[polymsm$genotype=="1.6/1.6"], ctype = "mean.single")
@ 

Pour représenter graphiquement les moyennes de groupe, on peut utiliser deux
approches. La première consiste à constuire le graphique manuellement, en
affichant les moyennes sous forme de points et les intervalles de confiance
sous forme de segments.
<<ex4-1i, fig=TRUE>>=
mm <- as.data.frame(cbind(m, lci, uci))
mm$g <- levels(polymsm$genotype)
rownames(mm) <- NULL
dotplot(m ~ g, data=mm, ylim=c(40,75),
        panel=function(x, y, ...) {
          panel.dotplot(x, y, ...)
          panel.segments(x, mm$lci, x, mm$uci)
        })
@       

On a donc regroupé les moyennes de groupe, ainsi que les bornes inférieures
et supérieures des intervalles de confiance et les niveaux du facteur de
classification dans un même \texttt{data.frame}. La commande graphique
consiste à enchaîner un appel à \texttt{panel.dotplot} pour afficher les
moyennes, puis \texttt{panel.segments} pour tracer les intervalles de
confiance associés.

L'autre solution consiste la commande \texttt{xYplot} (à ne pas confondre
avec \texttt{xyplot}) du package \texttt{Hmisc}. La seule subtilité est que
la variable explicative (reportée sur l'axe des abscisses) doit être traitée
en tant que variable numérique. Il faut adapter légèrement la syntaxe pour
parvenir à l'effet désiré, en particulier afficher correctement les
étiquettes correspondant aux différents génotypes.
<<ex4-1j, eval=FALSE>>=
library(Hmisc)
xYplot(Cbind(m,lci,uci) ~ 1:3, data=mm, scales=list(x=list(at=1:3, labels=mm$g)), 
       xlab="", ylim=c(40,75))
@
\end{sol}
\end{exo}
%
% METHO p. 87
%
\begin{exo}\label{exo:4.2}
On a mesuré en fin de traitement chez 18 patients répartis par tirage au
sort en trois groupes de traitement A, B, et C, un paramètre biologique dont
on sait que la distribution est normale. Les résultats sont les suivants :
\vskip1em

\begin{tabular}{ccc}
\toprule
A & B & C \\
\midrule
19.8 & 15.9 & 15.4 \\
20.5 & 19.7 & 17.1 \\
23.7 & 20.8 & 18.2 \\
27.1 & 21.7 & 18.5 \\
29.6 & 22.5 & 19.3 \\
29.9 & 24.0 & 21.2 \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Réaliser une ANOVA à un facteur.
\item[(b)] Selon le résultat du test, procéder aux comparaisons par paire de
  traitement des moyennes, en appliquant une correction simple de Bonferroni
  (c'est-à-dire où les degrés de significativité estimé sont multipliés par
  le nombre de comparaisons effectuées). Comparer avec de simples tests de
  Student non corrigés pour les comparaisons multiples. 
\item[(c)] D'après des études plus récentes, il s'avère que la normalité des
  distributions parentes peut-être remise en question. Effectuer la
  comparaison des trois groupes par une approche non-paramétrique.
\end{description}
\begin{sol}
Dans un premier temps, il s'agit de saisir les données sous un format
approprié pour leur traitement sous \R. Plutôt que de construire un tableau
à trois colonnes, il est préférable de construire un \texttt{data.frame},
contenant une colonne avec l'ensemble des mesures biologiques (soit $3\times
6=18$ observations) et une autre colonne codant pour le type de traitement
(trois niveaux, répétés 6 fois chacun).
<<ex4-2a>>=
pb <- c(19.8,20.5,23.7,27.1,29.6,29.9,
        15.9,19.7,20.8,21.7,22.5,24.0,
        15.4,17.1,18.2,18.5,19.3,21.2)
tx <- gl(3, 6, labels=c("A","B","C"))
dfrm <- data.frame(pb, tx)
head(dfrm, 8)
@ 
\cmd{gl}\cmd{data.frame}\cmd{head}
On notera que l'on a arrangé les données différemment : dans une colonne
figure l'ensemble des mesures et dans l'autre les niveaux du facteur de
classification, qui ont été générés à l'aide de la commande
\texttt{gl}. Cette représentation des données sera beaucoup plus commode par
la suite, notamment pour réaliser l'ANOVA. Cependant, on pourrait dans un
premier temps travailler avec un tableau à 3 colonnes :
<<ex4-2b>>=
pbm <- matrix(pb, nc=3)
pbm
@ 
et "opérer par colonne" pour les résumés numériques. Par exemple, les
commandes suivantes sont équivalentes et fourniront une estimation de la
moyenne par traitement :
<<ex4-2c>>=
apply(pbm, 2, mean)
sapply(data.frame(pbm), mean)
@ 
\cmd{apply}\cmd{sapply}
Notons également qu'il est très facile de revenir à une structure à deux
variables/colonnes à l'aide de la commande \texttt{melt} du package
\texttt{reshape}. Cette commande a pour effet de concaténer verticalement
les séries de mesure et d'associer à chaque ligne le nom de la colonne
correspodant, en l'occurence le type de traitement.
<<ex4-2d>>=
library(reshape)
head(melt(data.frame(pbm)))
@ 

Si l'on revient à la représentation des données en deux variables/colonnes,
on peut produire un rapide résumé numérique (moyenne et variance) des
mesures enregistrées par groupe de traitement comme suit :
<<ex4-2e>>=
with(dfrm, tapply(pb, tx, mean))
with(dfrm, tapply(pb, tx, var))
@
\cmd{tapply}\cmd{mean}\cmd{var}
Enfin, on peut visualiser la distribution des mesures individuelles par
groupe de traitement à l'aide d'un diagramme de type boîte à moustaches.
<<ex4-2f, fig=TRUE>>=
bwplot(pb ~ tx, data=dfrm)
@ 
\cmd{bwplot}

À présent, il est possible de réaliser l'ANOVA avec la commande \texttt{aov}.
<<ex4-2g>>=
aov.res <- aov(pb ~ tx, data=dfrm)
summary(aov.res)
@ 
\cmd{aov}\cmd{summary.aov}
Le résultat du test significatif indique qu'au moins une paire de moyennes
peut être considérée comme significativement différente au seuil 5~\%. Pour
comparer l'ensemble des traitements deux à deux, en se protégeant de
l'inflation du risque d'erreur, il est possible d'utiliser la commande
\texttt{pairwise.t.test} pour réaliser des tests de Student, avec l'option
\verb|p.adjust="bonf"| pour appliquer une correction de Bonferroni.
<<ex4-2h>>=
pairwise.t.test(dfrm$pb, dfrm$tx, p.adjust="bonf")
@ 

Si l'hypothèse de normalité n'est pas réaliste ou peut être remise en
question, on peut réaliser une ANOVA en considérant les rangs des
observations avec la commande \texttt{kruskal.test} :
<<ex4-2i>>=
kruskal.test(pb ~ tx, data=dfrm)
@
La comparaison des paires de traitements peut être effectuée à l'aide de
simples tests de Wilcoxon pour échantillons indépendants, par exemple 
\texttt{wilcox.test}. 
<<ex4-2j, eval=FALSE>>=
wilcox.test(pb ~ tx, data=dfrm, subset=tx!="C")
@ 
ou bien
<<ex4-2k>>=
with(dfrm, wilcox.test(pb[tx=="A"], pb[tx=="B"]))
@ 
L'option \texttt{subset} est disponible dans la plupart des tests que l'on
rencontrera (ANOVA, régression, tests sur deux échantillons), ainsi que dans
les commandes graphiques : elle permet de restreindre l'analyse à un
sous-ensemble de l'échantillon remplissant certaines conditions (ici, les
individus ne figurant pas dans le groupe C).
\end{sol}
\end{exo}
%
% Peat 2005 p. 113
%
\begin{exo}\label{exo:4.3}
Un service d'obstétrique s'intéresse au poids de nouveaux-nés nés à terme et
âgés de 1 mois. Pour cet échantillon de 550 bébés, on dispose également
d'une information concernant la parité (nombre de frères et soeurs), mais on
sait qu'il n'y aucune relation de gemellité parmi les enfants ayant des
frères et soeurs. L'objet de l'étude est de déterminer si la parité (4
classes) influence le poids des nouveaux-nés à 1 mois. Les données sont
résumées dans le tableau suivant, et elles sont disponibles dans un fichier
SPSS, \texttt{weights.sav}.\autocite[p.~113]{peat05}
\vskip1em

\begin{tabular}{lrrrrr}
\toprule
& \multicolumn{4}{c}{Nombre de frères et soeurs} & Total \\
& 0 & 1 & 2 & $\ge 3$ & \\
\midrule
\emph{Échantillon} & & & & \\ 
Effectif & 180 & 192 & 116 & 62 & 550 \\
Fréquence & 32.7 & 34.9 & 21.1 & 11.3 & 100.0 \\
\emph{Poids (kg)} & & & & \\
Moyenne & 4.26 & 4.39 & 4.46 & 4.43 & \\
Écart-type & 0.62 & 0.59 & 0.61 & 0.54 & \\
(Min–Max) & (2.92–5.75) & (3.17–6.33) & (3.09–6.49) & (3.20–5.48) & \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Vérifier les données reportées dans le tableau précédent.
\item[(b)] Procéder à une analyse de variance à un facteur. Conclure sur la
  significativité globale et indiquer la part de variance expliquée par le
  modèle.
\item[(c)] Afficher la distribution des poids selon la parité. Procéder à un
  test d'homogénéité des variances (rechercher dans l'aide en ligne le test
  de Levenne). 
\item[(d)] On décide de regrouper les deux dernières catégories (2 et $\ge
  3$). Refaire l'analyse et comparer aux résultats obtenus en (b).
\item[(e)] Réaliser un test de tendance linéaire (par ANOVA) sur les données
  recodées en trois niveaux pour la parité.
\end{description}
\begin{sol}
Pour charger les données, il est nécessaire d'importer la librairie
\texttt{foreign} qui permet de lire les fichiers enregistrés par SPSS.
<<ex4-3a>>=
library(foreign)
weights <- read.spss("data/weights.sav", to.data.frame=TRUE)
str(weights)
@
\cmd{foreign}\cmd{read.spss}
Comme on l'a vu dans l'exercice~\ref{exo:3.6}, l'option
\verb|to.data.frame=TRUE| est importante car c'est elle qui permet de
stocker les données lues sous forme de \texttt{data.frame} (variable en
colonnes, individus en lignes).

Dans un premier temps, procédons au résumé numérique uni- et bivarié des
données d'intérêt (variables \texttt{WEIGHT} et \texttt{PARITY}). Pour la
variable qualitative, les tableaux d'effectifs et de fréquences relatives
sont obtenus ainsi :
<<ex4-3b>>=
table(weights$PARITY)
round(prop.table(table(weights$PARITY))*100, 1)
@ 
\cmd{table}\cmd{prop.table}\cmd{round}
Pour la variable quantitative, les moyennes et écart-types par type de
parité sont obtenus ainsi :
<<ex4-3c>>=
round(with(weights, tapply(WEIGHT, PARITY, mean)), 2)
round(with(weights, tapply(WEIGHT, PARITY, sd)), 2)
@ 
\cmd{round}\cmd{mean}\cmd{sd}

Pour l'analyse de variance, on utilise le même principe qu'à
l'exercice~\ref{exo:4.2}. 
<<ex4-3d>>=
aov.res <- aov(WEIGHT ~ PARITY, data=weights)
summary(aov.res)
@
\cmd{aov}\cmd{summary.aov}
Le test $F$ est significatif, donc on peut rejeter l'hypothèse nulle
d'égalité des quatre moyennes. La part de variance expliquée est simplement
le rapport entre la somme des carrés (\texttt{Sum Sq}) associée au facteur
d'étude (\texttt{PARITY}), soit 3.48, et la somme des carrés totaux, soit
3.48+3.48+195.36 : on obtient 0.018, soit environ 2~\%.

Pour afficher la distribution des poids, on peut bien sûr utiliser des
boîtes à moustaches, comme dans les exercices précédents. Si l'on souhaite
visualiser directement les données individuelles, un diagramme de dispersion
conditionné sur les groupes est également intéressant.
<<ex4-3e, fig=TRUE>>=
stripplot(WEIGHT ~ PARITY, data=weights, ylab="Poids (kg)", jitter.data=TRUE)
@ 
\cmd{stripplot}

L'autre alternative consiste à utiliser des histogrammes pour chaque groupe.
<<ex4-3f, fig=TRUE>>=
histogram(~ WEIGHT | PARITY, data=weights)
@ 

On notera que l'on a ajouté un léger décalage aléatoire des données (sur
l'axe horizontal uniquement) en utilisant l'option \verb|jitter.data=TRUE|,
ce qui permet d'éviter le chevauchement total des points.

Le test de Levene n'est pas disponible dans les commandes de base de R, mais
on peut installer le package \texttt{car} qui fournit la commande
\texttt{leveneTest} :
<<ex4-3g>>=
library(car)
leveneTest(WEIGHT ~ PARITY, data=weights)
@ 
Une alternative consisterait à utiliser un test de Bartlett
(\texttt{bartlett.test}) pour l'homogénéité des variances. Cette commande
s'utilise exactement de la même manière que \texttt{leveneTest} (variable
réponse décrite par le facteur d'étude, nom du \texttt{data.frame} où
chercher les variables).

Pour regrouper les deux dernières catégories, on peut créer une nouvelle
variable et générer manuellement les nouvelles modalités associés, ou plus
simplement "recoder" la variable qualitative \texttt{PARITY} en une nouvelle
variable :
<<ex4-3h>>=
PARITY2 <- weights$PARITY
levels(PARITY2)[3:4] <- "2 siblings or more"
@ %$
Le modèle d'analyse de variance est construit de la même manière que
précédemment :
<<ex4-3i>>=
aov.res2 <- aov(WEIGHT ~ PARITY2, data=weights)
summary(aov.res2)
@ 

Enfin, pour réaliser un test de tendance centrale, il y a deux solutions :
soit par la méthode des contrastes, soit par une approche de régression
linéaire. Ces deux approches fournissent des résultats identiques, et
suppose généralement que les niveaux du facteur de classification sont
équi-espacés (variation du même nombre d'unités entre chaque niveau du
facteur). Dans le cas présent, on utlisera la commande \texttt{lm} pour
réaliser une régression linéaire simple (voir \ref{chap:reg},
p.~\pageref{chap:reg}, pour plus de détails). 
<<exo4-3j>>=
levels(PARITY2)
lm.res <- lm(WEIGHT ~ as.numeric(PARITY2), data=weights)
summary(lm.res)
@ 
Le test de la tendance linéaire pour l'ANOVA correspond au test de la pente
de la droite de régression, qui ici est significatif suggérant une
augmentation du poids moyen avec la taille de la fratrie.
De manière équivalente, on peut utiliser l'approche consistant à recoder le
facteur de classification en facteur à modalités ordonnées (ou niveaux) à
l'aide la commande \texttt{as.ordered}. Le test pour la tendance linéaire
correspondant au contraste nommé \texttt{as.ordered(PARITY2).L} dans la
sortie suivante.
<<exo4-3k>>=
levels(as.ordered(PARITY2))
summary(lm(WEIGHT ~ as.ordered(PARITY2), data=weights))
@
\end{sol}
\end{exo}
%
% STAB TD 3
%
\begin{exo}\label{exo:4.4}
On souhaite analyser les données traitées à l'exercice~\ref{exo:1.6}
(p.~\pageref{exo:1.6}) par un modèle d'analyse de variance.

\begin{description}
\item[(a)] Calculer moyenne et variance des mesures pour chaque traitement.
\item[(b)] Représenter graphiquement les moyennes par traitement dans un
  graphique d'interaction.
\item[(c)] Effectuer une ANOVA à deux facteurs, sans interaction.
\item[(d)] Refaire une ANOVA en incluant l'interaction entre les facteurs
  \texttt{Na} et \texttt{An}.
\end{description}
\begin{sol}
Les données ont été normalement été sauvegardées au format \R à
l'exercice~\ref{exo:1.6}. On supposera que le fichier a été sauvegardé sous
le nom \texttt{bioluminescence.RData}. Pour l'importer sous \R, on utilise
la commande \texttt{load} directement :
<<ex4-4a>>=
load("data/bioluminescence.RData")
ls()
@ 

Pour calculer la moyenne et la variance pour chaque traitement, on peut
utiliser le même principe qu'à l'exercice~\ref{exo:1.6}, c'est-à-dire une
combinaison des commandes \texttt{tapply} et \texttt{mean} ou \texttt{var}. 
<<ex4-4b>>=
with(biolum, tapply(y, list(An=An, Na=Na), mean))
with(biolum, tapply(y, list(An=An, Na=Na), var))
@ 
L'ajout des noms de variables dans \texttt{list} permet de mieux interpréter
les résultats afficher par \R : le premier facteur de la liste, \texttt{An},
est affiché en ligne, alors que les niveaux du second sont affichés en
colonnes. 

Pour représenter les données moyennes sous forme graphique (diagramme ou
graphique d'intercation), on utilisera la commande \texttt{xyplot} avec
l'option \verb|type="a"| qui permet de calculer automatiquement les moyennes
de groupe à partir des données brutes. Le reste des options
(\texttt{auto.key}) permet de générer la légende pour faciliter la lecture
du graphique.
<<ex4-4c, fig=TRUE>>=
xyplot(y ~ An, data=biolum, groups=Na, type=c("a","g"), 
       auto.key=list(corner=c(0,1), lines=TRUE, points=FALSE, title="Na", cex.title=.8))
@ 

<<ex4-4d>>=
summary(aov(y ~ An + Na, data=biolum))
@ 

<<ex4-4d>>=
summary(aov(y ~ An + Na + An:Na, data=biolum))
@ 

\end{sol}
\end{exo}

% ANOVA two-way cross-over
%
%% \begin{exo}
%% Les données inclues dans le fichier \texttt{headache.txt} ont été collectées
%% dans un plan randomisé en essais croisés (cross-over) avec trois bras de
%% traitement visant à comparer deux analgésiques (A et B) et un placebo (P)
%% pour le traitement des céphalées.\autocite[p.~617]{fitzmaurice04} La
%% comparaison principale porte sur les deux traitements actifs (l'un des deux
%% incluant en plus de la caffeine). On notera qu'il n'y a que deux périodes,
%% donc les patients n'ont reçu que deux des trois traitements, par
%% randomisation. La réponse mesurée est la diminution moyenne de douleur. Le
%% fichier de données comporte un descriptif détaillé de l'étude ainsi que le
%% nom des variables.

%% Par souci de simplicité, on ne va s'intéresser qu'aux deux bras actifs, A et
%% B, d'où les deux facteurs d'intérêt suivants : période (1 et 2) et
%% traitements séquentiels (AB ou BA). Les résultats moyens sont résumés dans
%% le tableau suivant (Tableau 21.1, p.~618) :
%% \vskip1em

%% \begin{tabular}{lrrrrr}
%% \toprule
%% & & \multicolumn{2}{c}{Période 1} & \multicolumn{2}{c}{Période 2} \\
%% \cmidrule(r){3-6}
%% Séquence & N & Moyenne & DS & Moyenne & DS \\
%% \midrule
%% AB & 126 & 10.196 & 3.347 & 9.153 & 3.429 \\
%% BA & 127 & 9.581 & 3.881 & 10.791 & 3.530 \\
%% \bottomrule
%% \end{tabular}
%% \vskip1em

%% \begin{description}
%% \item[(a)] Après avoir importé les données et restreint le tableau de
%%   données aux seuls comparaisons AB et BA, reonstruire le tableau de
%%   synthèse ci-dessus.
%% \item[(b)] Résumer l'effet traitement dans un graphique, en prenant en
%%   considération les deux périodes et la séquence.
%% \item[(c)] Tester l'effet séquence (traitement) et l'éffet période à l'aide
%%   de tests t de Student. Conclure au risque $\alpha$ de 5~\%.
%% \item[(d)] Réaliser une ANOVA en considérant ces deux mêmes facteurs, et
%%   leur interaction. Existe-t-il un effet rémanent (carry-over) ? Conclure
%%   sur l'effet global du traitement. 
%% \end{description}
%% \begin{sol}
%% Le fichier de données \texttt{headache.txt} contient un en-tête de 35 lignes
%% qui comprend la description de l'étude et le nom des variables. Les données
%% réelles sont ensuite arrangées sous forme de 7 colonnes (lignes
%% 36--881). Pour importer ces données, il conviendra donc de lire le fichier à
%% partir de la 36\ieme\ ligne. La 6\ieme\ colonne est identique à la 3\ieme\ qui
%% peut donc être supprimée.

%% <<exo4-5a>>=
%% headache <- read.table("headache.txt", header=FALSE, skip=35)
%% headache <- headache[,-3]
%% varnames <- c("ID", "Center", "Sequence", "Period", "Treatment", "Response")
%% names(headache) <- varnames
%% head(headache)
%% @ 
%% Pour restreindre les résultats aux seuls comparaisons AB et BA (séquences 1
%% et 2), on utilise \texttt{subset} de la manière suivante :
%% <<exo4-5b>>=
%% headache <- subset(headache, Sequence == 1 | Sequence == 2)
%% headache$Sequence <- factor(headache$Sequence, levels=1:2, labels=c("BA","AB"))
%% headache$Period <- factor(headache$Period, levels=0:1, labels=1:2)
%% @
%% Pour reconstruire le tableau de synthèse (moyenne et écart-type par
%% séquence et période), on peut utiliser la commande \texttt{aggregate}. La
%% subtilité consiste à correctement identifier les séquences AB et BA, ce qui
%% a été fait à l'étape précédente en renommant les étiquettes associées à
%% cette variable qualitative.
%% <<exo4-5c>>=
%% resultats <- aggregate(Response ~ Sequence + Period, data=headache, mean)
%% xtabs(Response ~ Sequence + Period, resultats)
%% @ 
%% On procèdera de même pour les écart-types en remplaçant la commande
%% \texttt{mean} par \texttt{sd}.

%% Ces résultats peuvent être résumés dans un graphique d'interaction où l'on
%% représente les moyennes calculées ci-dessus.
%% <<exo4-5d, fig=TRUE>>=
%% xyplot(Response ~ Period, data=resultats, groups=Sequence, 
%%        type=c("b","g"), auto.key=list(corner=c(0,1)))
%% @ 

%% On peut tester séparément les effets séquence (traitement) et période à
%% l'aide de simples tests $t$ pour échantillons indépendants.
%% <<exo4-5e>>=
%% t.test(Response ~ Period, data=headache, var.equal=TRUE)
%% t.test(Response ~ Sequence, data=headache, var.equal=TRUE)
%% @ 
%% Ces résultats suggère que l'ordre d'administration des traitements
%% n'influence pas le niveau moyen de la réponse, mais en même temps on ne met
%% pas en évidence d'effet global du traitement (indépendemment de la
%% période). 

%% Le modèle d'ANOVA considérant les deux facteurs, \texttt{Sequence} (AB ou
%% BA) et \texttt{Period} (1 et 2), se construit sur la base de la formule
%% \verb|Response ~ Period + Sequence + Period:Sequence|, le dernier terme
%% correspondant à l'effet d'interaction. Ce modèle peut s'abbréger sous la
%% forme \verb|Response ~ Period * Sequence|.
%% <<exo4-5f>>=
%% aov.res <- aov(Response ~ Period * Sequence, data=headache)
%% summary(aov.res)
%% @ 
%% À l'évidence, on retrouve un effet d'interaction suggérant que les deux
%% effets de séquence et période sont inter-dépendants, sans qu'aucun des deux ne
%% soit significatifs à 5~\%. Ceci suggère 
%% \end{sol}
%% \end{exo}
%
% ANCOVA
%
%% \begin{exo}
%%   ANCOVA
%% \end{exo}
\Closesolutionfile{solutions}

\chapter*{Devoir \no 3}
\addcontentsline{toc}{chapter}{Devoir \no 3}

Les exercices sont indépendants. Une seule réponse est correcte pour chaque
question. Lorsque vous ne savez pas répondre, cochez la case correspondante.

\section*{Exercice 1}
Dans une étude d'obstétrique, on s'est intéressé à la relation entre le
poids d'un bébé à la naissance et le gain de poids de la mère durant sa
grossesse. Les mères ont été regroupées en quatre classes selon leur gain de
poids : faible (< 11 kg), normal (11-16 kg), modéré (16-22 kg) et extrême (>
22 kg). Les bornes inférieures des intervalles sont exclues, les bornes
supérieures sont inclues. Le poids du bébé à la naissance est mesuré en
kilogrammes. Au total, on dispose des données de 59 dyades. Voici un aperçu
des données du \texttt{data.frame} appelé \texttt{bt} :
\begin{verbatim}
      bwt              wt              gain      
 Min.   :1.400   Min.   : 45.00   Min.   : 1.20  
 1st Qu.:3.095   1st Qu.: 52.15   1st Qu.:12.30  
 Median :3.480   Median : 54.50   Median :14.80  
 Mean   :3.434   Mean   : 57.93   Mean   :15.36  
 3rd Qu.:3.860   3rd Qu.: 59.00   3rd Qu.:18.05  
 Max.   :4.780   Max.   :104.00   Max.   :32.60
\end{verbatim}
On cherche à démontrer qu'il existe une relation entre le poids du bébé
(\texttt{bwt}) et le gain de poids (\texttt{gain}) des mères durant
leur grossesse, en considérant un risque de première espèce de 5~\%. La
variable \texttt{wt} représente le poids de la mère avant le début de la
grossesse.  

\begin{description}
\item[\bf 1.1] \marginpar{\phantom{text}\phantom{text} 1.1 $\square$} Quelle commande a permis de
  produire le résultat présenté ci-dessus ?
  \begin{description}
  \item[A.] \verb|describe|
  \item[B.] \verb|summary|
  \item[C.] \verb|str|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 1.2] \marginpar{\phantom{text}1.2 $\square$} Les données comportent-elles des
  valeurs manquantes ?
  \begin{description}
  \item[A.] Oui.
  \item[B.] Non.
  \item[C.] Je ne sais pas.
  \end{description}
\item[\bf 1.3] \marginpar{\phantom{text}1.3 $\square$} On désire recoder la variable
  \texttt{gain} en variable qualitative à quatre classes telles que décrites
  dans l'énoncé. Quelle commande est la plus appropriée ?
  \begin{description}
  \item[A.] \verb|cut(bt$gain, c(1,11,16,22,33), levels=c("faible","normal","modéré","extrême"))|
  \item[B.] \verb|cut(bt$gain, c(1,11,16,22,33), labels=c("faible","normal","modéré","extrême"))|
  \item[C.] \verb|cut(bt$gain, c(11,16,22), labels=c("faible","normal","modéré","extrême"))|
  \item[D.] \verb|factor(bt$gain, levels=c(11,16,22), labels=c("faible","normal","modéré","extrême"))|
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 1.4] \marginpar{\phantom{text}1.4 $\square$} Quelle commande a permis de fournir
  les résultats suivants :
\begin{verbatim}
  faible   normal   modéré  extrême 
3.126667 3.402400 3.641250 3.320000
\end{verbatim}
  \begin{description}
  \item[A.] \verb|with(bt, tapply(bwt, gain, mean))|
  \item[B.] \verb|with(bt, tapply(wt, gain, mean))|
  \item[C.] \verb|with(bt, tapply(bwt, gain, var))|
  \item[D.] \verb|with(bt, tapply(wt, gain, var))|
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 1.5] \marginpar{\phantom{text}1.5 $\square$} Voici le tableau d'analyse de
  variance où l'on considère \texttt{bwt} comme variable réponse et
  \texttt{gain} (4 classes) comme facteur d'étude :
\begin{verbatim}
            Df Sum Sq Mean Sq F value Pr(>F)
gain         3  1.799  0.5997   1.572  0.207
Residuals   55 20.980  0.3815
\end{verbatim}
Quelle est la part de variance expliquée par ce modèle ? 
\begin{description}
\item[A.] 8.6~\%.  % SSG/SSE
\item[B.] 21.8~\%. % MSG-MSE
\item[C.] 0.5~\%.  % (SSG*3)/(SSE*55)
\item[D.] Je ne sais pas.
\end{description}  
\item[\bf 1.6] \marginpar{\phantom{text}1.6 $\square$} Les données individuelles sont
  représentées dans le graphique suivant.
\begin{center}
  \includegraphics{./figs/dev3_bt}
\end{center}
On décide d'exclure toutes les observations pour lesquelles le poids du
bébé à la naissance est inférieur à 2.5 kg (représenté par le trait
horizontal), toujours en considérant la variable \texttt{gain} comme
facteur de classification à 4 niveaux. Comment peut-on procéder pour refaire
l'ANOVA dans ces conditions ? 
\begin{description}
\item[A.] \verb|aov(bwt, gain, data=bt, select = bwt >= 2.5)|
\item[B.] \verb|aov(bwt ~ gain, data=bt, select = bwt >= 2.5)|
\item[C.] \verb|aov(bwt ~ gain, data=bt, subset = bwt >= 2.5)|
\item[D.] Je ne sais pas.
\end{description}  
\end{description}

\section*{Exercice 2}\label{dev3:exo2}
Considérons certains des résultats d'un essai clinique sur la thérapie
hormonale \citep{hulley98}. Les variables d'intérêt sont l'ethnicité
(\texttt{raceth}) des participants et la pression systolique
(\texttt{SBP}). Au total, il y a 2763 sujets et, une fois importées sous R,
les données (appelée \texttt{d}) apparaissent comme reporté ci-dessous :
\begin{verbatim}
     raceth           SBP       
 Min.   :1.000   Min.   : 83.0  
 1st Qu.:1.000   1st Qu.:122.0  
 Median :1.000   Median :134.0  
 Mean   :1.147   Mean   :135.1  
 3rd Qu.:1.000   3rd Qu.:147.0  
 Max.   :3.000   Max.   :224.0  
\end{verbatim}
La variable \texttt{raceth} code pour l'ethnicité des participants, avec les
conventions suivantes : 1 = \texttt{White}, 2 = \texttt{Afr Amer} et 3 =
\texttt{Other}. La variable \texttt{SBP} représente la pression
systolique. On souhaite vérifier si celle-ci varie selon le facteur de
groupe. Voici un résumé numérique descriptif (effectif, moyenne, écart-type,
minimum et maximum) de la variable réponse selon les niveaux de
\texttt{raceth} : 
\begin{verbatim}
  raceth    n     mean       sd min max
1      1 2451 134.7838 18.83169  83 224
2      2  218 138.2339 19.99252  98 194
3      3   94 135.1809 21.25977  95 187
\end{verbatim}
\begin{description}
\item[\bf 2.1] \marginpar{\phantom{text}2.1 $\square$} Pour calculer le rapport entre la
  variance du groupe 3 et celle du groupe 1, on peut utiliser la commande
\begin{description}
\item[A.] \verb|var(d$SBP[d$raceth=3]/var(d$SBP[d$raceth=1]|
\item[B.] \verb|var(d$SBP[d$raceth==3]/var(d$SBP[d$raceth==1]|
\item[C.] \verb|var(d$SBP[d$raceth=="3"]/var(d$SBP[d$raceth=="1"]|
\item[D.] Je ne sais pas.
\end{description}  
\item[\bf 2.2] \marginpar{\phantom{text}2.2 $\square$} On souhaite représenter la
  distribution de la pression systolique à l'aide d'un histogramme pour
  chacun des trois groupes. Quelle commande est la plus appropriée ?
  \begin{description}
  \item[A.] \verb|histogram(SBP, raceth, data=d)|
  \item[B.] \verb+histogram(~ SBP | raceth, data=d)+
  \item[C.] \verb|histogram(SBP ~ raceth, data=d)|
  \item[D.] Je ne sais pas.
  \end{description}  
\item[\bf 2.3] \marginpar{\phantom{text}2.3 $\square$} On réalise une ANOVA à l'aide de la
  commande suivante :
\begin{verbatim}
> summary(aov(SBP ~ raceth, data=d))
              Df Sum Sq Mean Sq F value Pr(>F)
raceth         1    945   945.5   2.613  0.106
Residuals   2761 999057   361.8
\end{verbatim}
  Est-ce la commande appropriée ?  
  \begin{description}
  \item[A.] Oui.
  \item[B.] Non.
  \item[C.] Je ne sais pas.
  \end{description}  
\item[\bf 2.4] \marginpar{\phantom{text}2.4 $\square$} Que réalise la commande suivante ? 
\begin{verbatim}
bartlett.test(SBP ~ as.factor(raceth), data=d)
\end{verbatim}
  \begin{description}
  \item[A.] Elle permet de tester l'hypothèse d'égalité des variances (par
    rapport à la variable réponse \texttt{SBP}) dans les groupes définis par le
    facteur \texttt{raceth}.
  \item[B.] Elle permet de tester l'hypothèse de normalité de la distribution
    de \texttt{SBP} dans les groupes définis par le facteur \texttt{raceth}.
  \item[C.] Elle permet de vérifier que les effectifs par niveaux de
    \texttt{raceth} sont suffisants pour réaliser une ANOVA à un facteur.
  \item[D.] Je ne sais pas.
  \end{description}  
\end{description}


\section*{Exercice 3}
On s'intéresse à l'effet de différentes solutions à base de sucre sur la
croissance de pois de culture en présence d'auxine (phytohormone de
croissance végétale). Le diamètre des pois est mesuré en unité oculaire
($\times 0.114$ = mm). Il était prévu de disposer de 10 répliques par
traitement, mais certaines cultures ont dû être retirées en cours
d'expérimentation \citep[p.~218, données modifiées du tableau
9.4]{sokal95}. Les données, disponibles dans le fichier \texttt{peas.txt},
sont résumées dans le tableau suivant. Le fichier texte contient exactement
les mêmes données, présentées sous la même forme mais sans le nom des
colonnes. 
\vskip1em

\begin{tabular}{ccccc}
  \toprule
  Contrôle & 2~\% G & 2~\% F & 1~\% G + 1~\% F & 2~\% S \\
  \midrule
  75 & 57 & 58 & 58 & 62 \\
  67 & 58 & 61 & 59 & 66 \\
  70 & 60 & -  & 58 & 65 \\
  75 & 59 & 58 & 61 & 63 \\
  65 & 62 & 57 & 57 & 64 \\
  71 & 60 & 56 & -  & 62 \\
  67 & 60 & 61 & 58 & -  \\
  67 & 57 & 60 & 57 & -  \\
  76 & -  & 57 & 57 & 62 \\
  68 & 61 & 58 & 59 & 67 \\
  \bottomrule
  \multicolumn{5}{l}{\small G = glucose, F = fructose, S = sucrose} \\
\end{tabular}
\vskip1em

\begin{description}
\item[\bf 3.1] \marginpar{\phantom{text}3.1 $\square$} En supposant que le tableau de
  données (10 lignes x 5 colonnes) aît été stocké dans un
  \texttt{data.frame} appelé \texttt{peas} sous \R, de la manière suivante :
\begin{verbatim}
> tx <- c("C","2G","2F","1G1F","2S")
> peas <- read.table("peas.txt", header=FALSE, na.strings="-", col.names=tx)
> head(peas)
   C X2G X2F X1G1F X2S
1 75  57  58    58  62
2 67  58  61    59  66
3 70  60  NA    58  65
4 75  59  58    61  63
5 65  62  57    57  64
6 71  60  56    NA  62
\end{verbatim}
  quelle commande doit-on utiliser pour compter le nombre
  d'observations manquantes par traitement ? 
  \begin{description}
  \item[A.] \verb|summary(is.na(peas))|
  \item[B.] \verb|table(is.na(peas))|
  \item[C.] \verb|apply(peas, 1, function(x) sum(is.na(x))|
  \item[D.] \verb|sapply(peas, function(x) sum(is.na(x)))|
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 3.2] \marginpar{\phantom{text}3.2 $\square$} Plutôt que de travailler avec un
  tableau à 5 colonnes, on décide de convertir les données en une structure
  plus facile à manipuler, c'est-à-dire un tableau dans lequel on indique
  dans la première colonne le type de traitement auquel les poids ont été
  soumis, et dans la seconde le diamètre des poids. Les données devraient
  donc ressembler à l'aperçu ci-dessous (11 premières observations
  représentées uniquement) :
\begin{verbatim}
Using  as id variables
      tx value
1      C    75
2      C    67
3      C    70
4      C    75
5      C    65
6      C    71
7      C    67
8      C    67
9      C    76
10     C    68
11   X2G    57
\end{verbatim}
  Voici la commande que l'on se propose d'utiliser afin de réaliser cette
  opération :
\begin{verbatim}
> library(reshape)
> melt(peas, variable_name="tx")
\end{verbatim}
  Cette commande est-elle correcte ? 
  \begin{description}
  \item[A.] Oui. 
  \item[B.] Non.
  \item[C.] Je ne sais pas.
  \end{description}  
\item[\bf 3.3] \marginpar{\phantom{text}3.3 $\square$} On souhaite représenter sous forme
  d'un diagramme en points l'ensemble des mesures individuelles (diamètre,
  en abscisses) pour chacune des conditions expérimentales (traitement, en
  ordonnées). On supposera que le tableau est à présent correctement
  spécifié sous forme de deux colonnes, comme indiqué à
  l'exercice~3.2. Quelle commande est la plus appropriée ?
  \begin{description}
  \item[A.] \verb|striplot(value ~ tx, data=peas, jitter=TRUE)|
  \item[B.] \verb|striplot(tx ~ value, data=peas, jitter=TRUE)|
  \item[C.] \verb|dotplot(value ~ tx, data=peas, jitter=TRUE)|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 3.4] \marginpar{\phantom{text}3.4 $\square$} Voici le résultat d'une ANOVA à un
  facteur réalisé sur la structure de données décrite à l'exercice~3.2 :
\begin{verbatim}
            Df Sum Sq Mean Sq F value   Pr(>F)    
tx           4  989.6  247.41   42.37 7.13e-14 ***
Residuals   40  233.6    5.84                     
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
5 observations deleted due to missingness
\end{verbatim}
  Quelle commande permet de retrouver les degrés de liberté associés à la
  résiduelle ? 
  \begin{description}
  \item[A.] \verb|nrow(peas)-nlevels(peas$tx)|
  \item[B.] \verb|length(peas$value)-nlevels(peas$tx)|
  \item[C.] \verb|sum(!is.na(peas$value))-nlevels(peas$tx)|
  \item[D.] Je ne sais pas.
  \end{description}  
\item[\bf 3.5] \marginpar{\phantom{text}3.5 $\square$} Quelle commande fournit les
  moyennes de groupe ?
  \begin{description}
  \item[A.] \verb|with(peas, tapply(value, tx, mean))|
  \item[B.] \verb|with(peas, tapply(tx, value, mean))|
  \item[C.] \verb|tapply(peas$value, peas$tx, mean, na.rm=TRUE)|
  \item[D.] Je ne sais pas.
  \end{description}  
\item[\bf 3.6] \marginpar{\phantom{text}3.6 $\square$} On souhaite à présent comparer les
  traitements 2~\%~G et 2~\%~S. Quelle procédure peut-on utiliser ?
  \begin{description}
  \item[A.] \verb+summary(aov(value ~ tx, data=peas, subset=tx == "X2G" | tx == "X2S"))+
  \item[B.] \verb|summary(aov(value ~ tx, data=peas, subset=tx == "X2G" & tx == "X2S"))|
  \item[C.] Je ne sais pas.
  \end{description}
\item[\bf 3.7] \marginpar{\phantom{text}3.7 $\square$} Le résultat obtenu en 3.6 peut être
  retrouvé à partir de la commande :
  \begin{description}
  \item[A.] \verb|with(peas, t.test(value[tx=="X2G"], value[tx=="X2S"]))|
  \item[B.] \verb|with(peas, t.test(value[tx=="X2G"], value[tx=="X2S"]), var.equal=TRUE)|
  \item[C.] \verb|with(peas, t.test(value[tx=="X2G"], value[tx=="X2S"], var.equal=TRUE))|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 3.8] \marginpar{\phantom{text}3.8 $\square$} Supposons que le
  modèle testé à l'exercice~3.4 ait été stocké dans une variable appelée
  \texttt{m}. Que renverrait la commande suivante ?
\begin{verbatim}
fitted(m)-peas$value[!is.na(peas$value)]
\end{verbatim}
  \begin{description}
  \item[A.] Les valeurs prédites par le modèle pour chaque traitement,
    c'est-à-dire les moyennes de groupe, après exclusion des valeurs
    manquantes. 
  \item[B.] Les valeurs des résidus du modèle (écarts entre valeurs prédites
    et valeurs observées).
  \item[C.] Je ne sais pas.
  \end{description}
\end{description}  

%--------------------------------------------------------------- Chapter 05 --
\chapter{Corrélation et régression linéaire}\label{chap:reg}
\Opensolutionfile{solutions}[solutions5]

\section*{Énoncés}
%
% Everitt 2011 p. 184
%
\begin{exo}\label{exo:5.1}
Une étude a porté sur une mesure de malnutrition chez 25 patients âgés de 7
à 23 ans et souffrant de fibrose kystique. On disposait pour ces patients de
différentes informations relatives aux caractéristiques antropométriques
(taille, poids, etc.) et à la fonction pulmonaire. \autocite[p.~180]{everitt01}
Les données sont disponibles dans le fichier \texttt{cystic.dat}.
\begin{description}
\item[(a)] Calculer le coefficient de corrélation linéaire entre les
  variables \texttt{PEmax} et \texttt{Weight}, ainsi que son intervalle de
  confiance à 95~\%.
\item[(b)] Tester si le coefficient de corrélation calculé en (a) peut être
  considéré comme significativement différent de 0.3 au seuil 5~\%.
\item[(c)] Afficher l'ensemble des données numériques sous forme de
  diagrammes de dispersion, soit 45 graphiques arrangés sous forme d'une
  "matrice de dispersion".
\item[(d)] Calculer l'ensemble des corrélations de Pearson et de Spearman
  entre les variables numériques. 
  %Reporter les coefficients de
  %Bravais-Pearson supérieurs à 0.7 en valeur absolue.
\item[(e)] Calculer la corrélation entre \texttt{PEmax} et \texttt{Weight},
  en contrôlant l'âge (\texttt{Age}) (corrélation partielle). Représenter
  graphiquement la covariation entre \texttt{PEmax} et \texttt{Weight} en
  mettant en évidence les deux terciles les plus extrêmes pour la variable
  \texttt{Age}. 
\end{description}
\begin{sol}
Les données étant disponibles dans un format texte où les valeurs ("champs")
sont séparées par des tabulations, on utilisera la commande
\texttt{read.table} pour les importer dans \R. Comme la première ligne dans
le fichier permet d'identifier les variables, on ajoutera l'option
\verb|header=TRUE|. 
<<ex5-1a>>=
cystic <- read.table("data/cystic.dat", header=TRUE)
str(cystic)
summary(cystic)
@ 
\cmd{read.table}\cmd{str}\cmd{summary}
On voit d'emblée que le sexe des patients n'est pas codé sous la forme d'une
variable qualitative mais d'un nombre (0/1). Bien que cela ne soit pas
fondamentalement nécessaire dans cet exercice, il est toujours préférable de
convertir les variables dans le bon format.
<<ex5-1b>>=
cystic$Sex <- factor(cystic$Sex, labels=c("M","F"))
table(cystic$Sex)
@ %$
\cmd{factor}\cmd{table}

La corrélation linéaire entre \texttt{PEmax} et \texttt{Weight} est obtenue
à l'aide de la commande \texttt{cor}, qui par défaut calcule un coefficient
de corrélation de Bravais-Pearson :
<<ex5-1c>>=
with(cystic, cor(PEmax, Weight))
@ 
\cmd{with}\cmd{cor}
La construction un peu étrange \texttt{with(cystic, ...} permet de ne pas avoir
à répéter le nom du \texttt{data.frame}, \texttt{cystic}, pour désigner les
variables d'intérêt. Autrement, on aurait écrit : \verb|cor(cystic$PEmax, cystic$Weight)|. 
La commande \texttt{cor} ne permet que l'estimation ponctuelle du
paramètre. Pour obtenir l'intervalle de confiance associé, on utilisera
directement la commande \texttt{cor.test} qui répond en même temps au test
d'hypothèse sur la nullité du coefficient dans la population.
<<ex5-1d>>=
with(cystic, cor.test(PEmax, Weight))
@
\cmd{with}\cmd{cor.test}

Pour tester si ce coefficient de corrélation est significativement différent
d'une autre valeur que 0, il est nécessaire d'utiliser la commande
\texttt{r.test} du package \texttt{psych}.
Pour tester l'hypothèse $H_0:\, \rho=0.300$ au seuil 5~\%, on indiquera la
taille de l'échantillon, la corrélation dans la population et la corrélation
observée, soit :
<<ex5-1e>>=
library(psych)
r.test(25, 0.300, 0.6363)
@ 
\cmd{r.test}

Pour afficher l'ensemble des diagrammes de dispersion, on peut utiliser la
commande de base \texttt{pairs} ou bien la solution suivante :
<<ex5-1f, fig=TRUE>>=
splom(cystic[,-3], varname.cex=0.7, cex=.8)
@
\cmd{splom}

Notons que nous avons exclus la variable \texttt{Sex} qui figure dans la
3\ieme colonne du tableau de données. Les deux autres options
(\texttt{varname.cex} et \texttt{cex}) permettent de contrôler la taille des
polices et des points.

Pour calculer les corrélations de toutes les paires de variables, on peut
toujours utiliser la commande \texttt{cor}. Par contre, il ne sera pas
possible de tester ces corrélations avec la commande \texttt{cor.test} ; il
faudrait pour cela utiliser par exemple la commande \texttt{corr.test} du
package \texttt{psych}.
<<ex5-1g>>=
round(cor(cystic[,-3]), 3)
@
\cmd{round}\cmd{cor}
Pour les corrélations de Spearman, il suffit d'ajouter l'option
\verb|method="spearman"| comme ci-dessous :
<<ex5-1h>>=
round(cor(cystic[,-3], method="spearman"), 3)
@ 
\cmd{round}\cmd{cor}

%% La question du filtrage des corrélations les plus élevées en valeur absolue
%% est plus délicate car la matrice de corrélation contient deux fois la même
%% information puisqu'elle est symétrique. Il est relativement aisé d'isoler
%% les corrélations situées dans la diagonale supérieure ; par exemple, les
%% commandes suivantes nous donnent l'ensemble de ces corrélations (sans
%% considération de la paire de variables en question) :
%% <<ex5-1i>>=
%% cor.mat <- cor(cystic[,-3])
%% cor.mat[upper.tri(cor.mat)]
%% @ 
%% \cmd{cor}\cmd{upper.tri}
%% En revanche, il est plus élégant de transformer la matrice de corrélation en
%% un tableau à 3 colonnes indiquant pour chaque paire de variables la
%% corrélation de Pearson associée :
%% <<ex5-1j>>=
%% cor.mat[upper.tri(cor.mat, diag=TRUE)] <- NA
%% library(reshape)
%% subset(melt(cor.mat), abs(value) > 0.7)
%% @ 
%% \cmd{upper.tri}\cmd{library}\cmd{subset}\cmd{melt}\cmd{abs}
%% Pour éviter les doublons, on a recodé en valeurs manquantes les éléments
%% diagonaux et sous-diagonaux de la matrice de corrélation. Ensuite, on a
%% filtré les valeurs supérieures à 0.7 en valeur absolue.

Enfin, pour estimer la corrélation partielle entre \texttt{PEmax},
\texttt{Weight} et \texttt{Age}, on utilisera la commande \texttt{pcor.test}
du package \texttt{ppcor} :
<<ex5-1k>>=
library(ppcor)
with(cystic, pcor.test(PEmax, Weight, Age))
@ 
\cmd{library}\cmd{with}\cmd{pcor.test}
On voit que la corrélation entre \texttt{PEmax} et \texttt{Weight} diminue
fortement lorsque l'on tient compte de la corrélation entre ces deux
variables et l'âge.

<<ex5-1l, fig=TRUE>>=
cystic$Age.ter <- cut(cystic$Age, breaks=quantile(cystic$Age, c(0,0.33,0.66,1)), 
                      include.lowest=TRUE)
cystic2 <- subset(cystic, as.numeric(Age.ter) %in% c(1,3))
cystic2$Age.ter <- factor(cystic2$Age.ter)
xyplot(PEmax ~ Weight, data=cystic2, groups=Age.ter, auto.key=list(corner=c(0,1)))
@ %$
\cmd{cut}\cmd{subset}\cmd{factor}\cmd{xyplot}\cmd{quantile}

\noindent Les commandes utilisées ont réalisé, dans l'ordre : la création d'une
variable \texttt{Age.ter} représentant trois classes, déterminées à partir
des trois déciles ([7,12], (12,17] et (17,23]) ; la restriction du jeu de
données initiale aux seules observations comprises dans les premier et
troisième déciles de la variable \texttt{Age} ; la suppression de la
deuxième classe d'âge devenue inutile au niveau des labels associés à la
variable \texttt{Age.ter} ; et enfin, un diagramme de dispersion de
\texttt{PEmax}, en fonction de \texttt{Weight}, en soulignant à l'aide de
symboles différents les observations situées dans chacun des deux terciles
retenus. 
\end{sol}
\end{exo}
%
% METHO TD8 Exo 1
%
\begin{exo}\label{exo:5.2}
Les données disponibles dans le fichier \texttt{quetelet.csv} renseignent
sur la pression artérielle systolique (\texttt{PAS}), l'indice de Quetelet
(\texttt{QTT}), l'âge (\texttt{AGE}) et la consommation de tabac
(\texttt{TAB}=1 si fumeur, 0 sinon) pour un échantillon de 32 hommes de plus
de 40 ans. 
\begin{description}
\item[(a)] Indiquer la valeur du coefficient de corrélation linéaire entre
  la pression artérielle systolique et l'indice de Quetelet, avec un
  intervalle de confiance à 90~\%.
\item[(b)] Donner les estimations des paramètres de la droite de régression
  linéaire de la pression artérielle sur l'indice de Quetelet.
\item[(c)] Tester si la pente de la droite de régression est différente de 0
  (au seuil 5~\%).
\item[(d)] Représenter graphiquement les variations de pression artérielle
  en fonction de l'indice de Quetelet, en faisant apparaître distinctement
  les fumeurs et les non-fumeurs avec des symboles ou des couleurs
  différentes, et tracer la droite de régression dont les paramètres ont été
  estimés en (b). 
\item[(e)] Refaire l'analyse (b-c) en restreignant l'échantillon aux
  fumeurs.
\end{description}
\begin{sol}
Les données ont été exportées à partir d'un tableur de type Excel, avec
comme séparateur de champ le ";". On pourrait utiliser la commande
\texttt{read.table} et spécifier les options adéquates (\texttt{sep} et
\texttt{header}). Heureusement, \R offre des commandes qui simplifient cette
tâche : \texttt{read.csv} lit des fichiers dans lesquels le séparateur de
champ est "," tandis que \texttt{read.csv2} traite les fichiers avec un
séparateur de type ";".
<<ex5-2a>>=
dfrm <- read.csv2("data/quetelet.csv")
head(dfrm)
dfrm$TAB <- factor(dfrm$TAB, labels=c("NF","F"))
summary(dfrm[,-1])
@ 
\cmd{read.csv2}\cmd{head}\cmd{factor}\cmd{summary}
On a profité de l'inspection rapide des premières observations indiquant que
la variable "consommation de tabac" était représentée sous forme de nombres
(0/1) pour la recoder en variable qualitative avec labels plus informatifs
(\texttt{NF}=non-fumeur, \texttt{F}=fumeur).
  
L'estimation du coefficient de corrélation liénaire et son intervalle de
confiance à 90~\% ne pose pas de problème particulier : on utilisera la même
commande qu'à l'exercice~\ref{exo:5.1}, \texttt{cor.test}, en négligeant le
test d'hypothèse mais en modifiant l'option \texttt{conf.level} qui, par
défault, est fixée à 0.95.
<<ex5-2b>>=
with(dfrm, cor.test(PAS, QTT, conf.level=0.90))
@ 
\cmd{with}\cmd{cor.test}

La commande pour effectuer une régression linéaire, simple ou multiple, est
\texttt{lm} (pour \underline{l}inear \underline{m}odel). La forme générale
du modèle est symbolisée comme suit : variable réponse ~ variable
explivative. Les commandes \texttt{summary} et \texttt{coef} permettent
d'obtenir, respectivement, un tableau résumant les coefficients de
régression estimés à partir des données et leur degré de significativité (à
partir d'un test $t$ de Student), ainsi que d'autres informations que nous
discuterons plus loin, ou simplement les coefficients de régression
(ordonnée à l'origine et pente de la droite de régression).
<<ex5-2c>>=
reg.res <- lm(PAS ~ QTT, data=dfrm)
summary(reg.res)
coef(reg.res)
@ 
\cmd{lm}\cmd{summary.lm}\cmd{coef}
Le test sur la pente est directement accessible à partir du tableau de
régression ; ici $t=6.062$ et $p<0.001$. Le coefficient associé à
\texttt{QTT} reflète l'augmentation de \texttt{PAS} (21.5 points) lorsque
\texttt{QTT} varie d'une unité.

Pour représenter les données sous forme graphique, on utilisera
\texttt{xyplot} qui permet de représenter les variations d'une variable en
fonction des valeurs prises par une autre variable. Les options utilisées
ci-après permettent d'afficher les observations avec des symboles ou des
couleurs différentes selon le statut realtif à la consommation de tabac
(\texttt{group=TAB}) ainsi que les droites de régression estimées sur ces
deux sous-échantillons (option \verb|type="r"|).
<<ex5-2d, fig=TRUE>>=
xyplot(PAS ~ QTT, data=dfrm, groups=TAB, type=c("p", "g", "r"),
       auto.key=list(points=FALSE, lines=TRUE))
@ 
\cmd{xyplot}

\noindent Il existe une autre manière de représenter la droite de régression
associée estimée sur l'ensemble de l'échantillon ou un ou plusieurs
sous-groupes. Ici, l'option \verb|type="r"| simplifie beaucoup les choses
puisqu'elle tient compte de l'option de groupement, \texttt{groups=}.

Pour restreindre l'analyse aux seules observations pour lesquelles
\verb|TAB="F"| (les fumeurs), on peut utiliser la commande \texttt{subset}
pour filtrer les lignes du tableau de données. Toutefois, cette commande
peut également être utilisée sous forme d'une option lorsque l'on utilise la
commande \texttt{lm} pour le modèle de régression linéaire.
<<ex5-2e>>=
reg.res2 <- lm(PAS ~ QTT, data=dfrm, subset=TAB=="F")
summary(reg.res2)
coef(reg.res2)
@ 
\cmd{lm}\cmd{summary.lm}\cmd{coef}
\end{sol}
\end{exo}
%
% Dupont 2009 p. 63
%
\begin{exo}\label{exo:5.3}
Dans l'étude Framingham, on dispose de donnée sur la pression artérielle
systolique (\texttt{sbp}) et l'indice de masse corporelle (\texttt{bmi}) de
2047 hommes et 2643 femmes.\autocite[p.~63]{dupont09} On s'intéresse à la
relation entre ces deux variables (après transformation logarithmique) chez
les hommes et chez les femmes séparément.
Les données sont disponibles dans le fichier \texttt{Framingham.csv}.
\begin{description}
\item[(a)] Représenter graphiquement les variations entre pression
  artérielle et IMC (\texttt{bmi}) chez les hommes et chez les femmes.
\item[(b)] Les coefficients de corrélation linéaire estimés chez les hommes
  et chez les femmes sont-ils significativement différents à 5~\% ?
\item[(c)] Estimer les paramètres du modèle de régression linéaire
  considérant la pression artérielle comme variable réponse et l'IMC comme
  variable explicative, pour ces deux sous-échantillons. Donner un
  intervalle de confiance à 95~\% pour l'estimé des pentes respectives.
\item[(d)] Tester l'égalité des deux coefficients de régression associés à
  la pente (au seuil 5~\%).
\end{description}
\begin{sol}
Cette fois, les données ont été générées à partir d'un tableur (Excel ou
autre) mais le séparateur de champ est la ",", d'où l'usage de
\texttt{read.csv} au lieu de \texttt{read.csv2} comme dans
l'exercice~\ref{exo:5.2}. 
<<ex5-3a>>=
fram <- read.csv("data/Framingham.csv")
head(fram)
str(fram)
@ 
\cmd{read.csv}\cmd{head}\cmd{str}
La variable \texttt{sex} est traitée comme une variable quantitative (1/2)
et pour faciliter l'interprétation, nous la recodons d'emblée en variable
qualitative. 
<<ex5-3b>>=
table(fram$sex)
fram$sex <- factor(fram$sex, labels=c("M","F"))
@ %$
\cmd{table}\cmd{factor}

Avant de répondre à la question concernant les variations entre pression
artérielle et IMC, on peut vouloir vérifier la présence de valeurs
manquantes. Cela ne gêne en rien l'estimation des moyennes et écart-types,
des paramètres de la droite de régression ou la représentation graphique des
données, mais cela permet de connaître le nombre de "cas complets" sur les
données d'intérêt.
<<ex5-3c>>=
apply(fram, 2, function(x) sum(is.na(x)))
@ 
\cmd{apply}\cmd{sum}\cmd{is.na}\cmd{function}
On utilise une commande \texttt{apply} pour répéter une même opération pour
chaque variable, cette opération consistant à compter (\texttt{sum}) le
nombre de valeurs manquantes (\texttt{is.na(x)}). On constate que pour l'une
des variables de notre modèles, l'IMC (\texttt{bmi}), 9 observations sont
manquantes. D'où la distribution par sexe suivante :
<<ex5-3d>>=
with(fram, table(sex[!is.na(bmi)]))
@
\cmd{with}\cmd{table}

Pour représenter les données dans un diagramme de dispersion, on prendra
garde au fait qu'en raison des "grands" effectifs par sous-groupe (plus de
2000 observations), il risque d'y avoir beaucoup de points qui se
chevauchent. Une possibilité est d'utiliser la semi-transparence et de
réduire la taille des points.
<<ex5-3e, fig=TRUE>>=
xyplot(sbp ~ bmi | sex, data=fram, type=c("p","g"), alpha=0.5, cex=0.7, pch=19)
@ 
\cmd{xyplot}

Les corrélations entre les variables \texttt{sbp} et \texttt{bmi} chez les
hommes et chez les femmes peuvent être obtenues à partir de la commande
\texttt{cor}, en restreignant bien sûr l'analyse à chacun des deux
sous-échantillons :
<<ex5-3f>>=
with(subset(fram, sex=="M"), cor(sbp, bmi, use="pair"))
with(subset(fram, sex=="F"), cor(sbp, bmi, use="pair"))
@ 
\cmd{with}\cmd{subset}\cmd{cor}
On notera que l'on a rajouté l'option \verb|use="pair"| (l'option complète
se lit \verb|"pairwise.complete.obs"| mais il est possible d'abréger les
options lorsque cela ne pose pas de problème d'ambiguïté) pour calculer les
corrélations sur l'ensemble des données observées. Pour tester si ces deux
coefficients estimés à partir des données peuvent être considérés comme
significativement différents au seuil 5~\%, il faut utiliser la commande
\texttt{r.test} du package \texttt{psych}.
<<ex5-3g>>=
library(psych)
r.test(n=2047, r12=0.23644, n2=2643, r34=0.37362)
@
\cmd{library}\cmd{r.test}

Pour vérifier l'effet de la transformation logarithmique sur la distribution
des variables \texttt{sbp} et \texttt{bmi}, on peut procéder de deux
manières : soit créer un \texttt{data.frame} contenant les valeurs de
densité de chaque histogramme, associées au type de variable (\texttt{bmi},
\texttt{log(bmi)}, \texttt{sbp} et \texttt{log(sbp)}), soit construire
séparément les quatre histogrammes et les combiner en une seule figure à
l'aide de la commande \texttt{grid.arrange} du package
\texttt{gridExtra}. Voici ce qui est obtenu avec cette deuxième solution :
<<ex5-3h, fig=TRUE>>=
library(gridExtra)
p1 <- histogram(~ bmi, data=fram)
p2 <- histogram(~ log(bmi), data=fram)
p3 <- histogram(~ sbp, data=fram)
p4 <- histogram(~ log(sbp), data=fram)
grid.arrange(p1, p2, p3, p4)
@ 
\cmd{library}\cmd{grid.arrange}

Le modèle de régression dans chaque sous groupe est établi comme suit :
<<ex5-3i>>=
reg.resM <- lm(log(sbp) ~ log(bmi), data=fram, subset=sex=="M")
reg.resF <- lm(log(sbp) ~ log(bmi), data=fram, subset=sex=="F")
summary(reg.resM)   # Hommes
confint(reg.resM)
summary(reg.resF)   # Femmes
confint(reg.resF)
@
\cmd{lm}\cmd{summary.lm}\cmd{confint}
Pour les intervalles de confiance, on utilise la commande
\texttt{confint}. On peut regrouper tous les résultats qui nous intéressent
dans un même tableau. Par exemple,
<<ex5-3j>>=
res <- data.frame(pente=c(coef(reg.resM)[2], coef(reg.resF)[2]), 
                  rbind(confint(reg.resM)[2,], confint(reg.resF)[2,]))
rownames(res) <- c("M","F")
colnames(res)[2:3] <- c("2.5 %", "97.5 %")
round(res, 3)
@
\cmd{data.frame}\cmd{rownames}\cmd{colnames}\cmd{round}\cmd{rbind}
\cmd{confint}\cmd{coef}
% FIXME: test de l'égalité des pentes
\end{sol}
\end{exo}
%
% Hosmer & Lemeshow 1989
%
\begin{exo}
À partir des données sur les poids à la naissance décrites dans
l'exercice~\ref{exo:2.4}, on cherche à étudier la relation entre le poids des
bébés (traité en tant que variable numérique, \texttt{bwt}) et deux
caractéristiques de la mère : son poids (\texttt{lwt}) et son origine
ethnique (\texttt{race}).
\begin{description}
\item[(a)] Représenter graphiquement la relation entre poids des bébés et
  poids des mères, en fonction de l'ethnicité des mères.
\item[(b)] Estimer les paramètres de la régression linéaire en considérant
  les poids des bébés comme variable réponse et les poids des mères centrés
  sur leur moyenne comme variable explicative. La pente estimée est-elle
  significative au seuil usuel de 5~\% ?
\item[(c)] Estimer les paramètres de la régression linéaire où cette fois la
  variable explicative est l'ethnicité des mères, la variable réponse
  restant le poids des bébés. Comparer la significativité du modèle dans son
  ensemble avec les résultats obtenus à partir d'une ANOVA à un facteur
  (ethnicité). 
\item[(d)] Quelle est le poids prédit pour un bébé dont la mère pèse 60 kg ?
  Donner un intervalle de confiance à 95~\% pour une prédiction ponctuelle
  en moyenne.
%% \item[(d)] Refaire l'analyse de régression décrite en (c) après avoir
%%   modifié la manière dont \R génère les contrastes pour les variables
%%   qualitatives en tapant ceci à l'invite de commande \R :
%% \begin{verbatim}
%% > options(contrasts=c("contr.sum", "contr.poly"))
%% \end{verbatim}
%% Comparer avec les résultats précédents.
\end{description}
\begin{sol}
Les données peuvent être importées et recodées exactement comme dans les
solutions proposées pour l'exercice~\ref{exo:2.4}, soit les séries de
commandes \R suivantes :
<<ex5-4a, eval=FALSE>>=
<<ex2-4a>>
<<ex2-4b>>  
@ 

On peut représenter la relation entre poids des bébés et poids des mères à
l'aide d'un simple diagramme de dispersion. Ici, on a choisi de stratifier
sur l'ethnicité des mères et d'afficher trois graphiques séparés.
<<ex5-4c, fig=TRUE>>=
xyplot(bwt ~ lwt | race, data=birthwt, layout=c(3,1), type=c("p","g"), aspect=0.8)
@ 
\cmd{xyplot}

\noindent L'option \texttt{aspect=0.8} permet de modifier le rapport
largeur/hauteur des graphiques. L'option \verb|layout=c(3,1)| permet
d'afficher les trois graphiques côte à côte (1 ligne, 3 colonnes). En
modifiant légèrement la commande précédente, il serait tout à fait possible
d'afficher toutes les observations dans le même graphique mais en utilisant
des symboles ou couleurs différents selon l'origine ethnique :
\begin{verbatim}
> xyplot(bwt ~ lwt, data=birthwt, groups=race)
\end{verbatim}
\cmd{xyplot}

Les paramètres du modèle de régression linéaire sont estimés à partir de la
commande \texttt{lm}, sachant que la commande \texttt{scale} permet de
centrer et/ou réduire une série de mesures. Ici, on souhaite uniquement
centrer les poids des mères sur leur moyenne, pas les standardiser par
unités d'écart-type.
<<ex5-4b>>=
reg.res <- lm(bwt ~ scale(lwt, scale=FALSE), data=birthwt)
summary(reg.res)
@
\cmd{lm}\cmd{summary.lm}
Le test $t$ évaluant la nullité de l'hypothèse nulle pour la pente est
significatif si l'on considère un risque de première espèce de 5~\%
($p=0.011$).

Si l'on considère l'ethnicité comme variable explicative, le modèle de
régression est évalué de la même manière :
<<ex5-4d>>=
reg.res2 <- lm(bwt ~ race, data=birthwt)
summary(reg.res2)
@
\cmd{lm}\cmd{summary.lm}
L'ANOVA nous donne :
<<ex5-4e>>=
aov.res <- aov(bwt ~ race, data=birthwt)
summary(aov.res)
@
\cmd{aov}\cmd{summary.aov}
En fait, il est tout à fait possible d'obtenir le tableau de régression
précédent en utilisant \texttt{summary.lm} au lieu de \texttt{summary},
comme on peut le vérifier dans la sortie suivante.
<<ex5-4f>>=
summary.lm(aov.res)
@
\cmd{summary.lm}
Réciproquement, on peut tout à fait afficher un tableau d'ANOVA
correspondant au modèle de régression précédent :
<<ex5-4g>>=
anova(reg.res2)
@ 
\cmd{anova}
On peut vérifier que la statistique F est identique dans les deux cas (elle
vaut 4.913, pour 2 et 186 degrés de liberté).

Les contrastes utilisés dans le modèle de régression (\texttt{reg.res2})
sont appelés contrastes de traitement et ils permettent de tester la
différence entre les scores moyens (ici, l'âge) de deux catégories d'une
variable qualitative (ici, l'ethnicité), l'une des deux catégories servant
de catégorie dite \og de référence\fg. Avec R, la catégorie de référence est
toujours le premier niveau du facteur, en suivant l'ordre lexicographique,
soit dans le cas présent le niveau \texttt{White}. Les coefficients de
régression représentent alors la différence de poids moyen entre les
bébés des mères de la catégorie \texttt{Black} \emph{versus} \texttt{White}
(-383.03), et \texttt{Other} \emph{versus} \texttt{White} (-297.44). On peut
le vérifier en calculant manuellement ces différences de moyennes :
<<ex5-4h>>=
grp.means <- with(birthwt, tapply(bwt, race, mean))
grp.means[2:3] - grp.means[1]     # 1er modèle de régression (reg.res2)
@ 
%% Si l'on change la manière de coder les contrastes, par exemple en utilisant
%% des contrastes de 
%% <<ex5-4h>>=
%% op <- options(contrasts=c("contr.sum", "contr.poly"))
%% reg.res3 <- lm(bwt ~ race, data=birthwt)
%% summary(reg.res3)
%% options(op)
%% @ 
%% \cmd{options}\cmd{lm}\cmd{sumamry.lm}
%% On peut le vérifier à partir des moyennes de groupe :
%% <<ex5-4i>>=
%% grp.means <- with(birthwt, tapply(bwt, race, mean))
%% grp.means[2:3] - grp.means[1]     # 1er modèle de régression (reg.res2)
%% grp.means[1:2] - mean(grp.means)  # 2ème modèle de régression (reg.res3)
%% @ 
%% \cmd{with}\cmd{tapply}
%% Dans le premier modèle, l'intercept vaut \texttt{grp.means[1]}, tandis
%% que dans le second modèle il s'agit de la moyenne des moyennes de
%% groupe (\texttt{mean(grp.means)}). Les deux coefficients associés aux
%% pentes représentent dans le premier cas les déviations entre
%% \texttt{Black} et \texttt{Other} par rapport à \texttt{White}, et dans
%% le second cas entre \texttt{White} et \texttt{Black} et la moyenne des
%% trois groupes.
% \url{http://bit.ly/LFkFBg}.

Pour prédire le poids d'un individu dont la mère pèse 60 kg, on utilisera la
commande \texttt{predict}, sachant que cette même commande permet également
de faire une estimation par intervalle (en moyenne ou pour une observation
future) grâce à l'option \texttt{interval}.
<<ex5-4i>>=
m <- lm(bwt ~ lwt, data=birthwt)
d <- data.frame(lwt=60)
predict(m, newdata=d, interval="confidence") 
@ 
Les colonnes \texttt{lwr} et \texttt{upr} correspondent respectivement aux
bornes inférieure et supérieure de l'IC à 95~\%.
On notera qu'il est nécessaire de fournir la valeur des cofacteurs
d'intérêt dans un \texttt{data.frame} séparé (ici, \texttt{d}). Si l'on
souhaitait obtenir plusieurs prédictions, ou utuliser plusieurs cofacteurs,
on procèderait excatement de la même manière en construisant un tableau des
valeurs d'intérêt. À titre d'illustration, voici comment on procèderait si
l'on s'intéressait à la prédiction du poids d'un bébé dont la mère pèse 55
ou 60 kg et dont la classe d'appartenance pour l'ethnicité (\texttt{race})
est \texttt{Other} :
<<ex5-4j>>=
m <- lm(bwt ~ lwt + race, data=birthwt)
d <- data.frame(lwt=c(55, 60), race=rep("Other", 2))
predict(m, d, interval="confidence")
@ 
Si l'on ne précise aucune valeur pour l'option \texttt{newdata}, R produira
une estimation de la valeur prédite pour chaque unité statistique avec les
valeurs observées pour le ou les cofacteurs présents dans le modèle.
\end{sol}
\end{exo}
\Closesolutionfile{solutions}

\chapter*{Devoir \no 4}
\addcontentsline{toc}{chapter}{Devoir \no 4}

Les exercices sont indépendants. Une seule réponse est correcte pour chaque
question. Lorsque vous ne savez pas répondre, cochez la case correspondante.

\section*{Exercice 1}
Dans la même étude sur la thérapie hormonale (cf. exercice~2 du devoir \no
3, p.~\pageref{dev3:exo2}), on disposait de l'âge des participants dont la
distribution est reportée ci-dessous :
\begin{verbatim}
      age            raceth           SBP       
 Min.   :44.00   Min.   :1.000   Min.   : 83.0  
 1st Qu.:62.00   1st Qu.:1.000   1st Qu.:122.0  
 Median :67.00   Median :1.000   Median :134.0  
 Mean   :66.65   Mean   :1.147   Mean   :135.1  
 3rd Qu.:72.00   3rd Qu.:1.000   3rd Qu.:147.0  
 Max.   :79.00   Max.   :3.000   Max.   :224.0  
\end{verbatim}
Les données sont toujours contenue dans un \texttt{data.frame} nommé
\texttt{d} sous R. La question d'intérêt porte sur la relation entre les
variables \texttt{age} et \texttt{SBP}.
\begin{description}
\item[\bf 1.1] \marginpar{\phantom{text}1.1 $\square$} On souhaite afficher un diagramme
  de dispersion où l'âge figure sur l'axe horizontal (abscisses) et la
  pression systolique sur l'axe vertical (ordonnées). Quelle commande
  peut-on utiliser ?
  \begin{description}
  \item[A.] \verb|xyplot(age, SBP, data=d)|
  \item[B.] \verb|xyplot(age ~ SBP, data=d)|
  \item[C.] \verb|xyplot(SBP ~ age, data=d)|
  \item[D.] \verb|xyplot(SBP ~ age + raceth, data=d)|
  \item[E.] Je ne sais pas.
  \end{description}
\item[\bf 1.2] \marginpar{\phantom{text}1.2 $\square$} On souhaite vérifier l'étendue des
  âges (minimum et maximum) par groupe d'ethnicité (variable
  \texttt{raceth}). Quelle commande permet de répondre à cette question ?
  \begin{description}
  \item[A.] \verb|with(d, tapply(age, raceth, range(x)))|
  \item[B.] \verb|with(d, tapply(age, raceth, function(x) min(x), max(x)))|
  \item[C.] \verb|with(d, tapply(age, raceth, range))|
  \item[D.] \verb|with(d, tapply(age, raceth, c(max(age),min(age))))|
  \item[E.] Je ne sais pas.
  \end{description}
\item[\bf 1.3] \marginpar{\phantom{text}1.3 $\square$} On souhaite calculer les déciles
  pour la variable \texttt{age}. Quelle commande faut-il utiliser ?
  \begin{description}
  \item[A.] \verb|cut(d$age, 1:10)|
  \item[B.] \verb|factor(d$age, 1:10)|
  \item[C.] \verb|quantile(d$age, breaks=seq(0,1,.1))|
  \item[D.] \verb|quantile(d$age, probs=seq(0,1,.1))|
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 1.4] \marginpar{\phantom{text}1.4 $\square$} On souhaite à présent recoder la
  variable numérique \texttt{age} en une variable qualitative \texttt{aged}
  définie de sorte que chaque valeur numérique observée est remplacée par la
  valeur du décile dans laquelle elle se situe. Supposons que le résultat
  calculé à l'exercice~1.3 ait été stocké dans une variable appelée
  \texttt{ageq}. La commande suivante fournit-elle le résultat escompté ?
\begin{verbatim}
d$aged <- cut(d$age, ageq)
\end{verbatim}
  \begin{description}
  \item[A.] Oui.
  \item[B.] Non.
  \item[C.] Je ne sais pas.
  \end{description}
\item[\bf 1.5] \marginpar{\phantom{text}1.5 $\square$} On souhaite calculer la différence
  moyenne de pression systolique entre les deux déciles les plus extrêmes
  (10~\% d'âges les plus bas et 10~\% d'âge les plus hauts). En supposant
  que la variable \texttt{aged} (âges individuels recodés sous forme de
  déciles) ait été correctement créée (voir aperçu ci-après), quelle
  solution peut-on utiliser ?
\begin{verbatim}
> summary(d$aged)
[44,58] (58,61] (61,63] (63,65] (65,67] (67,69] (69,71] (71,73] (73,75] (75,79] 
    346     304     231     264     276     323     290     289     202     238
\end{verbatim}
  \begin{description}
  \item[A.] \verb|with(d, mean(SBP[aged==10]) - mean(SBP[aged==2]))|
  \item[B.] \verb|with(d, mean(SBP[aged=="10"]) - mean(SBP[aged=="2"]))|
  \item[C.] \verb|diff(with(d, tapply(SBP, aged, mean))[c(2,10)])|
  \item[D.] \verb|diff(mean(d$SBP[d$aged %in% c("[44,58]","(75,79]")]))|
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 1.6] \marginpar{\phantom{text}1.6 $\square$} Quel résultat produit cette commande : 
\begin{verbatim}
xyplot(SBP ~ age | factor(raceth), data=d, layout=c(3,1), type="r")
\end{verbatim}
  \begin{description}
  \item[A.] Trois diagrammes de dispersion de la pression systolique en
    fonction de l'âge pour chaque sous-groupe de participants défini selon
    le facteur ethnicité (\texttt{raceth}).
  \item[B.] Un diagramme de dispersion de la pression systolique en
    fonction de l'âge de l'ensemble des participants avec des symboles
    graphiques variables selon le facteur ethnicité (\texttt{raceth}).
  \item[C.] Trois graphiques montrant la droite de régression de la
    variable \texttt{age} sur la variable réponse \texttt{SBP} pour
    chaque sous-groupe de participants défini selon le facteur ethnicité
    (\texttt{raceth}).
  \item[D.] Je ne sais pas.
  \end{description}  
\item[\bf 1.7] \marginpar{\phantom{text}1.7 $\square$} Voici un diagramme de dispersion
  résumant la distribution conjointe des mesures de pression systolique et
  d'âge. Les déciles d'âge apparaissent sous forme de disques noirs.
\begin{center}
\includegraphics{./figs/dev4_sbp}  
\end{center}
Quelle commande doit-on utiliser pour estimer le coefficient de corrélation
linéaire de Bravais-Pearson et un intervalle de confiance à 90~\% ?

\begin{description}
\item[A.] \verb|confint(cor(d$age, d$SBP))|
\item[B.] \verb|cor.test(d$age, d$SBP)|
\item[C.] \verb|with(d, cor.test(age, SBP, conf.level=0.90))|
\item[D.] \verb|with(d, cor.test(age, SBP, conf.level=90))|
\item[E.] Je ne sais pas.
\end{description}  
\end{description}

\section*{Exercice 2}
Cet exercice est basé sur les mêmes données que celles présentées à
l'exercice précédent.

\begin{description}
\item[\bf 2.1] \marginpar{\phantom{text}2.1 $\square$} On souhaite à présent réaliser une
  régression linéaire en considérant la variable \texttt{SBP} comme variable
  réponse et l'âge comme variable prédictrice. On souhaite restreindre
  l'analyse à la classe ethnique majoritaire (qui vaut 1). Quelle commande
  faut-il utiliser ?
    \begin{description}
    \item[A.] \verb|lm(SBP ~ age, data=d[raceth==1,])|
    \item[B.] \verb|lm(SBP ~ age, data=subset(d, raceth=1))|
    \item[C.] \verb|lm(SBP ~ age, data=d, subset=raceth==1)|
    \item[D.] Je ne sais pas.
    \end{description}  
  \item[\bf 2.2] \marginpar{\phantom{text}2.2 $\square$} En considérant que le modèle
    décrit en 2.1 a été stocké dans une variable appelée \texttt{mod}, que
    renverrait la commande \verb|anova(mod)| ?
    \begin{description}
    \item[A.] Le tableau des coefficients de régression avec leur
      intervalles de confiance.
    \item[B.] Le tableau des coefficients de régression sans leur
      intervalles de confiance.
    \item[C.] Le tableau d'analyse de variance correspondant au modèle de
      régression.
    \item[D.] Je ne sais pas.
    \end{description}  
  \item[\bf 2.3] \marginpar{\phantom{text}2.3 $\square$} On souhaite extraire la valeur
    estimée pour la pente de la droite de régression à partir des
    coefficients du modèle. On se propose d'utiliser
    \verb|coef(mod)[j]|. Quelle valeur doit-on spécifier pour \texttt{j} ?
  \begin{description}
  \item[A.] 0
  \item[B.] 1
  \item[C.] 2
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 2.4] \marginpar{\phantom{text}2.4 $\square$} En supposant \texttt{j}
  correctement défini en 2.3, pour obtenir un intervalle de confiance à
  95~\% pour l'estimé de la pente de régression, on utilise la commande :
  \begin{description}
  \item[A.] \verb|confint(coef(mod)[j])|
  \item[B.] \verb|confint(mod)[j]|
  \item[C.] \verb|confint(mod)[j,]|
  \item[D.] Je ne sais pas.
  \end{description}  
\item[\bf 2.5] \marginpar{\phantom{text}2.5 $\square$} Quelle hypothèse de la régression
  linéaire la commande graphique suivante permet-elle de vérifier ?
\begin{verbatim}
xyplot(fitted(mod) ~ resid(mod))    
\end{verbatim}
  \begin{description}
  \item[A.] L'indépendance des observations.
  \item[B.] La constance de la variance.
  \item[C.] La normalité des résidus.
  \item[D.] Je ne sais pas.
  \end{description}
\end{description}

\section*{Exercice 3}
Considérons l'étude d'obstétrique décrite à l'exercice~4.3
(p.~\pageref{exo:4.3}). En plus du poids des bébés à la naissance, on
dispose de mesures de leur taille et de leur périmètre crânien. On cherche à
établir un lien entre ces trois variables. Pour rappel, voici un aperçu des
données (6 premières observations) après avoir importé les données sous \R :
\begin{verbatim}
> head(weights)
    ID WEIGHT LENGTH HEADC GENDER EDUCATIO             PARITY
1 L001   3.95   55.5  37.5 Female tertiary 3 or more siblings
2 L003   4.63   57.0  38.5 Female tertiary          Singleton
3 L004   4.75   56.0  38.5   Male   year12         2 siblings
4 L005   3.92   56.0  39.0   Male tertiary        One sibling
5 L006   4.56   55.0  39.5   Male   year10         2 siblings
6 L007   3.64   51.5  34.5 Female tertiary          Singleton
\end{verbatim}
Les variables d'intérêt sont : \texttt{WEIGHT} (poids, en kg),
\texttt{LENGTH} (taille, en cm) et \texttt{HEADC} (périmètre crânien, en
cm). 
\begin{description}
\item[\bf 3.1] \marginpar{\phantom{text}3.1 $\square$} On souhaite afficher les
  covariations entre ces trois mesures sous forme d'une matrice de
  diagrammes de dispersion. Quelle commande doit-on utiliser ?
  \begin{description}
  \item[A.] \verb|xyplot|
  \item[B.] \verb|stripplot|
  \item[C.] \verb|splom|
  \item[D.] \verb|splot|
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 3.2] \marginpar{\phantom{text}3.2 $\square$} Quelle commande permettrait
  d'afficher les identifiants des sujets dont le poids est inférieur
  (strictement) à 3500 g et la taille égale ou supérieure au premier
  quartile des tailles enregistrées chez ces 550 bébés ?
  \begin{description}
  \item[A.] \verb|subset(weights, WEIGHT < 3500 & LENGTH >= quantile(LENGTH, 0.25), ID)|
  \item[B.] \verb|subset(weights, WEIGHT < 3.500 & LENGTH >= quantile(LENGTH, 0.25), ID)|
  \item[C.] \verb|weights$ID[WEIGHT < 3.500 & LENGTH >= quantile(LENGTH, 0.25)]|
  \item[D.] Je ne sais pas.
  \end{description}  
\item[\bf 3.3] \marginpar{\phantom{text}3.3 $\square$} En supposant le résultat précédent
  (liste des identifiants des sujets remplissant les conditions mentionnées
  en 3.2) sauvegardé dans une variable appelée \texttt{idx}, comment
  calculerait-on le périmètre crânien moyen de ces individus ?
\begin{verbatim}
> idx
 [1] L077 L104 L236 W054 W093 W123 W172 W189 W214 W249 W303
550 Levels: L001 L003 L004 L005 L006 L007 L008 L009 L010 L011 L013 L014 ... W323
\end{verbatim}
  \begin{description}
  \item[A.] \verb|mean(weights$HEADC)[idx]|
  \item[B.] \verb|mean(weights$HEADC[weights$ID == idx])|
  \item[C.] \verb|mean(weights$HEADC[weights$ID %in% idx])|
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 3.4] \marginpar{\phantom{text}3.4 $\square$} Voici le résultat des deux
  régressions des variables \texttt{LENGTH} et \texttt{HEADC} sur le poids
  (\texttt{WEIGHT}) des bébés. Quel modèle explique la plus grande part de
  variance ?  
\begin{verbatim}
> summary(lm(WEIGHT ~ HEADC, data=weights))
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -6.05987    0.56036  -10.81   <2e-16 ***
HEADC        0.27513    0.01478   18.62   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 0.4714 on 548 degrees of freedom
Multiple R-squared: 0.3875,Adjusted R-squared: 0.3863 
F-statistic: 346.6 on 1 and 548 DF,  p-value: < 2.2e-16 

> summary(lm(WEIGHT ~ LENGTH, data=weights))
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -5.412145   0.411040  -13.17   <2e-16 ***
LENGTH       0.178308   0.007488   23.81   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 0.4223 on 548 degrees of freedom
Multiple R-squared: 0.5085,Adjusted R-squared: 0.5076 
F-statistic:   567 on 1 and 548 DF,  p-value: < 2.2e-16 
\end{verbatim}
  \begin{description}
  \item[A.] \verb|lm(WEIGHT ~ HEADC, data=weights)|
  \item[B.] \verb|lm(WEIGHT ~ LENGTH, data=weights)|
  \item[C.] Je ne sais pas.
  \end{description}  
\item[\bf 3.5] \marginpar{\phantom{text}3.5 $\square$} Pour calculer le poids attendu d'un
  bébé ayant un périmètre crânien de 37 cm (1\ier quartile des périmètres
  crâniens observés dans l'échantillon), quelle commande doit-on utiliser ?
  \begin{description}
  \item[A.] \verb|predict(lm(WEIGHT ~ HEADC, data=weights), 37)|
  \item[B.] \verb|predict(lm(WEIGHT ~ HEADC, data=weights), HEADC=37)|
  \item[C.] \verb|predict(lm(WEIGHT ~ HEADC, data=weights), data.frame(HEADC=37))| 
  \item[D.] Je ne sais pas.
  \end{description}  
\end{description}

%--------------------------------------------------------------- Chapter 06 --
\chapter{Mesures d'association en épidémiologie et régression logistique}\label{chap:logistic}
% FIXME:
% exo kappa ici ou dans chapitre comparaison deux groupes ?
\Opensolutionfile{solutions}[solutions6]



\section*{Énoncés}
%
% RC TD3 exo 2
%
\begin{exo}\label{exo:6.1}
On étudie l'effet d'un traitement prophylactique d'un macrolide à faibles
doses (Traitement A) sur les épisodes infectieux chez des patients atteints
de mucoviscidose dans un essai randomisé multicentrique contre placebo
(B). Les résultats sont les suivants :
\vskip1em

\begin{tabular}{lccc}
\toprule
& \multicolumn{2}{c}{Infection} & \\
\cmidrule(r){2-3}
& Non & Oui & Total \\
\midrule
Traitement (A) & 157 & 52 & 209 \\
Placebo (B) & 119 & 103 & 222 \\
Total & 276 & 155 & 431 \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] À partir d'un test du $\chi^2$, que peut-on répondre à la
  question : le traitement permet-il de prévenir la survenue d'épisodes
  infectieux (au seuil $\alpha=0.05$) ? Vérifier que les effectifs
  théoriques sont bien tous supérieurs à 5.
\item[(b)] Conclut-on de la même manière à partir de l'intervalle de
  confiance de l'odds-ratio associé à l'effet traitement ?
\item[(c)] On souhaite vérifier s'il existe une disparité du point de vue
  des pourcentages d'épisodes infectieux en fonction du centre. Les données
  par centre sont indiquées dans le tableau ci-après. Conclure à partir d'un
  test du $\chi^2$.

  \begin{table}[!htb] \hskip40pt
  \begin{minipage}[b]{0.33\linewidth}
  \scalebox{0.65}{\begin{tabular}{|l|r|r|r|}
    \multicolumn{1}{c}{} & \multicolumn{2}{c}{Infection} &  \multicolumn{1}{c}{} \\
    \cline{2-4}
    \multicolumn{1}{c|}{} & Non & Oui & Total \\
    \hline
    Traitement (A) & 51 & 8 & 59 \\
    \hline
    Placebo (B) & 47 & 19 & 66 \\
    \hline
    Total & 98 & 27 & 125 \\
    \hline
    \multicolumn{4}{c}{Centre 1}
  \end{tabular}} 
  \end{minipage} \hspace{0.1cm}
  \begin{minipage}[b]{0.3\linewidth}
  \scalebox{0.65}{\begin{tabular}{|l|r|r|r|}
    \multicolumn{1}{c}{} & \multicolumn{2}{c}{Infection} &  \multicolumn{1}{c}{} \\
    \cline{2-4}
    \multicolumn{1}{c|}{} & Non & Oui & Total \\
    \hline
    Traitement (A) & 91 & 35 & 126 \\
    \hline
    Placebo (B) & 61 & 71 & 132 \\
    \hline
    Total & 152 & 106 & 258 \\
    \hline
    \multicolumn{4}{c}{Centre 2}
  \end{tabular}} 
  \end{minipage} \hspace{0.1cm}
  \begin{minipage}[b]{0.3\linewidth}
  \scalebox{0.65}{\begin{tabular}{|l|r|r|r|}
    \multicolumn{1}{c}{} & \multicolumn{2}{c}{Infection} &  \multicolumn{1}{c}{} \\
    \cline{2-4}
    \multicolumn{1}{c|}{} & Non & Oui & Total \\
    \hline
    Traitement (A) & 15 & 9 & 24 \\
    \hline
    Placebo (B) & 11 & 13 & 24 \\
    \hline
    Total & 26 & 22 & 48 \\
    \hline
    \multicolumn{4}{c}{Centre 3}
  \end{tabular}}
  \end{minipage}
  \end{table}
\item[(d)] À partir du tableau précédent, on cherche à vérifier si l'effet
  traitement est indépendent du centre ou non. On se propose de réaliser un
  test de comparaison entre les deux traitements ajustés sur le centre (test
  de Mantel-Haenszel). Indiquer le résultat du test ainsi que la valeur de
  l'odds-ratio ajusté.
\end{description}
\begin{sol}
Dans un premier temps, il faut créer le tableau de données sous \R.
<<ex6-1a>>=
macrolid <- matrix(c(157,119,52,103), nr=2, 
                   dimnames=list(traitement=c("A","B"), 
                     infection=c("Non","Oui")))
macrolid
@ 
\cmd{matrix}

On peut représenter graphiquement les données en considérant les proportions
relatives d'épisodes infectieux selon le type de traitement (52/209 contre
103/222) sous forme de diagrammes en barres :
<<ex6-1b, fig=TRUE>>=
barchart(prop.table(macrolid, 1), xlab="Fréquence relative", 
         auto.key=list(space="top", column=2, title="Infection"))
@ 
\cmd{barchart}\cmd{prop.table}

Le test du $\chi^2$ est simple à mettre en \oe uvre à l'aide de la commande
\texttt{chisq.test} :
<<ex6-1c>>=
chisq.test(macrolid, correct=FALSE)
@
\cmd{chisq.test}
On a désactivé la correction de continuité proposée par défaut par \R avec
l'option \verb|correct=FALSE|. Le résultat du test suggère l'existence d'une
association entre le traitement par macrolides et la survenue d'un épisode
infectieux au seuil usuel de 5~\%. On peut obtenir les effectifs théoriques
à partir de la même commande :
<<ex1-6d>>=
chisq.test(macrolid, correct=FALSE)$expected
@ %$
\cmd{chisq.test}
En règle générale, \R signalera que l'approximation par la distribution du
$\chi^2$ est incorrecte dans le cas où les effectifs théoriques sont trop
petits. 

L'odds-ratio peut se calculer manuellement, en tenant compte de l'ordre de
présentation des modalités des variables dans le tableau (il faut
intervertir l'ordre des colonnes) :
<<ex1-6e>>=
(52*119)/(103*157)
macrolid.bis <- matrix(nc=2, nr=2)
macrolid.bis[,1:2] <- macrolid[,2:1]  # échange colonne 1 et 2
or <- (macrolid.bis[1,1]*macrolid.bis[2,2]) / (macrolid.bis[2,1]*macrolid.bis[1,2])
or
@ 
\cmd{matrix}
de même que son intervalle de confiance
<<ex1-6f>>=
se <- sqrt(sum(1/macrolid.bis))  # erreur standard
or * exp(qnorm(0.025)*se)        # borne inf. 95 %
or * exp(qnorm(0.975)*se)        # borne sup. 95 %
@ 
\cmd{sqrt}\cmd{sum}\cmd{exp}\cmd{qnorm}
Mais en règle générale on préfèrera utiliser la commande \texttt{oddsratio}
du package \texttt{vcd}.
<<ex1-6g>>=
library(vcd)
oddsratio(macrolid.bis, log=FALSE)
confint(oddsratio(macrolid.bis, log=FALSE))
@ 
\cmd{library}\cmd{oddsratio}\cmd{confint}
On concluerait donc de la même manière qu'avec le test $\chi^2$ puisque
l'intervalle de confiance à 95~\% pour l'odds-ratio ne contient pas la
valeur 1 (signifiant l'absence d'association entre les deux variables
étudiées). On notera qu'il existe également une commande relativement
pratique pour les analyses épidémiologiques dans le package \texttt{epiR} ;
en ce qui concerne le calcul de l'odds-ratio, il faudra tout de même prendre
garde à la manière dont le tableau de contingence est arrangé. Pour utiliser
la commande \texttt{epi.2by2} de ce package, il faudra échanger les colonnes
du tableau \texttt{macrolid}, soit
<<ex6-1g2>>=
library(epiR)
epi.2by2(macrolid[,c(2,1)])
@ 

Pour vérifier une éventuelle disparité entre les centres, il nous faut dans
un premier temps calculer les pourcentages d'épisodes infectieux dans chaque
centre. On peut le faire manuellement à partir des tableaux données dans
l'énoncé (par exemple, pour le centre 1 il s'agit de 27/125), auquel cas il
nous suffirait de créer un tableau avec les totaux colonnes :
<<ex1-6h>>=
macrolid.centre <- matrix(c(98,27,152,106,26,22), nr=3, byrow=TRUE)
rownames(macrolid.centre) <- paste("Centre", 1:3, sep=":")
colnames(macrolid.centre) <- c("Non","Oui")
macrolid.centre
@
\cmd{matrix}\cmd{rownames}\cmd{colnames}\cmd{paste}
Le test du $\chi^2$ est ensuite facile à réaliser :
<<ex1-6i>>=
chisq.test(macrolid.centre)
@ 
\cmd{chisq.test}
Mais comme à la question suivante on aura besoin de l'ensemble des données,
on peut également décider de travailler avec les trois tableaux. Dans ce
cas, pour répondre à la question de l'hétérogénéité entre centre du point de
vue du critère principal, il nous faudra recalculer les effectifs marginaux.
<<ex1-6j>>=
tab1 <- matrix(c(8,19,51,47), nr=2)
tab2 <- matrix(c(35,71,91,61), nr=2)
tab3 <- matrix(c(9,13,15,11), nr=2)
colnames(tab1) <- colnames(tab2) <- colnames(tab3) <- c("Oui","Non")
rownames(tab1) <-rownames(tab2) <- rownames(tab3) <- c("A","B")
@
\cmd{matrix}\cmd{colnames}\cmd{rownames}
Une fois les trois tableaux saisis, on calcule et on assemble les marges ainsi :
<<ex1-6k, eval=FALSE>>=
macrolid.centre <- rbind(centre1=apply(tab1, 2, sum), 
                         centre2=apply(tab2, 2, sum), 
                         centre3=apply(tab3, 2, sum))
chisq.test(macrolid.centre)
@ 
\cmd{rbind}\cmd{chisq.test}
La dernière commande produit bien évidemment le même résultat que celui
affiché précédemment.

Enfin, pour réaliser un test de Mantel-Haenszel, il faut réarranger les
données de sorte que chacun des trois tableaux construits à l'étape
précédent soit bien interprété par \R comme désignant une strate. Pour cela,
on utilise la commande \texttt{array} qui permet de généraliser la commande
\texttt{matrix} à des tableaux à plus de deux dimensions.
<<ex1-6l>>=
macrolid2 <- array(c(tab1, tab2, tab3), dim=c(2,2,3))
dimnames(macrolid2) <- list(c("A","B"), c("Oui","Non"), paste("Centre", 1:3, sep=":"))
macrolid2
mantelhaen.test(macrolid2)
@ 
\cmd{array}\cmd{dimnames}\cmd{mantelhaen.test}
La valeur de l'odds-ratio ajusté figure à la fin du résultat produit par \R
: dans ce cas, il est estimé à 0.362, avec un intervalle de confiance à
95~\% de [0.238–0.551]. Le résultat du test indique bien entendu que
l'odds-ratio ajusté peut être considéré comme significativement différent de
1 (absence d'association).

Le graphique suivant montre les trois odds-ratio et leurs intervalles de
confiance à 95~\%.
<<ex1-6m, fig=TRUE>>=
tab1.or <- oddsratio(tab1, log=FALSE)
tab2.or <- oddsratio(tab2, log=FALSE)
tab3.or <- oddsratio(tab3, log=FALSE)
dfrm <- data.frame(centre=factor(1:3), or=c(tab1.or, tab2.or, tab3.or), 
                   rbind(confint(tab1.or), confint(tab2.or), confint(tab3.or)))
dfrm
# -- %< -----
prepanel.ci <- function(x, y, lx, ux, subscripts, ...) {
  x <- as.numeric(x)
  lx <- as.numeric(lx[subscripts])
  ux <- as.numeric(ux[subscripts])
  list(ylim = range(x, ux, lx, finite=TRUE))
}
panel.ci <- function(x, y, lx, ux, subscripts, pch=16, ...) {
  x <- as.numeric(x)
  y <- as.numeric(y)
  lx <- as.numeric(lx[subscripts])
  ux <- as.numeric(ux[subscripts])
  panel.abline(h = 1, col = "black", lty = 2)
  panel.abline(v = unique(x), col = "grey")
  panel.arrows(x, lx, x, ux, col = 'black',
               length = 0.05, unit = "native",
               angle = 90, code = 3)
  panel.xyplot(x, y, pch = pch, ...)
}   
# -- %< -----
xyplot(or ~ centre, data=dfrm, pch=20, ylab="Odds-ratio (IC 95 %)", 
       lx=dfrm$lwr, ux=dfrm$upr, prepanel=prepanel.ci, panel=panel.ci)
@
\cmd{xyplot}\cmd{oddsratio}\cmd{data.frame}
\end{sol}
\end{exo}
%
% Pepe 2004 p. 22
%
\begin{exo}\label{exo:6.2}
Voici les résultats d'une étude de cohorte visant à déterminer, entre
autres, l'intérêt d'utliser comme outil de screening une mesure de test
d'effort physique (EST), pour lequel un résultat de type réussite/échec peut
être dérivé, lors du diagnostic d'une maladie coronarienne
(CAD).\autocite{pepe04}  
\vskip1em

\begin{tabular}{l|l|c|c|c}
\multicolumn{2}{c}{}&\multicolumn{2}{c}{CAD}&\\
\cline{3-4}
\multicolumn{2}{c|}{}&Non-malade&Malade&\multicolumn{1}{c}{Total}\\
\cline{2-4}
\multirow{2}{*}{EST}& Négatif & 327 & 208 & 535\\
\cline{2-4}
& Positif & 115 & 815 & 930\\
\cline{2-4}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{Total} & \multicolumn{1}{c}{442} & 
\multicolumn{1}{c}{1023} & \multicolumn{1}{c}{1465}\\
\end{tabular}
\vskip1em
On fera l'hypothèse qu'il n'y a pas de biais de vérification.

\begin{description}
\item[(a)] À partir de cette matrice de confusion, indiquer les valeurs
  suivantes (avec intervalles de confiance à 95~\%) : sensibilité et
  spécificité, valeur prédictive positive et négative. 
\item[(b)] Quelle est la valeur de l'aire sous la courbe pour les données
  reportées ?
\end{description}
\begin{sol}
La création du tableau de données peut se faire comme indiqué ci-après, à
partir d'un tableau de type \texttt{matrix} :
<<ex6-2a>>=
tab <- as.table(matrix(c(815,115,208,327), nrow=2, byrow=TRUE, 
                       dimnames=list(EST=c("+","-"), CAD=c("+","-"))))
tab
@   
\cmd{matrix}
On notera que le tableau a été légèrement réorganisé de manière à
correspondre aux notations habituelles, avec les événements \og positifs\fg\
sur la première ligne (typiquement, l'exposition) et la première colonne
(typiquement, la maladie) du tableau.

Il serait tout à fait possible de calculer en très peu d'opérations
arithmétiques avec \R toutes les quantités demandées. Cependant, le package
\texttt{epiR} contient toutes les commandes nécessaires pour répondre aux
questions épidémiologiques sur les tableaux $2\times 2$. Ainsi, la commande
\texttt{epi.tests} fournit les valeurs de prévalence,
sensibilité/spécificité, et valeurs prédictive positive et négative.
<<ex6-2b>>=
library(epiR)
epi.tests(tab)
@ 
Concernant l'aire sous la courbe, on peut également utiliser une commande
externe, \texttt{roc.from.table}, dans le package \texttt{epicalc}.
<<>>=
library(epicalc)
roc.from.table(tab, graph=FALSE)
@ 
\cmd{library}\cmd{roc.from.table}
\end{sol}
\end{exo}
%
% RC TD8 diagnostic cutoff
%
\begin{exo}\label{exo:6.3}
On dispose de données issues d'une étude cherchant à établir la validité
pronostique de la concentration en créatine kinase dans l'organisme sur la
prévention de la survenue d'un infarctus du myocarde.\autocite[p.~115]{rabe-hesketh04}

Les données sont disponibles dans le fichier \texttt{sck.dat} : la première
colonne correspond à la variable créatine kinase (\texttt{ck}), la deuxième
à la variable présence de la maladie (\texttt{pres}) et la dernière à la
variable absence de maladie (\texttt{abs}).
\begin{description}
\item[(a)] Quel est le nombre total de sujets ?
\item[(b)] Calculer les fréquences relatives malades/non-malades, et
  représenter leur évolution en fonction des valeurs de créatine kinase à
  l'aide d'un diagramme de dispersion (points + segments reliant les points).
\item[(c)] À partir d'un modèle de régression logistique dans lequel on
  cherche à prédire la probabilité d'être malade, calculer la valeur de
  \texttt{ck} à partir de laquelle ce modèle prédit que les personnes
  présentent la maladie en considérant une valeur seuil de 0.5
  (si $P(\text{malade})\ge 0.5$ alors \texttt{malade=1}).
\item[(d)] Représenter graphiquement les probabilités d'être malade prédites
  par ce modèle ainsi que les proportions empiriques en fonction des valeurs
  \texttt{ck}. 
\item[(e)] Établir la courbe ROC correspondante, et reporter la
  valeur de l'aire sous la courbe. 
%\item[(f)] Quelle est la valeur de seuil optimisant le compromis
%  sensibilité/spécificité ?
\end{description}
\begin{sol}
Comme le nom des variables ne figure pas dans le fichier de données, il
faudra les attribuer aussitôt après avoir importé les données.
<<ex6-3a>>=
sck <- read.table("data/sck.dat", header=FALSE)
names(sck) <- c("ck", "pres", "abs")
summary(sck)
@ 
\cmd{read.table}\cmd{names}\cmd{summary}

Le nombre total de sujets correspond à la somme des effectifs pour les deux
variables \texttt{pres} et \texttt{abs}, soit
<<ex6-3b>>=
sum(sck[,c("pres","abs")])
@ 
\cmd{sum}
ou de manière équivalente : \verb|sum(sck$pres) + sum(sck$abs)| (mais pas
\verb|sck$pres + sck$abs| !).

Pour calculer les fréquences relatives de ces deux variables, il est
nécessaire de connaître les effectifs totaux par variable. Ceux-ci peuvent
être obtenu en utilisant la commande \texttt{apply} et en opérant par
colonnes :
<<ex6-3c>>=
ni <- apply(sck[,c("pres","abs")], 2, sum)
@ 
\cmd{apply}
À partir de là, il suffit de diviser chaque valeur des variables
\texttt{pres} et \texttt{abs} par les nombres calculés ci-dessus. On
stockera les valeurs obtenues dans deux nouvelles variables, dans le même
tableau de données.
<<ex6-3d>>=
sck$pres.prop <- sck$pres/ni[1]
sck$abs.prop <- sck$abs/ni[2]
@ 
On peut vérifier que les calculs sont corrects : la somme des valeurs pour
chaque variable doit maintenant valoir 1 :
<<ex6-3e>>=
apply(sck[,c("pres.prop","abs.prop")], 2, sum)
@ 
\cmd{apply}
Il suffit ensuite de représenter les proportions obtenues dans un même
graphique, en considérant les valeurs de la variable \texttt{ck} comme
abscisses.
<<ex6-3f, fig=TRUE>>=
xyplot(pres.prop + abs.prop ~ ck, data=sck, type=c("b", "g"),
       auto.key=TRUE, ylab="Fréquence")
@ 
\cmd{xyplot}

L'instruction \verb|type=c("b", "g")| signifie que l'on souhaite afficher
des points reliés par des lignes (\verb|"b"|=\verb|"o"|+\verb|"l"|)
ainsi qu'un quadrillage (\verb|"g"|).

Le modèle de régression pour données groupées
<<ex6-3g>>=
glm.res <- glm(cbind(pres, abs) ~ ck, data=sck, family=binomial)
summary(glm.res)
@ 
\cmd{glm}\cmd{summary.glm}

Les prédictions, exprimées sous forme de probabilités et non sur l'échelle
log-odds, sont obtenues à l'aide de la commande \texttt{predict} en
précisant l'option \verb|type="response"| comme ci-dessous :
<<ex6-3h>>=
glm.pred <- predict(glm.res, type="response")
names(glm.pred) <- sck$ck
@ %$
\cmd{predict}\cmd{names}
En considérant que des probabilités $\ge 0.5$ signifie que les individus
sont malades, on obtient donc la répartition suivante :
<<ex6-3i>>=
glm.pred[glm.pred >= 0.5]
@ 
On en conclut que les personnes seront considérées malades, selon ce modèle,
pour des valeurs \texttt{ck} de 80 ou plus.

Cela se vérifie aisément sur un graphique dans lequel on reporte les
probabilités prédites en fonction des valeurs de la variable \texttt{ck}.
<<ex6-3j, fig=TRUE>>=
sck$malade <- sck$pres/(sck$pres+sck$abs)
xyplot(glm.pred ~ sck$ck, type="l", 
       ylab="Probabilité", xlab="ck", 
       panel=function(...) {
         panel.xyplot(...)
         panel.xyplot(sck$ck, sck$malade, pch=19, col="grey")
       })
@ %$
\cmd{xyplot}

Dans un premier temps, il faut "décompacter" les données groupées et crée un
tableau avec deux colonnes : la première représentant la variable
\texttt{ck} et la seconde représentant la présence ou absence de maladie. On
utilisera les effectifs par sous-groupe calculé précédemment et disponible
dans la variable \texttt{ni}.
% FIXME:
% voir epitools::expand.table
<<ex6-3k>>=
sck.expand <- data.frame(ck=c(rep(sck$ck, sck$pres), rep(sck$ck, sck$abs)), 
                         malade=c(rep(1, ni[1]), rep(0, ni[2])))
table(sck.expand$malade)
with(sck.expand, tapply(malade, ck, sum))
@ %$
\cmd{data.frame}\cmd{table}\cmd{tapply}
Les deux dernières commandes visent à s'assurer que l'on retombe bien sur
les mêmes effectifs et que la distribution des personnes malades par valeur
de \texttt{ck} est correcte. On peut également vérifier que l'on obtient
bien les mêmes résultats concernant la régression logistique, et en profiter
pour ajouter les valeurs prédites au tableau de données précédent (on
pourrait procéder comme précédemment et répliquer les prédictions, mais cela
est plus simple ainsi)
<<ex6-3l>>=
glm.res2 <- glm(malade ~ ck, data=sck.expand, family=binomial)
sck.expand$prediction <- ifelse(predict(glm.res2, type="response") >= 0.5, 1, 0)
with(sck.expand, table(malade, prediction))
@  %$
\cmd{glm}\cmd{ifelse}\cmd{predict}\cmd{with}\cmd{table}
La dernière commande permet d'afficher une matrice de confusion dans
laquelle on croise les diagnostiques réels et ceux prédits par le modèle de
régression. On peut ainsi comparer les taux de classification correcte
lorsque l'on varie le seuil de référence :
<<ex6-3m>>=
classif.tab <- with(sck.expand, table(malade, prediction))
sum(diag(classif.tab))/sum(classif.tab)
@ %$
\cmd{with}\cmd{sum}\cmd{diag}\cmd{table}

Pour afficher la courbe ROC, on utilisera le package \texttt{ROCR}.
<<ex6-3n, fig=TRUE>>=
library(ROCR)
pred <- prediction(predict(glm.res2, type="response"), sck.expand$malade)
perf <- performance(pred, "tpr","fpr")
plot(perf, ylab="Sensibilité", xlab="1-Spécificité")
grid()
abline(0, 1, lty=2)
@ %$
\cmd{library}\cmd{prediction}\cmd{performance}\cmd{plot}\cmd{grid}\cmd{abline}

La valeur de l'aire sous la courbe est obtenue comme suit :
<<ex6-3o>>=
performance(pred, "auc")@"y.values"
@ 

% FIXME:
% for previous exercice
% Une autre solution consiste à utiliser la commande \texttt{lroc} dans le
% package \texttt{epicalc}, mais celle-ci offre moins de
% facilités. L'avantage, en revanche, est que cette commande permet de
% travailler directement avec des matrices de confusion avec des modèles de
% régression logistique estimés à partir de données groupées.
% <<ex6-3p, eval=FALSE, fig=FALSE>>=
% library(epicalc)
% lroc(glm.res, line.col="black", grid.col="grey70", auc.coords=c(0.4,0.2))
% @
\end{sol}
\end{exo}
% 
% Everitt 2001 p. 208
%
\begin{exo}\label{exo:6.4}
Le tableau suivant résume la proportion d'infarctus du myocarde observée
chez des hommes âgés de 40 à 59 ans et pour lesquels on a relevé le niveau
de tension artérielle et le taux de cholesterol, considérées sous forme de
classes ordonnées.
\vskip1em

\begin{tabular}{l///////}
\toprule
& \multicolumn{7}{c}{Cholesterol (mg/100 ml)} \\
\cmidrule(r){2-8}
TA & \multicolumn{1}{c}{$<200$} & \multicolumn{1}{c}{$200-209$} & \multicolumn{1}{c}{$210-219$} & \multicolumn{1}{c}{$220-244$} & \multicolumn{1}{c}{$245-259$} & \multicolumn{1}{c}{$260-284$} & \multicolumn{1}{c}{$>284$} \\
$<117$ & 2/53 & 0/21 & 0/15 & 0/20 & 0/14 & 1/22 & 0/11 \\
$117-126$ & 0/66 & 2/27 & 1/25 & 8/69 & 0/24 & 5/22 & 1/19 \\
$127-136$ & 2/59 & 0/34 & 2/21 & 2/83 & 0/33 & 2/26 & 4/28 \\
$137-146$ & 1/65 & 0/19 & 0/26 & 6/81 & 3/23 & 2/34 & 4/23 \\
$147-156$ & 2/37 & 0/16 & 0/6 & 3/29 & 2/19 & 4/16 & 1/16 \\
$157-166$ & 1/13 & 0/10 & 0/11 & 1/15 & 0/11 & 2/13 & 4/12 \\
$167-186$ & 3/21 & 0/5 & 0/11 & 2/27 & 2/5 & 6/16 & 3/14 \\
$>186$ & 1/5 & 0/1 & 3/6 & 1/10 & 1/7 & 1/7 & 1/7 \\
\bottomrule
\end{tabular}
\vskip1em

Les données sont disponibles dans le fichier \texttt{hdis.dat} sous forme
d'un tableau comprenant 4 colonnes indiquant, respectivement, la pression
artérielle (8 catégories, notées 1 à 8), le taux de cholesterol (7
catégories, notées 1 à 7), le nombre d'infarctus et le nombre total
d'individus. On s'intéresse à l'association entre la pression artérielle et
la probabilité d'avoir un infarctus du myocarde.
\begin{description}
\item[(a)] Calculer les proportions d'infarctus pour chaque niveau de
  pression artérielle et les représenter dans un tableau et sous forme
  graphique.
\item[(b)] Exprimer les proportions calculées en (a) sous forme de
  \emph{logit}. 
\item[(c)] À partir d'un modèle de régression logistique, déterminer s'il
  existe une association significative au seuil $\alpha=0.05$ entre la
  pression artérielle, traitée en tant que variable quantitative en
  considérant les centres de classe, et la probabilité d'avoir un
  infarctus.
\item[(d)] Exprimer en unités \emph{logit} les probabilités d'infarctus
  prédites par le modèle pour chacun des niveaux de pression artérielle.
\item[(e)] Afficher sur un même graphique les proportions empiriques et la
  courbe de régression logistique en fonction des valeurs de pression
  artérielle (centres de classe).
\end{description}
% FIXME:
% Ajouter question sur prédiction pour une certaine valeur
% (niveau observé, et niveau non observé)
\begin{sol}
L'importation des données \texttt{hdis.dat} se fait comme suit :
<<ex6-4a>>=
bp <- read.table("data/hdis.dat", header=TRUE)
str(bp)
@ 
\cmd{read.table}\cmd{str}
Comme on peut le constater, aucun label n'est associé aux modalités de
variables d'intérêt (\texttt{bpress} pour la pression artérielle,
\texttt{chol} pour le taux de cholesterol). Pour générer et associer les
labels, on peut utiliser les commandes suivantes :
<<ex6-4b>>=
blab <- c("<117","117-126","127-136","137-146",
          "147-156","157-166","167-186",">186")
clab <- c("<200","200-209","210-219","220-244",
          "245-259","260-284",">284")
bp$bpress <- factor(bp$bpress, labels=blab)
bp$chol <- factor(bp$chol, labels=clab)
@ 
\cmd{factor}
La dernière commande convertit les variables d'origine en variables
qualitatives et associe à leurs modalités les labels définis par
\texttt{blab} et \texttt{clab}. Pour vérifier que la base de données est
bien sous la forme désirée, les commandes suivantes sont toujours utiles :
<<ex6-4c>>=
str(bp)
summary(bp)
@ 
\cmd{str}\cmd{summary}

On peut maintenant reproduire le tableau de fréquences relatives fourni dans
l'énoncé :
<<ex6-4d>>=
round(xtabs(hdis/total ~ bpress + chol, data=bp), 2)
@
\cmd{round}\cmd{xtabs}

Puisque l'on ne va s'intéresser qu'à la relation entre pression artérielle
et infarctus, il est nécessaire d'aggréger les données sur le taux de
cholestérol. En d'autres termes, il est nécessaire de cumuler les effectifs
pour chaque niveau de pression artérielle, tous niveaux de cholesterol
confondus. On en profitera également pour renommer les niveaux de la
variable \texttt{bpress} en utilisant les centres des intervalles de classe.
<<ex6-4e>>=
blab2 <- c(111.5,121.5,131.5,141.5,151.5,161.5,176.5,191.5)
# levels(bp$bpress) <- blab2
# bp$bpress <- as.numeric(as.character(bp$bpress))
bp$bpress <- rep(blab2, each=7)
dfrm <- aggregate(bp[,c("hdis","total")], list(bpress=bp[,"bpress"]), sum)
@ %$
\cmd{levels}\cmd{aggregate}\cmd{sum}
C'est la dernière commande, \texttt{aggregate}, qui permet d'aggréger les
données : on additionne tous les effectifs (\texttt{sum}) de la variable
\texttt{chol} qui ne figure pas dans la liste des variables que l'on
souhaite conserver pour l'analyse. On en profite pour stocker les résultats
dans une nouvelle base de données nommée \texttt{dfrm}. Un aperçu des
données aggrégées est fourni ci-après :
<<ex6-4f>>=
head(dfrm, 5)
@ 
\cmd{head}

Lorsque l'on dispose d'une proportion, $p$, sa valeur sur une échelle dont
les unités sont des \emph{logit} est donnée par la relation
$\log(p/(1-p))$. D'où les commandes suivantes pour convertir les
proportions, calculées comme \verb|hdis/total| en unités logit :
<<ex6-4g>>=
logit <- function(x) log(x/(1-x))
dfrm$prop <- dfrm$hdis/dfrm$total
dfrm$logit <- logit(dfrm$hdis/dfrm$total)
@ %$
\cmd{function}\cmd{log}
On remarquera que l'on a défini une petite fonction permettant de convertir
des valeurs \texttt{x}, qui ici sont supposées être des proportions, en leur
équivalent $\log(x/(1-x))$. On aurait pu écrire de manière équivalente :
<<ex6-4h, eval=FALSE>>=
log((dfrm$hdis/dfrm$total)/(1-dfrm$hdis/dfrm$total))
@ %$
\cmd{log}
Le résultat de ces calculs est reproduit ci-après.
<<ex6-4i>>=
dfrm
@ 

Le modèle de régression logistique s'écrit de la manière suivante :
<<ex6-4j, eval=FALSE>>=
glm(cbind(hdis, total-hdis) ~ bpress, data=dfrm, family=binomial)
@ 
\cmd{glm}
La formulation utilisée, \verb|cbind(hdis, total-hdis) ~ bpress|, tient
compte du fait que nous disposons de données groupées, et non des réponses
individuelles. La commande \texttt{glm} avec l'option \verb|family=binomial|
correspond à une régression logistique, qui, sans vouloir entrer trop dans
les détails, utilise par défaut la fonction \texttt{logit} comme lien
canonique. 
% On préférera toutefois utiliser la commande équivalente
% \texttt{lrm} du package \texttt{rms}, car celle-ci fournit en règle
% générale beaucoup plus d'informations sur la qualité du modèle, en plus des
% informations que l'on pourrait obtenir à partir de la commande
% \texttt{glm}. La formulation du modèle reste identique :
% <<ex6-4k>>=
% library(rms)
% lrm(cbind(hdis, total-hdis) ~ bpress, data=dfrm)
% @ 
On obtient donc les résultats suivants :
<<ex6-4k>>=
summary(glm(cbind(hdis, total-hdis) ~ bpress, data=dfrm, family=binomial))
@ 
\cmd{glm}\cmd{summary.glm}
Le résultat précédent comprend les informations essentielles pour répondre à
la question de la significativité statistique de l'association entre
pression artérielle et probabilité d'un infarctus : le coefficient de
régression (sur l'échelle log-odds) vaut 0.024 et est significatif au seuil
usuel de 5~\% (cf. colonne \verb+Pr(>|z|)+).

La probabilité d'avoir un infarctus selon les différents niveaux de pression
artérielle considérés est obtenue comme suit :
<<ex6-4l>>=
glm.res <- glm(cbind(hdis, total-hdis) ~ bpress, data=dfrm, family=binomial)
predict(glm.res)
@
\cmd{glm}\cmd{predict}
Notons que l'on a stocké les résultats intermédiaires générés par \R sous le
nom \texttt{glm.res} avant d'utiliser la commande \texttt{predict}. Les
prédictions générées par \R sont exprimées sous forme de \emph{logit}, et
l'on peut comparer les logit observés et prédits.
<<ex6-4m>>=
cbind(dfrm, logit.predit=predict(glm.res))
@ 
\cmd{cbind}\cmd{predict}

Pour représenter graphiquement les proportions d'infarctus observées et
prédites en fonction du niveau de pression artérielle, on a pratiquement
tous les éléments. Il nous manque les prédictions du modèle exprimées sous
forme de proportions, et non sur l'échelle log-odds. Ensuite, on peut
vouloir tracer la courbe de prédiction, c'est-à-dire la probabilité d'avoir
un infarctus en fonction de la pression artérielle, sans limiter cette
dernière aux 8 valeurs observées pour la variable \texttt{bpress}. Voici une
solution possible :
<<ex6-4n, fig=TRUE>>=
dfrm$prop.predit <- predict(glm.res, type="response") 
f <- function(x) 1/(1+exp(-(coef(glm.res)[1]+coef(glm.res)[2]*x)))
xyplot(hdis/total ~ bpress, data=dfrm, aspect=1.2, cex=.8,
       xlab="Pression artérielle", ylab="Probabilité infarctus",
       panel=function(x, y, ...) {
         panel.xyplot(x, y, col="gray30", pch=19, ...)
         panel.curve(f, lty=3, col="gray70")
         panel.points(x, dfrm$prop.predit, col="gray70", ...)
       })
@ 
\cmd{predict}\cmd{function}\cmd{exp}\cmd{coef}\cmd{xyplot}
\end{sol}
\end{exo}
%
% Dupont 2009 p. 138
%
\begin{exo}\label{exo:6.5}
Une enquête cas-témoin a porté sur la relation entre la consommation
d'alcool et de tabac et le cancer de l'oesophage chez l'homme (étude "Ille
et Villaine"). Le groupe des cas était composé de 200 patients atteints d'un
cancer de l'oesophage et diagnostiqué entre janvier 1972 et avril 1974. Au
total, 775 témoins de sexe masculin ont été sélectionnés à partir des listes
électorales. Le tableau suivant indique la répartition de l'ensemble des
sujets selon leur consommation journalière d'alcool, en considérant qu'une
consommation supérieure à 80 g est considérée comme un facteur de
risque.\autocite{breslow80} 
\vskip1em

\begin{tabular}{lccc}
\toprule
& \multicolumn{2}{c}{Consommation d'alcool (g/jour)} & \\
\cmidrule(r){2-3}
& $\ge 80$ & $<80$ & Total \\
\midrule
Cas & 96 & 104 & 200 \\
Témoins & 109 & 666 & 775 \\
Total & 205 & 770 & 975 \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
%\item[(a)] Quelle est la proportion de personnes considérées à risque dans
%  cet échantillon ?
\item[(a)] Quelle est la valeur de l'odds-ratio et son intervalle de
  confiance à 95~\% (méthode de Woolf) ? Est-ce une bonne estimation du
  risque relatif ? 
\item[(b)] Est-ce que la proportion de consommateurs à risque est la même
  chez les cas et chez les témoins (considérer $\alpha=0.05$) ?
\item[(c)] Construire le modèle de régression logistique permettant de
  tester l'association entre la consommation d'alcool et le statut des
  sujets. Le coefficient de régression est-il significatif ?
\item[(d)] Retrouvez la valeur de l'odds-ratio observé, calculé en (b), et
  son intervalle de confiance à partir des résultats de l'analyse de
  régression.
\end{description}
\begin{sol}
Comme les données brutes (individuelles) ne sont pas disponibles, il faut
travailler directement avec le tableau d'effectifs fourni dans l'énoncé. 
<<ex6-5a>>=
alcool <- matrix(c(666,104,109,96), nr=2, dimnames=list(c("Témoin","Cas"), c("<80",">=80")))
alcool
@
\cmd{matrix}

Concernant l'odds-ratio, on utilisera la commande \texttt{oddsratio} du
package \texttt{vcd} :
<<ex6-5c>>=
library(vcd)
oddsratio(alcool, log=FALSE)
@ 
\cmd{oddsratio}
% FIXME:
% Introduire epicalc?
% cc(NULL, NULL, cctable=make2x2(96,109,104,666))
L'option \texttt{log=FALSE} nous assure que le résultat renvoyé correspond
bien à un odds-ratio et non au log odds-ratio. Pour obtenir un intervalle de
confiance asymptotique, on utilise 
<<ex6-5d>>=
confint(oddsratio(alcool, log=FALSE))
@ 
\cmd{confint}\cmd{oddsratio}
De même, on pourrait utiliser \verb|summary(oddsratio(alcool))| pour
effectuer un test d'hypothèse sur le log odds-ratio
($H_0:\,\log(\hat\theta)=0$). 

Pour tester l'hypothèse que la proportion de personnes avec une consommation
journalière est $\ge 80$ g est identique chez les cas et les témoins, on
peut utiliser la commande \texttt{prop.test} en indiquant les effectifs
observés à partir du tableau croisé donnée dans l'énoncé.
<<ex6-5f>>=
prop.test(c(96,109), c(200,775), correct=FALSE)
@ 
\cmd{prop.test}
Ce test est exactement équivalent à un test $Z$ pour tester la différence
entre deux proportions estimées à partir des données (si l'on n'utilise pas
de correction de continuité).

Pour le modèle de régression logistique, on a besoin de transformer le
tableau de contingence en un tableau de données où l'on fait clairement
apparaître les deux variables qualitatives (maladie et exposition),
c'est-à-dire un \texttt{data.frame}. Cela peut être réalisé en utilisant une
commande qui permet de transformer les données sous forme tabulaire en
format "long" : la commande \texttt{melt} du package \texttt{reshape}. On en
profitera pour recoder les niveaux de la variable maladie
en 0 (témoins) et 1 (cas), même si ce n'est pas vraiment nécessaire, et
considérer le niveau 0 comme la catégorie de référence (ce qui facilite
l'interprétation des résultats).
<<ex6-5g>>=
library(reshape)
alcool.df <- melt(alcool)
names(alcool.df) <- c("maladie", "exposition", "n")
levels(alcool.df$maladie) <- c(1,0)
alcool.df$maladie <- relevel(alcool.df$maladie, "0")
@ %$
\cmd{library}\cmd{melt}\cmd{names}\cmd{levels}\cmd{relevel}

Le modèle de régression logistique
<<ex6-5h>>=
glm.res <- glm(maladie ~ exposition, data=alcool.df, family=binomial, weights=n)
summary(glm.res)
@
\cmd{glm}\cmd{summary.glm}
Le résultat qui nous intéresse est la ligne associée à
\verb|exposition>=80|, puisqu'elle nous renseigne sur la valeur du
coefficient de régression associé à l'exposition et estimé par \R, avec son
erreur standard, ainsi que la valeur de la statistique de test. Ici, le
coefficient de régression s'interprète comme le log de l'odds-ratio.  Notons
que l'on obtiendrait exactement les mêmes résultats en intervertissant le
rôle des variables dans la formulation précédente, \verb|exposition ~ maladie|.

On peut retrouver l'odds-ratio calculé plus haut à partir du coefficient de
régression associé au facteur d'intérêt (\texttt{exposition}), ainsi que son
intervalle de confiance à 95~\% :
<<ex6-5i>>=
exp(coef(glm.res)[2])
exp(confint(glm.res)[2,])
@ 
\cmd{exp}\cmd{coef}\cmd{confint}

Une deuxième solution consiste à considérer le nombre de cas et le nombre
total d'individus, comme dans l'exercice~\ref{exo:6.4}.
<<ex6-5j>>=
alcool2 <- data.frame(expos=c("<80",">=80"), cas=c(104,96), total=c(104+666, 96+109))
summary(glm(cbind(cas, total-cas) ~ expos, data=alcool2, family=binomial))
@ 
\cmd{data.frame}\cmd{glm}\cmd{summary.glm}

Voici enfin une troisième manière de procéder, toujours avec la même
structure de données.
<<ex6-5k, eval=FALSE>>=
summary(glm(cas/total ~ expos, data=alcool2, family=binomial, weights=total))
@
\cmd{summary.glm}\cmd{glm}
Notons que le nombre total de sujets selon l'exposition n'est pas vraiment
utile dans ce dernier cas (on aurait pu utiliser directement le nombre de
témoins). 

% FIXME:
% Autre solution avec rms::lrm
% d <- datadist(alcool.df)
% options(datadist="d")
% lrm(X1 ~ X2, data=alcool.df, weights=value)
% summary(lrm(X1 ~ X2, data=alcool.df, weights=value))
\end{sol}
\end{exo}
\Closesolutionfile{solutions}

\chapter*{Devoir \no 5}
\addcontentsline{toc}{chapter}{Devoir \no 5}

Les exercices sont indépendants. Une seule réponse est correcte pour chaque
question. Lorsque vous ne savez pas répondre, cochez la case correspondante.

\section*{Exercice 1}
Les analyses proposées se basent sur une étude sur les facteurs de risque
d'accidents coronariens \citep{rosenman64}. Les données cas-témoin sont
résumées dans le tableau de contingence suivant : les cas sont définis comme
les personnes ayant eu un accident coronarien durant les 10 années de suivi,
tandis que le facteur d'exposition est l'opacisation de la cornée qui est
supposée être reliée au taux de cholestérol ; sa présence conduit à
considérer la personne comme exposée.
\vskip1em

\begin{tabular}{l|cc|r}
& Exposé & Non-exposé & total \\
\hline
Cas & 102 & 153 & 255 \\
Témoin & 839 & 2058 & 2897 \\
\hline
total & 941 & 2211 & 3152
\end{tabular}
\vskip1em

L'objet de l'analyse est de vérifier si l'obscurcissement de la cornée constitue
effectivement un facteur prédictif du risque de développer un infarctus.
\begin{description}
\item[\bf 1.1] \marginpar{\phantom{text}1.1 $\square$} En notant \texttt{M+/M-} les cas et
  les témoins, et \texttt{E+/E-} les personnes exposées/non-exposées, quelle
  solution peut-on proposer pour stocker ce type de données sous \R ?
  \begin{description}
  \item[A.] \verb|matrix(c(102,153,839,2058), nr=2, byrow=TRUE, dimnames=list(c("M-","M+"),c("E-","E+")))|
  \item[B.] \verb|matrix(c(102,153,839,2058), nr=2, byrow=TRUE, dimnames=list(c("M+","M-"),c("E+","E-")))|
  \item[C.] \verb|matrix(c(102,153,839,2058), nr=2, dimnames=list(c("M+","M-"),c("E+","E-")))|
  \item[C.] \verb|matrix(c(102,153,839,2058), nr=2, dimnames=list(c("M-","M+"),c("E-","E+")))|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 1.2] \marginpar{\phantom{text}1.2 $\square$} Supposons que le tableau de données
  ait été appelé \texttt{tab} sous \R. On souhaite calculer l'odds-ratio à
  l'aide de la commande \texttt{oddsratio} du package \texttt{vcd}. Quelle
  est la commande à utiliser ?
  \begin{description}
  \item[A.] \verb|oddsratio(tab, log=FALSE)|
  \item[B.] \verb|oddsratio(tab)|
  \item[C.] \verb|oddsratio(tab[c(2,1),c(2,1)], log=FALSE)|
  \item[D.] \verb|oddsratio(tab[c(2,1),c(2,1)])|
  \item[E.] Je ne sais pas.
  \end{description}
\item[\bf 1.3] \marginpar{\phantom{text}1.3 $\square$} Pour calculer l'intervalle de
  confiance à 95~\% associé à l'odds-ratio estimé en 1.2 que l'on a
  enregistré dans la variable \texttt{tab.or}, on utilise la commande :
  \begin{description}
  \item[A.] \verb|confint(tab.or)|
  \item[B.] \verb|summary(tab.or, conf.level=0.95)|
  \item[C.] \verb|confint(tab.or, log=FALSE)|
  \item[D.] Je ne sais pas.
  \end{description}  
\item[\bf 1.4] \marginpar{\phantom{text}1.4 $\square$} Peut-on retrouver la valeur de
  l'odds-ratio à partir d'une autre commande ? Si oui, laquelle ?
  \begin{description}
  \item[A.] \verb|fisher.test|
  \item[B.] \verb|binom.test|
  \item[C.] \verb|tabstat|
  \item[D.] Ce n'est pas possible.
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 1.5] \marginpar{\phantom{text}1.5 $\square$} Quelle commande utiliserait-on pour
  représenter graphiquement la fréquence relative des cas (uniquement) selon
  le facteur d'exposition ?
  \begin{description}
  \item[A.] \verb|barchart(table(tab["M+",])/sum(tab))|
  \item[B.] \verb|barchart(table(tab[2,])/sum(tab))|
  \item[C.] \verb|barchart(prop.table(tab[2,]))|
  \item[D.] \verb|barchart(prop.table(tab["M+",]))|
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 1.6] \marginpar{\phantom{text}1.6 $\square$} Que renvoit la commande suivante ? 
\begin{verbatim}
(tab[1,1]/sum(tab[1,])) / (tab[2,1]/sum(tab[2,]))
\end{verbatim}
  \begin{description}
  \item[A.] Le risque absolu chez les exposés.
  \item[B.] Le risque absolu chez les non-exposés.      
  \item[C.] Le risque relatif.
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 1.7] \marginpar{\phantom{text}1.7 $\square$} Quelle commande du package
  \texttt{epiR} permet de fournir une estimation par intervalle (95~\%) de
  la prévalence observée sur cet échantillon ?
  \begin{description}
  \item[A.] \verb|epi.2by2|
  \item[B.] \verb|epi.tests|
  \item[C.] Je ne sais pas.
  \end{description}  
\end{description}

\section*{Exercice 2}
Cet exercice est basé sur les mêmes données que celles présentées à
l'exercice précédent.

On souhaite modéliser la relation entre la probabilité d'être défini comme
un cas en fonction de l'exposition préalable à l'aide d'une régression
logistique.
  
\begin{description}
\item[\bf 2.1] \marginpar{\phantom{text}2.1 $\square$} On se propose dans un premier temps
  de transformer le tableau de contingence sous forme d'un
  \texttt{data.frame} comportant trois colonnes décrivant le statut relatif
  à la maladie (M+/M-) et à l'exposition (E+/E-) et les effectifs
  associés. La commande suivante est-elle correcte ?
\begin{verbatim}
tab <- melt(tab, varnames=c("maladie","exposition"))
\end{verbatim}
  \begin{description}
  \item[A.] Oui.
  \item[B.] Non.
  \item[C.] Je ne sais pas.
  \end{description}
\item[\bf 2.2] \marginpar{\phantom{text}2.2 $\square$} En supposant que les données sont à
  présent correctement spécifiées telles que décrit en 2.1, soit
\begin{verbatim}
> tab
  maladie exposition value
1      M+         E+   102
2      M-         E+   839
3      M+         E-   153
4      M-         E-  2058
\end{verbatim}
  la commande permettant d'établir le modèle de régression désiré et de le
  stocker dans une variable appelée \texttt{m} s'écrit :
  \begin{description}
  \item[A.] \verb|m <- glm(maladie ~ exposition, data=tab)|
  \item[B.] \verb|m <- glm(maladie ~ exposition, data=tab, weights=value)|
  \item[C.] \verb|m <- glm(maladie ~ exposition, data=tab, family=binomial, weights=value)|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 2.3] \marginpar{\phantom{text}2.3 $\square$} Pour afficher le résultat des tests
  de nullité des coefficients du modèle \texttt{m} (ordonnée à l'origine et
  pente), on utilise la commande :
  \begin{description}
  \item[A.] \verb|summary|
  \item[B.] \verb|anova|
  \item[C.] Je ne sais pas.
  \end{description}  
\item[\bf 2.4] \marginpar{\phantom{text}2.4 $\square$} On se propose de retrouver la
  valeur de l'odds-ratio calculé à l'exercice~1.2 à partir du modèle
  précédent. Quelle combinaison de commandes est nécessaire pour réaliser
  cette opération ?
  \begin{description}
  \item[A.] \verb|coef| et \verb|log|
  \item[B.] \verb|coef| et \verb|exp|
  \item[C.] \verb|summary| et \verb|exp|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 2.5] \marginpar{\phantom{text}2.5 $\square$} On souhaite calculer la probabilité
  d'être un cas sachant que l'on a été exposé. Quelle commande permet de
  répondre à cette question ?
  \begin{description}
  \item[A.] \verb|predict(m, data.frame(X2=1), type="link")|
  \item[B.] \verb|predict(m, data.frame(X2="E+"), type="link")|
  \item[C.] \verb|predict(m, data.frame(X2=1), type="response")|
  \item[D.] \verb|predict(m, data.frame(X2="E+"), type="response")|
  \item[E.] Je ne sais pas.
  \end{description}  
\end{description}

\section*{Exercice 3}
Dans une étude diagnostique portant sur 150 patients, on a cherché à
vérifier si un examen radiologique (rayons X) du patient permettait de
prédire correctement un risque de fracture, sachant qu'une opération
chirurgicale intervenant après l'évaluation permettait de confirmer la
présence ou l'absence de fracture chez chaque patient
\citep[p.~280]{peat05}. L'opération chirurgicale tient lieu de "gold
standard" et l'examen radiologique de test de screening. Le tableau suivant
résume les données recueillies à l'issue de l'étude.
\vskip1em

\begin{tabular}{clllll}
\toprule
& & & \multicolumn{2}{c}{Fracture détectée} & \\
\cmidrule(r){4-5}
& & & Présente & Absente & Total \\
\midrule
Résultats Rayons X & Positif & N  & 36 & 24 & 60 \\
(test)             &         & \% & 60.0 & 40.0 & 100.0 \\
                   & Négatif & N  & 8 & 82 & 90 \\
                   &         & \% & 8.9 & 91.1 & 100.0 \\
Total              &         & N  & 44 & 106 & 150 \\
                   &         & \% & 29.3 & 70.7 & 100.0 \\
\bottomrule
\end{tabular}
\vskip1em

On suppose par la suite que le tableau 2x2 (c'est-à-dire sans les marges)
est stocké exactement de la même manière sous \R (résultats du test en
lignes, résultats du diagnostic final en colonnes), et se nomme
\texttt{tab}.
\begin{description}
\item[\bf 3.1] \marginpar{\phantom{text}3.1 $\square$} La commande 
\begin{verbatim}
tab[2,2]/sum(tab[2,])
\end{verbatim}
  fournit une estimation de la valeur prédictive positive ? 
  \begin{description}
  \item[A.] Vrai.
  \item[B.] Faux.
  \item[C.] Je ne sais pas.
  \end{description}  
\item[\bf 3.2] \marginpar{\phantom{text}3.2 $\square$} Soit la valeur de VPN=0.91 et la
  commande suivante :
\begin{verbatim}
0.91 + 1.96 * sqrt(0.91*0.09/150)
\end{verbatim}
  Que produit cette commande ? 
  \begin{description}
  \item[A.] La valeur de l'erreur standard associée à la VPN en utilisant
    une approximation par la loi normale.
  \item[B.] La borne supérieure de l'intervalle de confiance à 90~\% de la
    VPN en utilisant une approximation par la loi normale.
  \item[C.] La borne supérieure de l'intervalle de confiance à 95~\% de la
    VPN en utilisant une approximation par la loi normale.
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 3.3] \marginpar{\phantom{text}3.3 $\square$} Peut-on obtenir un résultat
  comparable à partir d'une autre commande \R ?
  \begin{description}
  \item[A.] \verb|fisher.test|
  \item[B.] \verb|prop.test|
  \item[C.] Je ne sais pas.
  \end{description}
\item[\bf 3.4] \marginpar{\phantom{text}3.4 $\square$} Voici le contenu partiel d'une
  sortie produite par \R :
\begin{verbatim}
Point estimates and 95 % CIs:
---------------------------------------------------------
Apparent prevalence                    0.4 (0.32, 0.48)
True prevalence                        0.29 (0.22, 0.37)
Sensitivity                            0.82 (0.67, 0.92)
Specificity                            0.77 (0.68, 0.85)
Positive predictive value              0.6 (0.47, 0.72)
Negative predictive value              0.91 (0.83, 0.96)
---------------------------------------------------------
\end{verbatim}
  Quelle commande du package \texttt{epiR} a permis de produire un tel
  résultat ? 
  \begin{description}
  \item[A.] \verb|epi.2by2|
  \item[B.] \verb|epi.tests|
  \item[C.] Je ne sais pas.
  \end{description}
\end{description}

%--------------------------------------------------------------- Chapter 07 --
\chapter{Données de survie}\label{chap:survival}
\Opensolutionfile{solutions}[solutions7]


\section*{Énoncés}
%
% 
%
\begin{exo}\label{exo:7.1}
Dans un essai contre placebo sur la cirrhose biliaire, la D-penicillamine
(DPCA) a été introduite dans le bras actif sur une cohorte de 312
patients. Au total, 154 patients ont été randomisés dans le bras actif
(variable traitement, \texttt{rx}, 1=Placebo, 2=DPCA). Un ensemble de
données telles que l'âge, des données biologiques et signes cliniques variés
incluant le niveau de bilirubine sérique (\texttt{bilirub}) sont disponibles
dans le fichier \texttt{pbc.txt}.\autocite{vittinghoff05} Le status du
patient est enregistré dans la variable \texttt{status} (0=vivant, 1=décédé)
et la durée de suivi (\texttt{years}) représente le temps écoulé en années
depuis la date de diagnostic.
\begin{description}
\item[(a)] Combien dénombre-t-on d'individus décédés? Quelle proportion de
  ces décès retrouve-t-on dans le bras actif ?  
\item[(b)] Afficher la distribution des durées de suivi des 312 patients, en
  faisant apparaître distinctement les individus décédés. Calculer le temps
  médian (en années) de suivi pour chacun des deux groupes de
  traitement. Combien y'a-t-il d'événements positifs au-delà de 10.5 années
  et quel est le sexe de ces patients ?
\item[(c)] Les 19 patients dont le numéro (\texttt{number}) figure parmi la
  liste suivante ont subi une transplantation durant la période de suivi.
\begin{verbatim}  
5 105 111 120 125 158 183 241 246 247 254 263 264 265 274 288 291
295 297 345 361 362 375 380 383
\end{verbatim}   
  Indiquer leur âge moyen, la distribution selon le sexe et la durée médiane
  de suivi en jours jusqu'à la transplantation.
\item[(d)] Afficher un tableau résumant la distribution des événements à
  risque en fonction du temps, avec la valeur de survie associée.
\item[(e)] Afficher la courbe de Kaplan-Meier avec un intervalle de
  confiance à 95~\%, sans considérer le type de traitement.
\item[(f)] Calculer la médiane de survie et son intervalle de confiance à
  95~\% pour chaque groupe de sujets et afficher les courbes de survie
  correspondantes.
\item[(g)] Effectuer un test du log-rank en considérant comme prédicteur le
  facteur \texttt{rx}. Comparer avec un test de Wilcoxon.
\item[(h)] Effectuer un test du log-rank sur le facteur d'intérêt
  (\texttt{rx}) en stratifiant sur l'âge. On considèrera trois groupe
  d'âge : 40 ans ou moins, entre 40 et 55 ans inclus, plus de 55 ans.
\item[(i)] Retrouver les résultats de l'exercice 1.g avec une régression de
  Cox. 
\end{description}
\begin{sol}
Le chargement des données ne pose pas de difficultés :
<<ex7-1a>>=
pb <- read.table("data/pbc.txt", header=TRUE)
names(pb)[1:20]
@ 
Pour faciliter la lecture des résultats, recodons d'emblée les variables
traitement (\texttt{rx}) et sexe (\texttt{sex}) comme variables qualitatives
: 
<<ex7-1b>>=
pb$rx <- factor(pb$rx, labels=c("Placebo", "DPCA"))
table(pb$rx)
pb$sex <- factor(pb$sex, labels=c("M","F"))
@ 
Le nombre de patients décédés peut être obtenu directement à partir d'un
tableau d'effectif sur la variable \texttt{status} :
<<ex7-1c>>=
prop.table(table(pb$status))
@ 
La proportion de décès par groupe de traitement s'obtient en croisant les
deux variables \texttt{status} et \texttt{rx} et en calculant les fréquences
relatives par ligne :
<<ex7-1c>>=
prop.table(with(pb, table(status, rx)), 1)
@
Pour afficher la distribution des temps de suivi, on peut utiliser un simple
diagramme de dispersion dans lequel les coordonnées des points à afficher
sont définis par le numéro d'observation et la durée de suivi depuis le
diagnostique. 
<<ex7-1d, fig=TRUE>>=
xyplot(number ~ years, data=pb, pch=pb$status, cex=.8)
@ 

Le temps médian de suivi par groupe de traitement est obtenu à partir d'une
commande de type \texttt{tapply} en considérant la variable \texttt{rx}
comme variable de classification, soit :
<<ex7-1e>>=
with(pb, tapply(years, rx, median))
@ 
Le nombre de patients décédés au-delà de 10.5 ans est obtenu comme suit :
<<ex7-1f>>=
with(pb, table(status[years > 10.5]))
with(pb, table(sex[years > 10.5 & status == 1]))
@ 
soit 4 patients, dont 2 patients de sexe féminin. Notons que la dernière
commande pourrait également s'écrire 
<<ex7-1g>>=
subset(pb, years > 10.5 & status == 1, sex)
@ 
ce qui a l'avantage de fournir les numéros d'observation.

Les patients transplantés figurent normalement parmi les individus vivants à
la date de point. On peut le vérifier aisément à l'aide d'un simple tri à
plat du \texttt{status} de ces patients.
<<ex7-1h>>=
idx <- c(5,105,111,120,125,158,183,241,246,247,254,263,264,265,274,288,291,295,
         297,345,361,362,375,380,383)
table(pb$status[pb$number %in% idx])
@ 
Pour calculer les quantités demandées (âge moyen, sexe, durée de suivi), on
peut dans un premier temps réduire le tableau de données initial à ces seuls
patients :
<<ex7-1i>>=
pb.transp <- subset(pb, number %in% idx, c(age, sex, years))
@ 
Ensuite, il s'agit simplement d'appliquer les commandes \texttt{mean},
\texttt{table} et \texttt{median} aux variables sélectionnées pour ces 19
patients.
<<ex7-1j>>=
mean(pb.transp$age)
table(pb.transp$sex)
median(pb.transp$years * 365)
@ 

Pour obtenir un tableau des événements avec la survie associée, il est
nécessaire d'utiliser la commande \texttt{Surv} du package
\texttt{survival}. La commande de base pour recoder des données
événements/temps en données de survie est \texttt{Surv} dont voici un
exemple d'utilisation :
<<ex7-1k>>=
library(survival)
head(with(pb, Surv(time=years, event=status)))
@ 
Les données censurées (patients vivant) apparaissent suffixées avec un
\texttt{+}. Un tableau des événements en fonction du temps peut être
construit à l'aide de la commande \texttt{survfit}. On notera que lorsque
l'on ne considère aucun facteur de groupe ou de stratification, il faut
utiliser la syntaxe un peu particulière \verb|Surv(time, event) ~ 1|.
<<ex7-1l, results=hide>>=
s <- survfit(Surv(years, status) ~ 1, data=pb)
summary(s)
@ 

Une fois que le tableau de survie a été construit, il est très simple
d'afficher l'estimateur de la survie sous la forme d'une courbe de
Kaplan-Meier à l'aide de la commande \texttt{plot}.
<<ex7-1m, fig=TRUE>>=
plot(s)
@

La médiane de survie pour chaque bras de traitement peut être obtenue à
l'aide de la commande \texttt{survdiff}. Redéfinissons dans un premier temps
le modèle de survie pour inclure le prédicteur d'intérêt :
<<ex7-1n>>=
s <- survfit(Surv(years, status) ~ rx, data=pb)
@ 
Pour obtenir la médiane de survie, il suffit d'afficher le résultat
précédent :
<<ex7-1o>>=
s
@

On utilisera la commande \texttt{plot} pour obtenir les courbes de survie
correspondantes. 
<<ex7-1oo, fig=TRUE>>=
plot(s)
@ 

Le test du log-rank se réalise à partir de la commande \texttt{survdiff},
avec les mêmes notations que dans le cas du calcul de la médiane de survie.
<<ex7-1p>>=
survdiff(Surv(years, status) ~ rx, data=pb)
@ 
L'option \texttt{rho} permet de contrôler le type de test réalisé. En
ajoutant \texttt{rho=1} on réalise un test de Wilcoxon au lieu du log-rank
classique :
<<ex7-1q>>=
survdiff(Surv(years, status) ~ rx, data=pb, rho=1)
@ 

Pour réaliser une analyse stratifiée sur l'âge, il est nécessaire de recoder
cette variable numérique en variable qualitative, ce que l'on peut réaliser
à l'aide de la commande \texttt{cut}.
<<ex7-1r>>=
agec <- cut(pb$age, c(26, 40, 55, 79))
@ 
On peut effectuer de nouveau le test du log-rank en incluant ce facteur de
stratification additionnel en adaptatnt légèrement le modèle précédent :
<<ex7-1s>>=
survdiff(Surv(years, status) ~ rx + strata(agec), data=pb)
@ 

Enfin, concernant le modèle de Cox, c'est la commande \texttt{coxph} qu'il
faut utiliser.
<<ex7-1t>>=
summary(coxph(Surv(years, status) ~ rx + strata(agec), data=pb))
@ 
\end{sol}
%
% Hill et al. 1996 p. 22
%
\end{exo}
\begin{exo}\label{exo:7.2}
Ci-dessous figurent les durées de rémission en semaines dans un essai
comparant 6-MP à un placebo.\autocite{freireich63} Les observations
censurées (sujet sans rechute aux dernières nouvelles) sont signalées par
une astérisque.
\vskip1em

\begin{tabular}{ll}
\toprule
Placebo & 1 1 2 2 3 4 4 5 5 8 8 8 8 11 11 12 12 \\
        & 15 17 22 23 \\
6-MP    & 6 6 6 6* 7 9* 10 10* 11* 13 16 17* 19* \\
        & 20* 22 23 25* 32* 32* 34* 35* \\
\bottomrule
\end{tabular}
\vskip1em

\begin{description}
\item[(a)] Construire un tableau indiquant le nombre d'exposés et le nombre
  de rechutes en fonction du temps pour le groupe 6-MP.
\item[(b)] Afficher la courbe de survie dans le groupe 6-MP, ainsi que son
  intervalle de confiance à 95~\% (estimateur de Kaplan-Meier).
\item[(c)] Afficher la fonction de risque cumulée (estimateur de Nelson)
  pour le même groupe.
\item[(d)] Calculer la médiane de survie du groupe 6-MP.
\item[(e)] Calculer la survie espérée pour un patient du groupe 6-MP à 15
  semaines. 
\end{description}
\begin{sol}
Dans un premier temps, il est nécessaire de construire un tableau des
données brutes, dans lequel on reportera le traitement, la durée de survie
et la censure (0=vivant/1=décédé).
<<ex7-2a>>=
placebo.time <- c(1,1,2,2,3,4,4,5,5,8,8,8,8,11,11,12,12,15,17,22,23)
placebo.status <- rep(1, length(placebo.time))
mp.time <- c(6,6,6,6,7,9,10,10,11,13,16,17,19,20,22,23,25,32,32,34,35)
mp.status <- c(1,1,1,0,1,0,1,0,0,1,1,0,0,0,1,1,0,0,0,0,0)
mp <- data.frame(tx=rep(c("Placebo","6-MP"), c(21,21)),
                 time=c(placebo.time, mp.time), 
                 status=c(placebo.status, mp.status))
summary(mp)
@   
On peut vérifier que les événements ont été correctement enregistrés dans ce
nouveau tableau de données en vérifiant le format des données de survie sous
\R :
<<ex7-2b>>=
with(mp, Surv(time, status))
@ 

Le tableau des événements dans le groupe 6-MP peut être construit à l'aide
de la commande \texttt{survfit}. On pourrait également limiter les calculs
aux seules observations du groupe 6-MP à l'aide de l'option \texttt{subset}.
<<ex7-2c>>=
s <- survfit(Surv(time, status) ~ tx, data=mp)
summary(s)
@ 
On utilisera le même résultat, contenu dans la variable \texttt{s} pour
afficher la courbe de survie correspondante. Par défaut, lorsqu'il n'y a
qu'un groupe \R affiche automatiquement l'intervalle de confiance à 95~\%.
<<ex7-2d, fig=TRUE>>=
plot(survfit(Surv(time, status) ~ tx, data=mp, subset=tx=="6-MP"))
@ 

Pour afficher la fonction de risque cumulée, on ajoutera l'option
\verb|fun="cumhaz"| à la commande graphique précédente, soit en considérant
les deux groupes :
<<ex7-2e, fig=TRUE>>=
plot(s, fun="cumhaz")
@ 

La médiane de survie est simplement obtenue en affichant la variable
\texttt{s}, c'est-à-dire :
<<ex7-2f>>=
s
@
\end{sol}
\end{exo}
% 
% Everitt p. 348
%
\begin{exo}\label{exo:7.3}
Dans un essai randomisé, on a cherché à comparer deux traitements pour le
cancer de la prostate. Les patients prenaient chaque jour par voie orale
soit 1 mg de diethylstilbestrol (DES, bras actif) soit un placebo, et le
temps de survie est mesuré en mois.\autocite{collett94} La question
d'intérêt est de savoir si la survie diffère entre les deux groupes de
patients, et on négligera les autres variables présentes dans le fichier de
données \texttt{prostate.dat}. 
\begin{description}
\item[(a)] Calculer la médiane de survie pour l'ensemble des patients, et
  par groupe de traitement.
\item[(b)] Quelle est la différence entre les proportions de survie dans les
  deux groupes à 50 mois ?
\item[(c)] Afficher les courbes de survie pour les deux groupes de patients.
\item[(d)] Effectuer un test du log-rank pour tester l'hypothèse selon
  laquelle le traitement par DES a un effet positif sur la survie des
  patients. 
\end{description}
\begin{sol}
Pour importer les données, on utilisera la commande \texttt{read.table}. On
profitera de l'inspection préalable des données pour recoder la variable
\texttt{Treatment} en variable qualitative.
<<ex7-3a>>=
prostate <- read.table("data/prostate.dat", header=TRUE)
str(prostate)
head(prostate)
prostate$Treatment <- factor(prostate$Treatment)
table(prostate$Status)
@ 
Il n'est pas vraiment nécessaire de recoder \texttt{Status} en variable
qualitative puisque l'on va utiliser cette variable conjointement avec
\texttt{Time} pour l'analyse de survie. Dans un premier temps, pour
transformer les données en données de survie, incluant la censure, il est
nécessaire de passer par la commande \texttt{Surv} du package \texttt{survival}.
<<ex7-3b>>=
library(survival)
with(prostate, Surv(time=Time, event=Status))
@ 

La médiane de survie s'obtient à l'aide de \texttt{survfit}, qui une
commande générale permettant de manipuler les données de survie par la
méthode de Kaplan-Meier ou le modèle de Cox. Si l'on ne tient pas compte des
groupes de traitement (et plus généralement d'un quelconque co-facteur), on
l'utilisera ainsi : 
<<ex7-3c>>=
survfit(Surv(Time, Status) ~ 1, data=prostate)
@

Pour prendre en compte le facteur traitement dans l'analyse, on utilisera en
revanche : 
<<ex7-3d>>=
survfit(Surv(Time, Status) ~ Treatment, data=prostate)
@ 

Pour afficher les courbes de survie correspondant aux deux groupes de
traitement, on procèdera comme suit :
<<ex7-3e, fig=TRUE>>=
plot(survfit(Surv(Time, Status) ~ Treatment, data=prostate))
@ 

Le test du log-rank peut être obtenu de deux manières différentes. La plus
simple consiste à utiliser la commande \texttt{survdiff} pour comparer deux
courbes de survie à partir de l'estimateur de Kaplan-Meier.
<<ex7-3f>>=
survdiff(Surv(time=Time, event=Status) ~ Treatment, data=prostate)
@ 

L'autre solution consiste à utiliser une régression de Cox
<<ex7-3f>>=
summary(coxph(Surv(time=Time, event=Status) ~ Treatment, data=prostate))
@ 
\end{sol}
\end{exo}

\Closesolutionfile{solutions}

\chapter*{Devoir \no 6}
\addcontentsline{toc}{chapter}{Devoir \no 6}

Les exercices sont indépendants. Une seule réponse est correcte pour chaque
question. Lorsque vous ne savez pas répondre, cochez la case correspondante.

\section*{Exercice 1}
Considérons les données de survie d'un échantillon de 23 participants
afro-américains, tous de sexe masculin, provenant de l'étude \emph{San
  Francisco Men's Health Study} (1983). Les durées présentées dans le
tableau ci-dessous représentent le temps en mois depuis un diagnostique de
Sida jusqu'au décès du participant ou la fin de la durée de suivi
\citep[p.~130]{selvin08}, pour les participants fumeurs (F) et non-fumeurs
(NF). Les données censurées sont représentées par une astérisque.  \vskip1em

\begin{tabular}{ll}
\toprule
Non-fumeurs & 2* 42* 27* 22 26* 16 31 37 15 30 12* 5 80 \\
            & 29 13 1 14 \\
Fumeurs    & 21* 4 25 8 23 18 \\
\bottomrule
\end{tabular}
\vskip1em

Les données ont été enregistrées sous \R de la même manière qu'à
l'exercice~7.2, c'est-à-dire sous la forme d'un tableau à 3 colonnes :
fumeur (oui/non), temps et statut (en 0/1). Voici un aperçu de
ces données sous \R :
\begin{verbatim}
> head(sfmhs)
  fumeur temps status
1    non     2      0
2    non    42      0
3    non    27      0
4    non    22      1
5    non    26      0
6    non    16      1
\end{verbatim}
\begin{description}
\item[\bf 1.1] \marginpar{\phantom{text}1.1 $\square$} La commande suivante permet-elle
  d'indiquer la proportion de sujets fumeurs encore en vie à la fin de
  l'étude ?
\begin{verbatim}
> with(sfmhs, sum(status[fumeur=="oui"])) 
\end{verbatim}
  \begin{description}
  \item[A.] Oui.
  \item[B.] Non.
  \item[C.] Je ne sais pas.
  \end{description}
\item[\bf 1.2] \marginpar{\phantom{text}1.2 $\square$} On souhaite afficher la table de
  mortalité des non-fumeurs. Quelle commande doit-on utiliser ?
  \begin{description}
  \item[A.] \verb|summary(Surv(temps, status) ~ fumeur, data=sfmhs)|
  \item[B.] \verb|summary(Surv(status, temps) ~ fumeur, data=sfmhs)|
  \item[C.] \verb|summary(survfit(Surv(temps, status) ~ fumeur, data=sfmhs))|
  \item[D.] \verb|summary(survfit(Surv(status, temps) ~ fumeur, data=sfmhs))|
  \item[E.] Je ne sais pas.
  \end{description}
\item[\bf 1.3] \marginpar{\phantom{text}1.3 $\square$} En notant \texttt{st} la variable
  dans laquelle on a stocké les données de survie enregistrées au format \R
  via la commande \texttt{Surv}, la commande suivante permet de fournir le
  même résultat que la commande de l'exercice~1.2 ?
\begin{verbatim}
> summary(survfit(st ~ 1, data=sfmhs, subset=fumeur=="non"))
\end{verbatim}  
  \begin{description}
  \item[A.] Vrai.
  \item[B.] Faux.
  \item[C.] Je ne sais pas.
  \end{description}
\item[\bf 1.4] \marginpar{\phantom{text}1.4 $\square$} Pour afficher la courbe de survie
  du groupe des patients non-fumeurs, sans intervalles de confiance, quelle
  commande peut-on utiliser ? On supposera que la table de mortalité a été
  stockée dans une variable appelée \texttt{stab}.
  \begin{description}
  \item[A.] \verb|plot(stab)|
  \item[B.] \verb|plot(stab, confint=0)|
  \item[C.] \verb|plot(stab, conf.int=FALSE)|    
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 1.5] \marginpar{\phantom{text}1.5 $\square$} À quelle procédure de test
  correspond la commande suivante ?
\begin{verbatim}
> survdiff(st ~ fumeur, data=sfmhs, rho=1)
\end{verbatim}
  \begin{description}
  \item[A.] Un test du log-rank.
  \item[B.] Un test de Wilcoxon.
  \item[C.] Un test du coefficient de régression d'un modèle de Cox.
  \item[D.] Je ne sais pas.
  \end{description}  
\end{description}

\section*{Exercice 2}
Dans une étude clinique réalisée chez des patients AML, on s'intéresse à
deux facteurs pronostic, l'âge ($< 50$ et $\ge 50$ ans) et le type de
cellularité de la moelle (100~\% ou moins), sur la survie des patients
\citep[p.~275]{lee03}. Les données des 30 patients sont présentées dans le
tableau suivant. Les variables $x_1$ et $x_2$ correspondent à l'âge et la
cellularité, avec $x_1=0$ si le patient a moins de 50 ans et $x_2=0$ lorsque
la cellularité est inférieure à 100~\%. Les censures sont indiquées par le
symbole \texttt{+}, et les durées sont reportées en mois.  
\vskip1em

\begin{tabular}{lcc|lcc}
  \toprule
  Survie & $x_1$ & $x_2$ & Survie & $x_1$ & $x_2$ \\
  \midrule
  18 & 0 & 0 & 8 & 1 & 0 \\
  9 & 0 & 1 & 2 & 1 & 1 \\
  28+ & 0 & 0 & 26+ & 1 & 0 \\
  31 & 0 & 1 & 10 & 1 & 1 \\
  39+ & 0 & 1 & 4 & 1 & 0 \\
  19+ & 0 & 1 & 3 & 1 & 0 \\
  45+ & 0 & 1 & 4 & 1 & 0 \\
  6 & 0 & 1 & 18 & 1 & 1 \\
  8 & 0 & 1 & 8 & 1 & 1 \\
  15 & 0 & 1 & 3 & 1 & 1 \\
  23 & 0 & 0 & 14 & 1 & 1 \\
  28+ & 0 & 0 & 3 & 1 & 0 \\
  7 & 0 & 1 & 13 & 1 & 1 \\
  12 & 1 & 0 & 13 & 1 & 1 \\
  9 & 1 & 0 & 35+ & 1 & 0 \\
  \bottomrule
\end{tabular}
\vskip1em

Voici un aperçu du fichier de données, \texttt{aml.dat}, dans lequel les
données ont été stockées :
\begin{verbatim}
18 0 0 0
8 0 1 0
9 0 0 1
2 0 1 1
28 1 0 0
18 0 0 0
8 0 1 0
9 0 0 1
28 1 0 0
26 1 1 0
\end{verbatim}
Chaque ligne correspond à un individu. La première colonne correspond au
nombre de mois indiqué dans le tableau précédent, la deuxième colonne
correspond à la présence d'une censure (censure = 1), la troisième colonne à
la variable indicatrice codant pour l'âge ($\ge 50$ = 0) et la quatrième et
dernière colonne correspond à la variable indicatrice codant pour la
cellularité (100~\% = 1). Une valeur de 1 pour $x_1$ ou $x_2$ est considérée
comme un indicateur de risque.
\begin{description}
\item[\bf 2.1] \marginpar{\phantom{text}2.1 $\square$} Quelle solution peut-on proposer
  pour importer ces données sous \R ?
  \begin{description}
  \item[A.] \verb|read.csv("aml.dat", header=TRUE, colnames=c("temps","status","age","cell"))|
  \item[B.] \verb|read.csv("aml.dat", colnames=c("temps","status","age","cell"), sep=" ")|
  \item[C.] \verb|read.table("aml.dat", col.names=c("temps","status","age","cell"))|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 2.2] \marginpar{\phantom{text}2.2 $\square$} On supposera par la suite que les
  données ont été importées et stockées dans une variable appelée
  \texttt{aml}. Pour travailler avec ces données sous \R, il est nécessaire
  de recoder la variable \texttt{status} de façon à ce que les valeurs 0
  soient transformées en 1 et, réciproquement, les valeurs 1 en 0. Quelle
  commande ou combinaison de commandes permet de réaliser cette opération ?
  \begin{description}
  \item[A.] \verb|aml$status[aml$status==0] <- 1; aml$status[aml$status==1] <- 0|
  \item[B.] \verb|aml$status <- abs(aml$status-1)|
  \item[C.] \verb|aml$status <- aml$status-1|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 2.3] \marginpar{\phantom{text}2.3 $\square$} Que produit la commande suivante ? 
\begin{verbatim}
> with(aml, table(age, cell))[2,2]
\end{verbatim}
  \begin{description}
  \item[A.] Le nombre d'individus à risque pour $x_1$ (\texttt{age}).
  \item[B.] Le nombre d'individus à risque pour $x_2$ (\texttt{cell}).
  \item[C.] Le nombre d'individus à risque pour $x_1$ et $x_2$.
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 2.4] \marginpar{\phantom{text}2.4 $\square$} Le nombre de données censurées par
  groupe d'âge peut être représenté graphiquement sous forme d'un diagramme
  en points à l'aide de la commande :
  \begin{description}
  \item[A.] \verb|dotplot(status ~ age, data=aml)|
  \item[B.] \verb|dotchart(status ~ age, data=aml)|
  \item[C.] \verb|dotplot(xtabs(status ~ age, data=aml))|
  \item[D.] \verb|dotplot(xtabs(~ status + age, data=aml))|
  \item[E.] Je ne sais pas.
  \end{description}  
\item[\bf 2.5] \marginpar{\phantom{text}2.5 $\square$} On souhaite effectuer une
  régression de Cox pour étudier l'effet du facteur âge. Quelle commande est
  la plus appropriée ?
  \begin{description}
  \item[A.] \verb|coxph(status ~ age, data=aml)|
  \item[B.] \verb|coxph(Surv(status, temps) ~ age, data=aml)|
  \item[C.] \verb|coxph(Surv(event=status, time=temps) ~ age, data=aml)|
  \item[D.] Je ne sais pas.
  \end{description}
\item[\bf 2.6] \marginpar{\phantom{text}2.6 $\square$} Voici les résultats retournés par
  \R pour un modèle de Cox incluant les deux prédicteurs :
\begin{verbatim}
      coef exp(coef) se(coef)     z     p
age  1.046      2.85    0.458 2.284 0.022
cell 0.359      1.43    0.440 0.815 0.420
\end{verbatim}
  Lorsque l'on considère simultanément les deux cofacteurs, le risque
  relatif pour un individu âgé de 50 ans ou plus et ayant une cellularité de
  100~\% est de :
  \begin{description}
  \item[A.] $1.05 + 0.36$
  \item[B.] $2.85 + 1.43$
  \item[C.] $\exp(1.05 + 0.36)$
  \item[D.] $\exp(2.85 + 1.43)$    
  \item[E.] Je ne sais pas.
  \end{description}
\item[\bf 2.7] \marginpar{\phantom{text}2.7 $\square$} La commande \texttt{confint},
  appliquée directement sur le modèle \texttt{coxph(...)} renverra :
  \begin{description}
  \item[A.] Les intervalles de confiance associés aux coefficients de
    régression. 
  \item[B.] Les intervalles de confiance associés aux risques relatifs.
  \item[C.] Je ne sais pas.
  \end{description}  
\end{description}

%-------------------------------------- Exos Stata ---------------------------
\blankpage
\blankpage

\thispagestyle{empty}
\centerline{\small Centre d'Enseignement de la Statistique Appliquée, à la Médecine et à la Biologie Médicale}
\vspace*{2cm}
\begin{center}
\centerline{\includegraphics[scale=.55]{cesam}}
\vspace*{2cm}
\begin{minipage}{.75\textwidth}
\begin{mdframed}[style=titlep]
\centerline{\Huge Programme de travail}
\vskip1em
\centerline{\Huge du cours d'informatique du CESAM}
\end{mdframed}
\end{minipage}
\end{center}
\vskip3em
\centerline{\Huge\bf Introduction au logiciel Stata}
\vskip5em
\begin{center}
  \begin{tabular}{ll}
    \textbf{Responsables :} & \\
    Christophe LALANNE & \url{christophe.lalanne@inserm.fr} \\
    Pr Mounir MESBAH   & \url{mounir.mesbah@upmc.fr}
  \end{tabular}
\end{center}
\vskip3em
\centerline{\Large \url{http://www.cesam.upmc.fr}}
\vskip3em
\centerline{\LARGE Année Universitaire 2012–2013}
\vfill
\begin{center}
\begin{minipage}{.6\textwidth}
\centering
Adresser toute correspondance à :\\
Université Pierre et Marie Curie – Paris 6
Secrétariat du CESAM – Les Cordeliers
Service Formation Continue, esc. B, 4ème étage,
15 rue de l’école de médecine,
75006 PARIS\\
ou par Courriel à : \url{cesam@upmc.fr}
\end{minipage}
\end{center}  



\blankpage

\chapter*{Calendrier}
\thispagestyle{empty}
\vskip3em

\begin{center}
\begin{tabular}{|l|p{10cm}|l|l|}
\hline
%%   \multicolumn{4}{|c|}{Module R} \\
%% \hline
%%   Sem. 05/02 & Éléments du langage, gestion de données & Cours 1 & Corrigés pp.~\pageref{start:sol1}–\pageref{stop:sol1} \\
%%   Sem. 12/02 & Statistiques descriptives et estimation & Cours 2 & Corrigés pp.~\pageref{start:sol2}–\pageref{stop:sol2}\\
%%   Sem. 19/02 & Comparaisons de deux variables & Cours 3 & Corrigés pp.~\pageref{start:sol3}–\pageref{stop:sol3}\\
%%   Sem. 26/02 & Analyse de variance et plans d'expérience & Cours 4 & Corrigés pp.~\pageref{start:sol4}–\pageref{stop:sol4}\\
%%   Sem. 12/03 & Corrélation et régression linéaire & Cours 5 & Corrigés pp.~\pageref{start:sol5}–\pageref{stop:sol5}\\
%%   Sem. 19/03 & Mesures d'association en épidémiologie et régression
%%   logistique & Cours 6 & Corrigés pp.~\pageref{start:sol6}–\pageref{stop:sol6}\\
%%   Sem. 26/03 & Données de survie & Cours 7 & Corrigés pp.~\pageref{start:sol7}–\pageref{stop:sol7}\\
%% \hline
  \multicolumn{4}{|c|}{Module Stata} \\
\hline
  Sem. 02/04 & Élements du langage et statistiques descriptives & &\\
  Sem. 09/04 & Mesures d'association et comparaison de deux variables & &\\
  Sem. 16/04 & Régression linéaire et logistique & &\\
  Sem. 23/04 & Données de survie & &\\
\hline
\end{tabular}
\end{center}


\setcounter{page}{1}
\include{exos_stata}

\cleardoublepage

\blankpage
\blankpage

% -------------------------------------------------- Corrigés R ----------------
\thispagestyle{empty}

\centerline{\small Centre d'Enseignement de la Statistique Appliquée, à la Médecine et à la Biologie Médicale}
\vspace*{2cm}
\begin{center}
\centerline{\includegraphics[scale=.55]{cesam}}
\vspace*{2cm}
\begin{minipage}{.75\textwidth}
\begin{mdframed}[style=titlep]
\centerline{\Huge Corrigés des exercices}
\vskip1em
\centerline{\Huge du cours d'informatique du CESAM}
\end{mdframed}
\end{minipage}
\end{center}
\vskip3em
\centerline{\Huge\bf Introduction au logiciel R}
\vskip5em
\begin{center}
  \begin{tabular}{ll}
    \textbf{Responsables :} & \\
    Christophe LALANNE & \url{christophe.lalanne@inserm.fr} \\
    Pr Mounir MESBAH   & \url{mounir.mesbah@upmc.fr}
  \end{tabular}
\end{center}
\vskip3em
\centerline{\Large \url{http://www.cesam.upmc.fr}}
\vskip3em
\centerline{\LARGE Année Universitaire 2012–2013}
\vfill
\begin{center}
\begin{minipage}{.6\textwidth}
\centering
Adresser toute correspondance à :\\
Université Pierre et Marie Curie – Paris 6
Secrétariat du CESAM – Les Cordeliers
Service Formation Continue, esc. B, 4ème étage,
15 rue de l’école de médecine,
75006 PARIS\\
ou par Courriel à : \url{cesam@upmc.fr}
\end{minipage}
\end{center}

\blankpage

% remove chapter prefix because we no longer want chapter name.
\titleformat{\chapter}
  {\normalfont\huge\bfseries}{}{20pt}{\huge}


\setcounter{page}{1}

\chapter*{Semaine 1\markboth{Corrigés de la semaine 1}{}}
\label{start:sol1}
\input{solutions1} 
\label{stop:sol1}

\chapter*{Semaine 2\markboth{Corrigés de la semaine 2}{}}
\label{start:sol2}
\input{solutions2} 
\label{stop:sol2}

\chapter*{Semaine 3\markboth{Corrigés de la semaine 3}{}}
\label{start:sol3}
\input{solutions3} 
\label{stop:sol3}

\chapter*{Semaine 4\markboth{Corrigés de la semaine 4}{}}
\label{start:sol4}
\input{solutions4} 
\label{stop:sol4}

\chapter*{Semaine 5\markboth{Corrigés de la semaine 5}{}}
\label{start:sol5}
\input{solutions5} 
\label{stop:sol5}

\chapter*{Semaine 6\markboth{Corrigés de la semaine 6}{}}
\label{start:sol6}
\input{solutions6} 
\label{stop:sol6}

\chapter*{Semaine 7\markboth{Corrigés de la semaine 7}{}}
\label{start:sol7}
\input{solutions7} 
\label{stop:sol7}

\cleardoublepage

\blankpage
\blankpage

% -------------------------------------------------- Corrigés Stata ----------------
\thispagestyle{empty}

\centerline{\small Centre d'Enseignement de la Statistique Appliquée, à la Médecine et à la Biologie Médicale}
\vspace*{2cm}
\begin{center}
\centerline{\includegraphics[scale=.55]{cesam}}
\vspace*{2cm}
\begin{minipage}{.75\textwidth}
\begin{mdframed}[style=titlep]
\centerline{\Huge Corrigés des exercices}
\vskip1em
\centerline{\Huge du cours d'informatique du CESAM}
\end{mdframed}
\end{minipage}
\end{center}
\vskip3em
\centerline{\Huge\bf Introduction au logiciel Stata}
\vskip5em
\begin{center}
  \begin{tabular}{ll}
    \textbf{Responsables :} & \\
    Christophe LALANNE & \url{christophe.lalanne@inserm.fr} \\
    Pr Mounir MESBAH   & \url{mounir.mesbah@upmc.fr}
  \end{tabular}
\end{center}
\vskip3em
\centerline{\Large \url{http://www.cesam.upmc.fr}}
\vskip3em
\centerline{\LARGE Année Universitaire 2012–2013}
\vfill
\begin{center}
\begin{minipage}{.6\textwidth}
\centering
Adresser toute correspondance à :\\
Université Pierre et Marie Curie – Paris 6
Secrétariat du CESAM – Les Cordeliers
Service Formation Continue, esc. B, 4ème étage,
15 rue de l’école de médecine,
75006 PARIS\\
ou par Courriel à : \url{cesam@upmc.fr}
\end{minipage}
\end{center}



\blankpage

% remove chapter prefix because we no longer want chapter name.
\titleformat{\chapter}
  {\normalfont\huge\bfseries}{}{20pt}{\huge}
  

\setcounter{page}{1}
\include{sols_stata}

%-------------------------------------- Exos SAS  ---------------------------
\blankpage
\blankpage

\thispagestyle{empty}
\centerline{\small Centre d'Enseignement de la Statistique Appliquée, à la Médecine et à la Biologie Médicale}
\vspace*{2cm}
\begin{center}
\centerline{\includegraphics[scale=.55]{cesam}}
\vspace*{2cm}
\begin{minipage}{.75\textwidth}
\begin{mdframed}[style=titlep]
\centerline{\Huge Programme de travail}
\vskip1em
\centerline{\Huge du cours d'informatique du CESAM}
\end{mdframed}
\end{minipage}
\end{center}
\vskip3em
\centerline{\Huge\bf Introduction au logiciel SAS}
\vskip5em
\begin{center}
  \begin{tabular}{ll}
    \textbf{Responsables :} & \\
    Christophe LALANNE & \url{christophe.lalanne@inserm.fr} \\
    Pr Mounir MESBAH   & \url{mounir.mesbah@upmc.fr}
  \end{tabular}
\end{center}
\vskip3em
\centerline{\Large \url{http://www.cesam.upmc.fr}}
\vskip3em
\centerline{\LARGE Année Universitaire 2012–2013}
\vfill
\begin{center}
\begin{minipage}{.6\textwidth}
\centering
Adresser toute correspondance à :\\
Université Pierre et Marie Curie – Paris 6
Secrétariat du CESAM – Les Cordeliers
Service Formation Continue, esc. B, 4ème étage,
15 rue de l’école de médecine,
75006 PARIS\\
ou par Courriel à : \url{cesam@upmc.fr}
\end{minipage}
\end{center}  



\blankpage


\chapter*{Calendrier}
\thispagestyle{empty}
\vskip3em

\begin{center}
\begin{tabular}{|l|p{10cm}|l|l|}
\hline
  \multicolumn{4}{|c|}{Introduction au logiciel SAS} \\
\hline
  Sem. 02/04 & Élements du langage et statistiques descriptives & Corrigés
  pp.~\pageref{start:sol8b}–\pageref{stop:sol8b} & Devoir
  \no 7\\
  Sem. 09/04 & Mesures d'association et comparaison de deux variables &
  Corrigés pp.~\pageref{start:sol9b}–\pageref{stop:sol9b} &
  Devoir \no 8\\
  Sem. 16/04 & Régression linéaire et logistique & Corrigés pp.~\pageref{start:sol10b}–\pageref{stop:sol10b} & Devoir \no 9\\
  Sem. 23/04 & Données de survie & Corrigés pp.~\pageref{start:sol11b}–\pageref{stop:sol11b} & Devoir \no 10\\
\hline
\end{tabular}
\end{center}

\setcounter{page}{1}
\setcounter{chapter}{7}
\include{exos_sas}

\cleardoublepage

\blankpage
\blankpage

% -------------------------------------------------- Corrigés SAS ----------------
\thispagestyle{empty}

\centerline{\small Centre d'Enseignement de la Statistique Appliquée, à la Médecine et à la Biologie Médicale}
\vspace*{2cm}
\begin{center}
\centerline{\includegraphics[scale=.55]{cesam}}
\vspace*{2cm}
\begin{minipage}{.75\textwidth}
\begin{mdframed}[style=titlep]
\centerline{\Huge Corrigés des exercices}
\vskip1em
\centerline{\Huge du cours d'informatique du CESAM}
\end{mdframed}
\end{minipage}
\end{center}
\vskip3em
\centerline{\Huge\bf Introduction au logiciel SAS}
\vskip5em
\begin{center}
  \begin{tabular}{ll}
    \textbf{Responsables :} & \\
    Christophe LALANNE & \url{christophe.lalanne@inserm.fr} \\
    Pr Mounir MESBAH   & \url{mounir.mesbah@upmc.fr}
  \end{tabular}
\end{center}
\vskip3em
\centerline{\Large \url{http://www.cesam.upmc.fr}}
\vskip3em
\centerline{\LARGE Année Universitaire 2012–2013}
\vfill
\begin{center}
\begin{minipage}{.6\textwidth}
\centering
Adresser toute correspondance à :\\
Université Pierre et Marie Curie – Paris 6
Secrétariat du CESAM – Les Cordeliers
Service Formation Continue, esc. B, 4ème étage,
15 rue de l’école de médecine,
75006 PARIS\\
ou par Courriel à : \url{cesam@upmc.fr}
\end{minipage}
\end{center}



\blankpage

% remove chapter prefix because we no longer want chapter name.
\titleformat{\chapter}
  {\normalfont\huge\bfseries}{}{20pt}{\huge}
  

\setcounter{page}{1}
\include{sols_sas}

\end{document}
